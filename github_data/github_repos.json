[
  {
    "name": "NLWeb",
    "url": "https://github.com/davidatoms/NLWeb",
    "description": "Natural Language Web",
    "type": "fork",
    "updated_at": "2025-05-19T19:51:19Z",
    "readme": "# What is NLWeb\n\n\nBuilding conversational interfaces for websites is hard. NLWeb seeks to\nmake it easy for websites to do this. And since NLWeb natively speaks MCP,\nthe same natural language APIs can be used both by humans and agents.\n\nSchema.org and related semi-structured formats like RSS,\nin use by over 100m websites, have become not just the defacto\nsyndication mechanism but also the semantic layer for the web. NLWeb leverages\nthese to make it much easier to create natural language interfaces. \n\nNLWeb is a collection of open protocols and associated\nopen source tools. Its main focus is establishing a foundational \nlayer for the AI Web \u2014 much like HTML revolutionized document sharing.\nTo make this vision reality, NLWeb provides practical implementation code\u2014not as the \ndefinitive solution, but as proof-of-concept demonstrations showing one possible \napproach. We expect and encourage the community to develop diverse, innovative \nimplementations that surpass our examples. This mirrors the web's own evolution, \nfrom the humble 'htdocs' folder in NCSA's http server to today's massive data center \ninfrastructures\u2014all unified by shared protocols that enable seamless communication.\n\nAI has the potential to enhance every web interaction, but realizing this vision \nrequires a collaborative effort reminiscent of the Web's early \"barn raising\" spirit. \nSuccess demands shared protocols, sample implementations, and community participation. \nNLWeb combines protocols, Schema.org formats, and sample code to help sites rapidly \ncreate these endpoints, benefiting both humans through conversational interfaces and \nmachines through natural agent-to-agent interaction. \n\nJoin us in building this connected web of agents.\n\n\n# How it Works\n There are two distinct components to NLWeb.\n 1. A protocol, very simple to begin with, to interface with a site in natural \n     language and a format, leveraging json and schema.org for the returned answer. \n     See the documentation on the [REST API](/docs/RestAPI.md) for more details.\n\n 2. A straightforward implementation of (1) that leverages existing markup, for\n      sites that can be abstracted as lists of items (products, recipes, attractions,\n      reviews, etc.). Together with a set of user interface widgets, sites can \n      easily provide conversational interfaces to their content. See the documentation\n      on [Life of a chat query](docs/LifeOfAChatQuery.md) for more details on how this works.\n\n\n# NLWeb and MCP\n MCP (Model Context Protocol) is an emerging protocol for Chatbots and AI assistants\n to interact with tools. Every NLWeb instance is also an MCP server, which supports one core method,\n <code>ask</code>, which is used to ask a website a question in natural language. The returned response\n leverages schema.org, a widely-used vocabulary for describing web data. Loosely speaking, \n MCP is NLWeb as Http is to HTML.\n\n\n# NLWeb and platforms\nNLWeb is deeply agnostic:\n- About the platform: We have tested it running on Windows, MacOS, Linux, ...\n- About the vector stores used: Qdrant, Snowflake, Milvus, Azure AI Search, ...\n- About the LLM: OAI, Deepseek, Gemini, Anthropic, Inception, ...\n- It is intended to be both lightweight and scalable, running on everything from clusters \n  in the cloud to laptops and soon phones.\n\n\n# Repository\nThis repository contains the following:\n\n- The code for the core service -- handling a natural language query on how this can be extended / customized.\n- Connectors to some of the popular LLMs and vector databases.\n- Tools for adding data in schema.org jsonl, RSS, etc. to a vector database of choice.\n- A web server front end for this service. The service, being small enough runs in the web server.\n- A simple UI for enabling users to issue queries via this web server.\n\nWe expect most production deployments to use their own UI. They are also likely to integrate\nthe code into their application environment (as opposed to running a standalone NLWeb server). They\nare also encouraged to connect NLWeb to their 'live' database as opposed to copying\nthe contents over, which inevitably introduces freshness issues.\n\n\n# Documentation\n\n## Getting Started\n- [Hello world on your laptop](HelloWorld.md)\n- [Running it on Azure](docs/Azure.md)\n- Running it on GCP ... coming soon\n- Running it AWS ... coming soon\n\n## NLWeb\n- [Life of a Chat Query](docs/LifeOfAChatQuery.md)\n- [Modifying behaviour by changing prompts](docs/Prompts.md)\n- [Modifying control flow](docs/ControlFlow.md)\n- [Modifying the user interface](/docs/UserInterface.md)\n- [REST interface](docs/RestAPI.md)\n- [Adding memory to your NLWeb interface](/docs/Memory.md)\n\n\n\n-----------------------------------------------------------------\n\n## License \n\nNLWeb uses the [MIT License](LICENSE).\n\n\n## Deployment (CI/CD)\n\n_At this time, the repository does not use continuous integration or produce a website, artifact, or anything deployed._\n\n## Access\n\nFor questions about this GitHub project, please reach out to [NLWeb Support](mailto:NLWebSup@microsoft.com).\n\n## Contributing\n\nPlease see [Contribution Guidance](CONTRIBUTING.md) for more information.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
  },
  {
    "name": "llmscript",
    "url": "https://github.com/davidatoms/llmscript",
    "description": "\ud83e\uddd1\u200d\ud83d\udcbb Write your shell scripts in natural language",
    "type": "fork",
    "updated_at": "2025-05-18T06:51:59Z",
    "readme": "# llmscript\n\n[![CI](https://github.com/statico/llmscript/actions/workflows/ci.yml/badge.svg)](https://github.com/statico/llmscript/actions/workflows/ci.yml)\n\nllmscript is a shell script that uses a large language model (LLM) to build and test shell programs so that you can write scripts in natural language instead of bash or other shell scripting languages.\n\n<img src=\"https://github.com/user-attachments/assets/6257eb3c-fe66-41b0-9b45-39ec29b40a3a\" width=\"640\" alt=\"a terminal window showing a demonstration of the llmscript tool to print hello world\" />\n\nYou can configure it to use [Ollama](https://ollama.com/) (free and local), [Claude](https://www.anthropic.com/claude) (paid), or [OpenAI](https://openai.com/) (paid).\n\n> [!NOTE]\n> Does this actually work? Yeah, somewhat! Could it create scripts that erase your drive? Maybe! Good luck!\n>\n> Most of this project was written by [Claude](https://www.anthropic.com/claude) with [Cursor](https://www.cursor.com). I can't actually claim that I \"wrote\" any of the source code. I barely know Go.\n\n## Example\n\n```\n#!/usr/bin/env llmscript\n\nCreate an output directory, `output`.\nFor every PNG file in `input`:\n  - Convert it to 256x256 with ImageMagick\n  - Run pngcrush\n```\n\nRunning it with a directory of PNG files would look like this:\n\n```\n$ ./convert-pngs\n\u2713 Script generated successfully!\nCreating output directory\nConvering input/1.png\nConvering input/2.png\nConvering input/3.png\nRunning pngcrush on output/1.png\nRunning pngcrush on output/2.png\nRunning pngcrush on output/3.png\nDone!\n```\n\nRunning it again will use the cache and not generate any new scripts:\n\n```\n$ ./convert-pngs\n\u2713 Cached script found\nCreating output directory\nConvering input/1.png\n...\n```\n\nIf you want to generate a new script, use the `--no-cache` flag.\n\n## Prerequisites\n\n- [Go](https://go.dev/) (1.22 or later)\n- One of:\n  - [Ollama](https://ollama.com/) running locally\n  - A [Claude](https://www.anthropic.com/claude) API key\n  - An [OpenAI](https://openai.com/) API key\n\n## Installation\n\n```\ngo install github.com/statico/llmscript/cmd/llmscript@latest\n```\n\n(Can't find it? Check `~/go/bin`.)\n\nOr, if you're spooked by running LLM-generated shell scripts (good for you!), consider running llmscript via Docker:\n\n```\ndocker run --network host -it -v \"$(pwd):/data\" -w /data ghcr.io/statico/llmscript --verbose examples/hello-world\n```\n\n## Usage\n\nCreate a script file like the above example, or check out the [examples](examples) directory for more. You can use a shebang like:\n\n```\n#!/usr/bin/env llmscript\n```\n\nor run it directly like:\n\n```\n$ llmscript hello-world\n```\n\nBy default, llmscript will use Ollama with the `llama3.2` model. You can configure this by creating a config file with the `llmscript --write-config` command to create a config file in `~/.config/llmscript/config.yaml` which you can edit. You can also use command-line args (see below).\n\n## How it works\n\nWant to see it all in action? Run `llmscript --verbose examples/hello-world`\n\nGiven a script description written in natural language, llmscript works by:\n\n1. Generating a feature script that implements the functionality\n2. Generating a test script that verifies the feature script works correctly\n3. Running the test script to verify the feature script works correctly, fixing the feature script if necessary, possibly going back to step 1 if the test script fails too many times\n4. Caching the scripts for future use\n5. Running the feature script with any additional arguments you provide\n\nFor example, given a simple hello world script:\n\n```\n#!/usr/bin/env llmscript\n\nPrint hello world\n```\n\nllmscript might generate the following feature script:\n\n```bash\n#!/bin/bash\necho \"Hello, world!\"\n```\n\n...and the following test script to test it:\n\n```bash\n#!/bin/bash\n[ \"$(./script.sh)\" = \"Hello, world!\" ] || exit 1\n```\n\n## Configuration\n\nllmscript can be configured using a YAML file located at `~/.config/llmscript/config.yaml`. You can auto-generate a configuration file using the `llmscript --write-config` command.\n\nHere's an example configuration:\n\n```yaml\n# LLM configuration\nllm:\n  # The LLM provider to use (required)\n  provider: \"ollama\" # or \"claude\", \"openai\", etc.\n\n  # Provider-specific settings\n  ollama:\n    model: \"llama3.2\" # The model to use\n    host: \"http://localhost:11434\" # Optional: Ollama host URL\n\n  claude:\n    api_key: \"${CLAUDE_API_KEY}\" # Environment variable reference\n    model: \"claude-3-opus-20240229\"\n\n  openai:\n    api_key: \"${OPENAI_API_KEY}\"\n    model: \"gpt-4-turbo-preview\"\n\n# Maximum number of attempts to fix the script allowed before restarting from step 2\nmax_fixes: 10\n\n# Maximum number of attempts to generate a working script before giving up completely\nmax_attempts: 3\n\n# Timeout for script execution during testing (in seconds)\ntimeout: 30\n\n# Additional prompt to provide to the LLM\nadditional_prompt: |\n  Use ANSI color codes to make the output more readable.\n```\n\n### Environment Variables\n\nYou can use environment variables in the configuration file using the `${VAR_NAME}` syntax. This is particularly useful for API keys and sensitive information.\n\n### Configuration Precedence\n\n1. Command line flags (highest priority)\n2. Environment variables\n3. Configuration file\n4. Default values (lowest priority)\n\n### Command Line Flags\n\nYou can override configuration settings using command line flags:\n\n```shell\nllmscript --llm.provider=claude --timeout=10 script.txt\n```\n\n## Caveats\n\n> [!WARNING]\n> This is an experimental project. It generates and executes shell scripts, which could be dangerous if the LLM generates malicious code. Use at your own risk and always review generated scripts before running them.\n"
  },
  {
    "name": "llm",
    "url": "https://github.com/davidatoms/llm",
    "description": "Access large language models from the command-line",
    "type": "fork",
    "updated_at": "2025-05-18T06:43:26Z",
    "readme": "# LLM\n\n[![PyPI](https://img.shields.io/pypi/v/llm.svg)](https://pypi.org/project/llm/)\n[![Documentation](https://readthedocs.org/projects/llm/badge/?version=latest)](https://llm.datasette.io/)\n[![Changelog](https://img.shields.io/github/v/release/simonw/llm?include_prereleases&label=changelog)](https://llm.datasette.io/en/stable/changelog.html)\n[![Tests](https://github.com/simonw/llm/workflows/Test/badge.svg)](https://github.com/simonw/llm/actions?query=workflow%3ATest)\n[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://github.com/simonw/llm/blob/main/LICENSE)\n[![Discord](https://img.shields.io/discord/823971286308356157?label=discord)](https://datasette.io/discord-llm)\n[![Homebrew](https://img.shields.io/homebrew/installs/dy/llm?color=yellow&label=homebrew&logo=homebrew)](https://formulae.brew.sh/formula/llm)\n\nA CLI utility and Python library for interacting with Large Language Models, both via remote APIs and models that can be installed and run on your own machine.\n\n[Run prompts from the command-line](https://llm.datasette.io/en/stable/usage.html#executing-a-prompt), [store the results in SQLite](https://llm.datasette.io/en/stable/logging.html), [generate embeddings](https://llm.datasette.io/en/stable/embeddings/index.html) and more.\n\nConsult the **[LLM plugins directory](https://llm.datasette.io/en/stable/plugins/directory.html)** for plugins that provide access to remote and local models.\n\nFull documentation: **[llm.datasette.io](https://llm.datasette.io/)**\n\nBackground on this project:\n\n- [llm, ttok and strip-tags\u2014CLI tools for working with ChatGPT and other LLMs](https://simonwillison.net/2023/May/18/cli-tools-for-llms/)\n- [The LLM CLI tool now supports self-hosted language models via plugins](https://simonwillison.net/2023/Jul/12/llm/)\n- [LLM now provides tools for working with embeddings](https://simonwillison.net/2023/Sep/4/llm-embeddings/)\n- [Build an image search engine with llm-clip, chat with models with llm chat](https://simonwillison.net/2023/Sep/12/llm-clip-and-chat/)\n- [You can now run prompts against images, audio and video in your terminal using LLM](https://simonwillison.net/2024/Oct/29/llm-multi-modal/)\n- [Structured data extraction from unstructured content using LLM schemas](https://simonwillison.net/2025/Feb/28/llm-schemas/)\n- [Long context support in LLM 0.24 using fragments and template plugins](https://simonwillison.net/2025/Apr/7/long-context-llm/)\n\n## Installation\n\nInstall this tool using `pip`:\n```bash\npip install llm\n```\nOr using [Homebrew](https://brew.sh/):\n```bash\nbrew install llm\n```\n[Detailed installation instructions](https://llm.datasette.io/en/stable/setup.html).\n\n## Getting started\n\nIf you have an [OpenAI API key](https://platform.openai.com/api-keys) you can get started using the OpenAI models right away.\n\nAs an alternative to OpenAI, you can [install plugins](https://llm.datasette.io/en/stable/plugins/installing-plugins.html) to access models by other providers, including models that can be installed and run on your own device.\n\nSave your OpenAI API key like this:\n\n```bash\nllm keys set openai\n```\nThis will prompt you for your key like so:\n```\nEnter key: <paste here>\n```\nNow that you've saved a key you can run a prompt like this:\n```bash\nllm \"Five cute names for a pet penguin\"\n```\n```\n1. Waddles\n2. Pebbles\n3. Bubbles\n4. Flappy\n5. Chilly\n```\nRead the [usage instructions](https://llm.datasette.io/en/stable/usage.html) for more.\n\n## Installing a model that runs on your own machine\n\n[LLM plugins](https://llm.datasette.io/en/stable/plugins/index.html) can add support for alternative models, including models that run on your own machine.\n\nTo download and run Mistral 7B Instruct locally, you can install the [llm-gpt4all](https://github.com/simonw/llm-gpt4all) plugin:\n```bash\nllm install llm-gpt4all\n```\nThen run this command to see which models it makes available:\n```bash\nllm models\n```\n```\ngpt4all: all-MiniLM-L6-v2-f16 - SBert, 43.76MB download, needs 1GB RAM\ngpt4all: orca-mini-3b-gguf2-q4_0 - Mini Orca (Small), 1.84GB download, needs 4GB RAM\ngpt4all: mistral-7b-instruct-v0 - Mistral Instruct, 3.83GB download, needs 8GB RAM\n...\n```\nEach model file will be downloaded once the first time you use it. Try Mistral out like this:\n```bash\nllm -m mistral-7b-instruct-v0 'difference between a pelican and a walrus'\n```\nYou can also start a chat session with the model using the `llm chat` command:\n```bash\nllm chat -m mistral-7b-instruct-v0\n```\n```\nChatting with mistral-7b-instruct-v0\nType 'exit' or 'quit' to exit\nType '!multi' to enter multiple lines, then '!end' to finish\n> \n```\n\n## Using a system prompt\n\nYou can use the `-s/--system` option to set a system prompt, providing instructions for processing other input to the tool.\n\nTo describe how the code in a file works, try this:\n\n```bash\ncat mycode.py | llm -s \"Explain this code\"\n```\n\n## Help\n\nFor help, run:\n\n    llm --help\n\nYou can also use:\n\n    python -m llm --help\n"
  },
  {
    "name": "following-instructions-human-feedback",
    "url": "https://github.com/davidatoms/following-instructions-human-feedback",
    "description": null,
    "type": "fork",
    "updated_at": "2025-05-17T04:11:42Z",
    "readme": "# InstructGPT: Training Language Models to Follow Instructions with Human Feedback\n\n[Paper link][LINK_TO_PAPER]\n\n> Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI-API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback (RLHF). We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.\n\n\n## Contents\n- [model-card.md](model-card.md) - InstructGPT model card\n- [automatic-eval-samples](automatic-eval-samples/) - Samples from our models (both GPT-3 and InstructGPT) on public NLP benchmarks.\n- [API distribution labeling instructions](https://docs.google.com/document/d/1MJCqDNjzD04UbcnVZ-LmeXJ04-TKEICDAepXyMCBUb8/edit#) - Google doc of instructions given to contractors for final evaluations on our API prompt distribution.\n- [Toxicity labeling instructions](https://docs.google.com/document/d/1d3n6AqNrd-SJEKm_etEo3rUwXxKG4evCbzfWExvcGxg/edit?usp=sharing) - Google doc of instructions given to contractors for labeling toxic outputs on the RealToxicityPrompts dataset\n\n[LINK_TO_PAPER]: https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf\n\n"
  },
  {
    "name": "awesome-langgraphjs",
    "url": "https://github.com/davidatoms/awesome-langgraphjs",
    "description": "A list of open-source LangGraph.js examples, projects, applications and YouTube videos.",
    "type": "fork",
    "updated_at": "2025-05-16T20:00:27Z",
    "readme": "# Awesome LangGraph.js\n\nA collection of open source projects and applications built using LangGraph.js. Also, a list of YouTube videos and tutorials covering LangGraph.js.\n\n> [!NOTE]\n> Have your own open source project/video and want it added to this list? See the [Contributing](#contributing) section below.\n\n# Open Source Projects\n\n## By the LangChain Team\n\nThe projects listed below are built & maintained by the LangChain team.\n\n- [Open Agent Platform](https://github.com/langchain-ai/open-agent-platform) - An open-source, no-code agent building platform. Connect to MCP tools, internal data sources, and other agents.\n- [Open Canvas](https://github.com/langchain-ai/open-canvas) - A production-ready chat application for generating and editing text based artifacts.\n- [Social Media Agent](https://github.com/langchain-ai/social-media-agent) - A collection of graphs for automating social media content curation and posting on Twitter and LinkedIn.\n- [Agent Chat UI](https://github.com/langchain-ai/agent-chat-ui) - A chat interface for interacting with _any_ LangGraph agent.\n- [`npx create-agent-chat-app`](https://github.com/langchain-ai/create-agent-chat-app) - A CLI for creating a new LangGraph.js full stack chat application. It installs a chat interface and up to four prebuilt LangGraph.js agents.\n- [Agent Inbox](https://github.com/langchain-ai/agent-inbox) - An inbox UI/UX for managing and interacting with human-in-the-loop (HITL) LangGraph agents.\n- [LangGraph Generative UI Examples](https://github.com/langchain-ai/langgraphjs-gen-ui-examples) - A collection of generative UI examples using LangGraph.js\n- [Generative UI Computer Use Agent](https://github.com/bracesproul/gen-ui-computer-use) - A full-stack application for interacting with computer use agents (CUA) via a generative UI interface with LangGraph.js.\n- [LangGraph.js Examples](https://github.com/bracesproul/langgraphjs-examples) - A collection of example applications and workflows built using LangGraph.js. (Basic RAG Graph, Human-in-the-Loop, Stockbroker Generative UI, Streaming Messages Fullstack Application)\n- [Site RAG](https://github.com/bracesproul/site-rag) - A Chrome extension built with LangGraph.js for preforming question answering on any website.\n- [Fully Local PDF Chatbot](https://github.com/jacoblee93/fully-local-pdf-chatbot) - A full-stack local chatbot application for interacting with PDF documents.\n- [Chat LangChain.js](https://github.com/langchain-ai/chat-langchainjs) - A RAG chat application for asking questions over the LangChain documentation.\n- [LangGraph.js Computer Use Agent](https://github.com/langchain-ai/langgraphjs/tree/main/libs/langgraph-cua) - A prebuilt agent package for using computer use agents (CUA) with LangGraph.js.\n- [LangGraph.js Supervisor Agent](https://github.com/langchain-ai/langgraphjs/tree/main/libs/langgraph-supervisor) - A prebuilt agent package for implementing supervisor style agents with LangGraph.js\n- [LangGraph.js Swarm Agent](https://github.com/langchain-ai/langgraphjs/tree/main/libs/langgraph-swarm) - A prebuilt agent package for implementing swarm style agents with LangGraph.js\n- [LLManager](https://github.com/langchain-ai/llmanager) - A LangGraph workflow for managing approval requests. It uses reflection to improve and learn over time, along with dynamic prompt composition to handle a wide variety of approval requests.\n\n## By the LangGraph.js Community\n\nThe projects listed below are built by the LangGraph.js community.\n\n> [!IMPORTANT]\n> We're looking for more projects! If you have a project that you think would be helpful to the LangGraph.js community, please open a pull request to add it to this list!\n\n# YouTube Videos\n\n## By the LangChain Team\n\nThe videos listed below are created by the LangChain team.\n\n- [Build a Generative Ul App in LangGraph](https://youtu.be/sCqN01R8nIQ)\n- [Introducing `npx create-agent-chat-app`](https://youtu.be/DJXYUxoWkOU)\n- [LangGraph Computer Use Agents](https://youtu.be/ndCFqT6xFQ4)\n- [Introducing Agent Chat UI](https://youtu.be/lInrwVnZ83o)\n- [React JS Hook for your LangGraph Agent](https://youtu.be/h8rML95qWX8)\n- [SiteRAG - Open Source Chrome Extension for RAG](https://youtu.be/Af0Dz9bxcWY)\n- [Open Source Agent Inbox for LangGraph](https://youtu.be/gF341XMN8cY)\n- [Open Source Social Media Agent](https://youtu.be/TmTl5FMgkCQ)\n- [LLManager - Automate Approvals Through a Memory Agent](https://youtu.be/uqRK_aJBR2w)\n\n## By the LangGraph.js Community\n\nThe videos listed below are created by the LangGraph.js community.\n\n> [!IMPORTANT]\n> We're looking for more videos! If you have a video that you think would be helpful to the LangGraph.js community, please open a pull request to add it to this list!\n\n# Contributing\n\nIf you have an open source project which is built using LangGraph.js or LangChain.js, please open a pull request to add it to this list!\n\nPlease open a pull request adding your project/video under the `By the LangGraph.js Community` section. Ensure it is formatted like the other projects/videos in the list:\n\n```\n- [Project/Video Name](project/video URL) - A short, 1-2 sentence description of the project/video.\n```\n"
  },
  {
    "name": "ai-chatbot",
    "url": "https://github.com/davidatoms/ai-chatbot",
    "description": null,
    "type": "original",
    "updated_at": "2025-05-16T02:21:36Z",
    "readme": "<a href=\"https://chat.vercel.ai/\">\n  <img alt=\"Next.js 14 and App Router-ready AI chatbot.\" src=\"app/(chat)/opengraph-image.png\">\n  <h1 align=\"center\">Chat SDK</h1>\n</a>\n\n<p align=\"center\">\n    Chat SDK is a free, open-source template built with Next.js and the AI SDK that helps you quickly build powerful chatbot applications.\n</p>\n\n<p align=\"center\">\n  <a href=\"https://chat-sdk.dev\"><strong>Read Docs</strong></a> \u00b7\n  <a href=\"#features\"><strong>Features</strong></a> \u00b7\n  <a href=\"#model-providers\"><strong>Model Providers</strong></a> \u00b7\n  <a href=\"#deploy-your-own\"><strong>Deploy Your Own</strong></a> \u00b7\n  <a href=\"#running-locally\"><strong>Running locally</strong></a>\n</p>\n<br/>\n\n## Features\n\n- [Next.js](https://nextjs.org) App Router\n  - Advanced routing for seamless navigation and performance\n  - React Server Components (RSCs) and Server Actions for server-side rendering and increased performance\n- [AI SDK](https://sdk.vercel.ai/docs)\n  - Unified API for generating text, structured objects, and tool calls with LLMs\n  - Hooks for building dynamic chat and generative user interfaces\n  - Supports xAI (default), OpenAI, Fireworks, and other model providers\n- [shadcn/ui](https://ui.shadcn.com)\n  - Styling with [Tailwind CSS](https://tailwindcss.com)\n  - Component primitives from [Radix UI](https://radix-ui.com) for accessibility and flexibility\n- Data Persistence\n  - [Neon Serverless Postgres](https://vercel.com/marketplace/neon) for saving chat history and user data\n  - [Vercel Blob](https://vercel.com/storage/blob) for efficient file storage\n- [Auth.js](https://authjs.dev)\n  - Simple and secure authentication\n\n## Model Providers\n\nThis template ships with [xAI](https://x.ai) `grok-2-1212` as the default chat model. However, with the [AI SDK](https://sdk.vercel.ai/docs), you can switch LLM providers to [OpenAI](https://openai.com), [Anthropic](https://anthropic.com), [Cohere](https://cohere.com/), and [many more](https://sdk.vercel.ai/providers/ai-sdk-providers) with just a few lines of code.\n\n## Deploy Your Own\n\nYou can deploy your own version of the Next.js AI Chatbot to Vercel with one click:\n\n[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Fvercel%2Fai-chatbot&env=AUTH_SECRET&envDescription=Generate%20a%20random%20secret%20to%20use%20for%20authentication&envLink=https%3A%2F%2Fgenerate-secret.vercel.app%2F32&project-name=my-awesome-chatbot&repository-name=my-awesome-chatbot&demo-title=AI%20Chatbot&demo-description=An%20Open-Source%20AI%20Chatbot%20Template%20Built%20With%20Next.js%20and%20the%20AI%20SDK%20by%20Vercel&demo-url=https%3A%2F%2Fchat.vercel.ai&products=%5B%7B%22type%22%3A%22integration%22%2C%22protocol%22%3A%22ai%22%2C%22productSlug%22%3A%22grok%22%2C%22integrationSlug%22%3A%22xai%22%7D%2C%7B%22type%22%3A%22integration%22%2C%22protocol%22%3A%22storage%22%2C%22productSlug%22%3A%22neon%22%2C%22integrationSlug%22%3A%22neon%22%7D%2C%7B%22type%22%3A%22blob%22%7D%5D)\n\n## Running locally\n\nYou will need to use the environment variables [defined in `.env.example`](.env.example) to run Next.js AI Chatbot. It's recommended you use [Vercel Environment Variables](https://vercel.com/docs/projects/environment-variables) for this, but a `.env` file is all that is necessary.\n\n> Note: You should not commit your `.env` file or it will expose secrets that will allow others to control access to your various AI and authentication provider accounts.\n\n1. Install Vercel CLI: `npm i -g vercel`\n2. Link local instance with Vercel and GitHub accounts (creates `.vercel` directory): `vercel link`\n3. Download your environment variables: `vercel env pull`\n\n```bash\npnpm install\npnpm dev\n```\n\nYour app template should now be running on [localhost:3000](http://localhost:3000).\n"
  },
  {
    "name": "open-agent-platform",
    "url": "https://github.com/davidatoms/open-agent-platform",
    "description": "An open-source, no-code agent building platform.",
    "type": "fork",
    "updated_at": "2025-05-14T20:09:07Z",
    "readme": "# Open Agent Platform\n\nOpen Agent Platform is a citizen developer platform, allowing non-technical users to build, prototype, and use agents. These agents can be connected to a wide range of tools, RAG servers, and event other agents through an Agent Supervisor!\n\n\n<video src=\"https://github.com/user-attachments/assets/bc91304b-e704-41d7-a0cd-9806d37640c0.mp4\" controls=\"controls\"></video>\n\n# Setup\n\n## Agents\n\nThe first step in setting up Open Agent Platform is to deploy and configure your agents. To help with this, we're releasing two, pre-built agents, customized specifically for Open Agent Platform:\n\n- [Tools Agent](https://github.com/langchain-ai/oap-langgraph-tools-agent)\n- [Supervisor Agent](https://github.com/langchain-ai/oap-agent-supervisor)\n\nTo use these agents in your instance, you should:\n\n1. Clone the repositories\n2. Follow the instructions in the READMEs\n3. Deploy the agents to LangGraph Platform\n\nOnce deployed, you can connect them to your instance of Open Agent Platform by setting the configuration environment variables. The configuration object is as follows:\n\n```json\n{\n  \"id\": \"The project ID of the deployment. For locally running LangGraph servers, this can be any UUID.\",\n  \"tenantId\": \"The tenant ID of your LangSmith account. For locally running LangGraph servers, this can be any UUID.\",\n  \"deploymentUrl\": \"The API URL to your deployment\",\n  \"name\": \"A custom name for your deployment\",\n  \"isDefault\": \"Whether this deployment is the default deployment. Should only be set to true for one deployment.\",\n  \"defaultGraphId\": \"The graph ID of the default graph for the entire OAP instance. We recommend this is set to the graph ID of a graph which supports RAG & MCP tools. This must be set in the same deployment which isDefault is set to true on. Optional, but required in at least one deployment.\",\n}\n```\n\nTo easily find your project & tenant IDs, you can make a GET request to the `/info` endpoint of your deployment URL. Then, copy the `project_id` value into `id`, and `tenant_id` into `tenantId`.\n\nAfter constructing the JSON objects with these values for each of the deployments you want to include in your Open Agent Platform instance, you should stringify them into a single, flat array, then set them under the `NEXT_PUBLIC_DEPLOYMENTS` environment variable.\n\nThe following is an example of what this variable would look like for a single, local deployment:\n\n```bash\nNEXT_PUBLIC_DEPLOYMENTS=[{\"id\":\"bf63dc89-1de7-4a65-8336-af9ecda479d6\",\"deploymentUrl\":\"http://localhost:2024\",\"tenantId\":\"42d732b3-1324-4226-9fe9-513044dceb58\",\"name\":\"Local deployment\",\"isDefault\":true,\"defaultGraphId\":\"agent\"}]\n```\n\n## Authentication\n\nThe default authentication provider Open Agent Platform is configured to use is Supabase. To set up Supabase authentication, you must first create a new Supabase project. After creating a new project, set the following environment variables:\n\n```bash\nNEXT_PUBLIC_SUPABASE_URL=\"<your supabase url>\"\nNEXT_PUBLIC_SUPABASE_ANON_KEY=\"<your supabase anon key>\"\n```\n\nYou should also enable Google authentication in your Supabase project, or remove the UI code for Google authentication from the app.\n\n### LangGraph Server Authentication\n\nSince the pre-built LangGraph agents implement custom authentication, there is no need to specify a LangSmith API key when making requests to them. Instead, we pass the user's Supabase access token (JWT token) in the `Authorization` header of the request. Then, inside the auth middleware of the LangGraph server, we extract this token and verify it's valid with Supabase. If it is, we receive back a user ID, which is used to verify each user is *only* able to access their own agents, and threads. If you want to allow users access to agents they did not create, you should update the custom authentication middleware in the LangGraph server to allow access to the agents you want users to be able to access.\n\nAlong with the `Authorization` header, we duplicate passing the Supabase JWT via the `x-supabase-access-token` header. This is because all non-LangSmith specific headers which are sent to LangGraph servers which are prefixed with `x-` are included in the configurable fields of the thread. We will need this JWT to later authenticate with the MCP server.\n\n### Use LangSmith API Key Authentication\n\nIf you do *not* want to use custom authentication in your LangGraph server, and instead allow anyone to access your agents, you can do so by setting the `NEXT_PUBLIC_USE_LANGSMITH_AUTH` environment variable to `true`, and setting your `LANGSMITH_API_KEY` in the environment variables. Lastly, ensure you have the `NEXT_PUBLIC_BASE_API_URL` environment variable set to the base API URL of your **web** server. For local development, this should be set to:\n\n```bash\nNEXT_PUBLIC_BASE_API_URL=\"http://localhost:3000/api\"\n```\n\nThis will cause all requests made to your web client to first pass through a proxy route, which injects the LangSmith API key into the request from the server, as to not expose the API key to the client. The request is then forwarded on to your LangGraph server.\n\n> [!WARNING]\n> Remember *not* to prefix your LangSmith API key environment variable with `NEXT_PUBLIC_`, as this is a **secret** and should never be exposed to the client.\n\n### RAG Authentication\n\nAuthenticating to your LangConnect RAG server from the web client is handled in a similar way to LangGraph authentication. We pass the Supabase JWT in the `Authorization` header of the request. Then, inside the LangConnect RAG server, we extract this token and verify it's valid with Supabase. If it is, we receive back a user ID, which is used to verify each user is *only* able to access their own collections. We do not currently support sharing collections between users. If this is something you are interested in, please reach out to me, either on [X (Twitter)](https://x.com/bracesproul), or email: `brace@langchain.dev`\n\n## RAG Server\n\nOpen Agent Platform has first class support for RAG with your agents. To use this feature, you must deploy your own instance of a LangConnect RAG server.\n\nLangConnect is an open source managed retrieval service for RAG applications. It's built on top of LangChain's RAG integrations (vectorstores, document loaders, indexing API, etc.) and allows you to quickly spin up an API server for managing your collections & documents for any RAG application.\n\nTo set it up, you can follow the instructions in the [LangConnect README](https://github.com/langchain-ai/langconnect). After setting it up, and either running the server locally via Docker, or deploying it to a cloud provider, you should set the `NEXT_PUBLIC_RAG_API_URL` environment variable to the API URL of your LangConnect server. For local development, this should be set to:\n\n```bash\nNEXT_PUBLIC_RAG_API_URL=\"http://localhost:8080\"\n```\n\nOnce this is set, you can visit the `/rag` page in the web app to create your first collection, and upload documents to it.\n\nAfter setting up your RAG server, and configuring Open Agent Platform with an agent which can call it (like the Tools Agent), you can start to use it in your agents by selecting a collection when creating/editing an agent. This will give the agent access to a tool which can make requests to your RAG server, and retrieve relevant documents for the user's query. Since the RAG server is exposed to the agent as a tool, we recommend setting detailed descriptions on your collections when creating them, since these are the descriptions which will be attached to the tool.\n\n### Authentication\n\nAs stated above in the [RAG Authentication](#rag-authentication) section, we pass the Supabase JWT in the `Authorization` header of the request. Then, inside the LangConnect RAG server, we extract this token and verify it's valid with Supabase. If it is, we receive back a user ID, which is used to verify each user is *only* able to access their own collections. We do not currently support sharing collections between users. If this is something you are interested in, please reach out to me, either on [X (Twitter)](https://x.com/bracesproul), or email: `brace@langchain.dev`\n\n## MCP Server\n\n> [!TIP]\n> Open Agent Platform only supports connecting to MCP servers which support Streamable HTTP requests. As of **05/10/2025**, there are not many MCP servers which support this, which is why we've released the demo application connected to [Arcade's](https://arcade-ai.com/) MCP server.\n\nOpen Agent Platform was built with first class support for connecting agents to MCP servers which support Streamable HTTP requests. You can configure your MCP server in one of two ways.\n\nFirst, set your MCP server URL under the environment variable `NEXT_PUBLIC_MCP_SERVER_URL`. Ensure this URL does *not* end in `/mcp`, as the OAP web app will append this to the URL when making requests to the MCP server.\n\n### Authenticated Servers\n\nTo connect to an MCP server which requires authentication, you should set `NEXT_PUBLIC_MCP_AUTH_REQUIRED=true`. If this is set to `true`, OAP will route all requests to your MCP server through a proxy route in the web app's API routes. This pattern of using a proxy route is similar to the one outlined above for LangGraph server authentication via API keys. Essentially how this works is:\n\nThe web client makes a request to the proxy route (`/api/oap_mcp`). Inside this API route, we once again use Supabase's JWT to authenticate with the MCP server. This means you must implement an endpoint on your MCP server which allows for exchanging a Supabase JWT (or any other JWT if you choose to use a different authentication provider) for an MCP access token. This access token is then used to authenticate requests to the MCP server. After this exchange, we use the MCP access token to make requests to the MCP server on behalf of the user, passing it through the `Authorization` header of the request. When sending the request response back to the client, we include the MCP access token in the `x-mcp-access-token` header of the response. This allows the client to use the MCP access token in future requests to the MCP server, without having to authenticate with the MCP server each time. It's set to expire after one hour by default.\n\nThe URL set to `NEXT_PUBLIC_MCP_SERVER_URL` must be formatted so that the proxy route can append `/mcp` at the end to make requests to the MCP server, and `/oauth/token` at the end to make requests to the MCP server's OAuth token endpoint.\n\nOptionally, you can set the `MCP_TOKENS` environment variable to contain an object with an `access_token` field. If this environment variable is set, we will attempt to use that access token to authenticate requests to the MCP server. This is useful for testing, or if you want to use a different authentication provider than Supabase.\n\n### Unauthenticated Servers\n\nTo connect to an MCP server which does not require authentication, you should set the `NEXT_PUBLIC_MCP_SERVER_URL` environment variable to the URL of your MCP server. If this URL is set, and `NEXT_PUBLIC_MCP_AUTH_REQUIRED` is not set/not set to `true`, we will call your MCP server directly from the client.\n\n### Changing MCP Server URL\n\nIf you change the MCP server URL, you'll need to update all of your agents to use the new URL. We've included a script in this repo to do just that. This script can be found in [`apps/web/scripts/update-agents-mcp-url.ts`](apps/web/scripts/update-agents-mcp-url.ts).\n\nTo update your agent's MCP server URL, ensure the latest MCP server URL is set under the environment variable `NEXT_PUBLIC_MCP_SERVER_URL`, along with your deployments under `NEXT_PUBLIC_DEPLOYMENTS`, and a LangSmith API key under `LANGSMITH_API_KEY` (this is because the script uses LangSmith auth to authenticate with your LangGraph server, bypassing any user authentication). Then, run the script:\n\n```bash\n# Ensure you're inside the `apps/web` directory\n# cd apps/web\n\n# Run the script via TSX.\nnpx tsx scripts/update-agents-mcp-url.ts\n```\n\nThis will fetch every agent, from every deployment listed. It then checks to see if a given agent supports MCP servers. If it does it checks the MCP server URL is not already set to the new URL. If it is not, it updates the agent's config to use the new URL.\n\n# Building Your Own Agents\n\nWe built Open Agent Platform with custom agents in mind. Although we offer a few pre-built agents, we encourage you to build your own agents, and use OAP as a platform to prototype, test and use them! The following is a guide to help you build agents which are compatible with all of Open Agent Platform's features.\n\n## Platform\n\nOAP is built on top of LangGraph Platform, which means all agents which you build to be used in OAP must be LangGraph agents deployed on LangGraph Platform.\n\n## Configuration\n\nTo allow your agent to be configurable in OAP, you must set custom configuration metadata on your agent's configurable fields. There are currently three types of configurable fields:\n\n1. **General Agent Config:** This consists of general configuration settings like the model name, system prompt, temperature, etc. These are where essentially all of your custom configurable fields should go.\n2. **MCP Tools Config:** This is the config which defines the MCP server and tools to give your agent access to.\n3. **RAG Config:** This is the config which defines the RAG server, and collection name to give your agent access to.\n\n> [!TIP]\n> This section assumes you have a basic understanding of configurable fields in LangGraph. If you do not, read the LangGraph documentation ([Python](https://langchain-ai.github.io/langgraph/how-tos/graph-api/#add-runtime-configuration), [TypeScript](https://langchain-ai.github.io/langgraphjs/how-tos/configuration/)) for more information.\n\n### General Agent Config\n\nBy default, Open Agent Platform will show *all* fields listed in your configurable object as configurable in the UI. Each field will be configurable via a simple text input. To add more complex configurable field types (e.g boolean, dropdown, slider, etc), you should add a `x_oap_ui_config` object to `metadata` on the field. Inside this object is where you define the custom UI config for that specific field. The available options are:\n\n```typescript\nexport type ConfigurableFieldUIType =\n  | \"text\"\n  | \"textarea\"\n  | \"number\"\n  | \"boolean\"\n  | \"slider\"\n  | \"select\"\n  | \"json\";\n\n/**\n * The type interface for options in a select field.\n */\nexport interface ConfigurableFieldOption {\n  label: string;\n  value: string;\n}\n\n/**\n * The UI configuration for a field in the configurable object.\n */\nexport type ConfigurableFieldUIMetadata = {\n  /**\n   * The label of the field. This will be what is rendered in the UI.\n   */\n  label: string;\n  /**\n   * The default value to render in the UI component.\n   *\n   * @default undefined\n   */\n  default?: unknown;\n  /**\n   * The type of the field.\n   * @default \"text\"\n   */\n  type?: ConfigurableFieldUIType;\n  /**\n   * The description of the field. This will be rendered below the UI component.\n   */\n  description?: string;\n  /**\n   * The placeholder of the field. This will be rendered inside the UI component.\n   * This is only applicable for text, textarea, number, json, and select fields.\n   */\n  placeholder?: string;\n  /**\n   * The options of the field. These will be the options rendered in the select UI component.\n   * This is only applicable for select fields.\n   */\n  options?: ConfigurableFieldOption[];\n  /**\n   * The minimum value of the field.\n   * This is only applicable for number fields.\n   */\n  min?: number;\n  /**\n   * The maximum value of the field.\n   * This is only applicable for number fields.\n   */\n  max?: number;\n  /**\n   * The step value of the field. E.g if using a slider, where you want\n   * people to be able to increment by 0.1, you would set this field to 0.1\n   * This is only applicable for number fields.\n   */\n  step?: number;\n};\n```\n\nIn the examples below, we'll look at how to add configurable fields for `model_name`, `system_prompt`, `max_tokens`, and `temperature`, in both Python and TypeScript graphs. The same principals apply to any configurable field (which is not for MCP tools, or RAG).\n\n#### Python\n\n<details>\n<summary>In Python, this looks like:</summary>\n\n```python\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\n\nclass GraphConfigPydantic(BaseModel):\n    model_name: Optional[str] = Field(\n        default=\"anthropic:claude-3-7-sonnet-latest\",\n        metadata={\n            \"x_oap_ui_config\": {\n                \"type\": \"select\",\n                \"default\": \"anthropic:claude-3-7-sonnet-latest\",\n                \"description\": \"The model to use in all generations\",\n                \"options\": [\n                    {\n                        \"label\": \"Claude 3.7 Sonnet\",\n                        \"value\": \"anthropic:claude-3-7-sonnet-latest\",\n                    },\n                    {\n                        \"label\": \"Claude 3.5 Sonnet\",\n                        \"value\": \"anthropic:claude-3-5-sonnet-latest\",\n                    },\n                    {\"label\": \"GPT 4o\", \"value\": \"openai:gpt-4o\"},\n                    {\"label\": \"GPT 4o mini\", \"value\": \"openai:gpt-4o-mini\"},\n                    {\"label\": \"GPT 4.1\", \"value\": \"openai:gpt-4.1\"},\n                ],\n            }\n        }\n    )\n    temperature: Optional[float] = Field(\n        default=0.7,\n        metadata={\n            \"x_oap_ui_config\": {\n                \"type\": \"slider\",\n                \"default\": 0.7,\n                \"min\": 0,\n                \"max\": 2,\n                \"step\": 0.1,\n                \"description\": \"Controls randomness (0 = deterministic, 2 = creative)\",\n            }\n        }\n    )\n    max_tokens: Optional[int] = Field(\n        default=4000,\n        metadata={\n            \"x_oap_ui_config\": {\n                \"type\": \"number\",\n                \"default\": 4000,\n                \"min\": 1,\n                \"description\": \"The maximum number of tokens to generate\",\n            }\n        }\n    )\n    system_prompt: Optional[str] = Field(\n        default=None,\n        metadata={\n            \"x_oap_ui_config\": {\n                \"type\": \"textarea\",\n                \"placeholder\": \"Enter a system prompt...\",\n                \"description\": \"The system prompt to use in all generations\",\n            }\n        }\n    )\n\n# ENSURE YOU PASS THE GRAPH CONFIGURABLE SCHEMA TO THE StateGraph:\nworkflow = StateGraph(State, config_schema=GraphConfigPydantic)\n```\n\n</details>\n\n#### TypeScript\n\n> [!TIP]\n> In order for the Open Agent Platform to recognize & render your UI fields, your configuration object must be defined using the LangGraph Zod schema.\n\n<details>\n<summary>And in TypeScript, this looks like:</summary>\n\n```typescript\nimport \"@langchain/langgraph/zod\";\nimport { z } from \"zod\";\n\nexport const GraphConfiguration = z.object({\n  /**\n   * The model ID to use for the reflection generation.\n   * Should be in the format `provider/model_name`.\n   * Defaults to `anthropic/claude-3-7-sonnet-latest`.\n   */\n  modelName: z\n    .string()\n    .optional()\n    .langgraph.metadata({\n      x_oap_ui_config: {\n        type: \"select\",\n        default: \"anthropic/claude-3-7-sonnet-latest\",\n        description: \"The model to use in all generations\",\n        options: [\n          {\n            label: \"Claude 3.7 Sonnet\",\n            value: \"anthropic/claude-3-7-sonnet-latest\",\n          },\n          {\n            label: \"Claude 3.5 Sonnet\",\n            value: \"anthropic/claude-3-5-sonnet-latest\",\n          },\n          {\n            label: \"GPT 4o\",\n            value: \"openai/gpt-4o\",\n          },\n          {\n            label: \"GPT 4.1\",\n            value: \"openai/gpt-4.1\",\n          },\n          {\n            label: \"o3\",\n            value: \"openai/o3\",\n          },\n          {\n            label: \"o3 mini\",\n            value: \"openai/o3-mini\",\n          },\n          {\n            label: \"o4\",\n            value: \"openai/o4\",\n          },\n        ],\n      },\n    }),\n  /**\n   * The temperature to use for the reflection generation.\n   * Defaults to `0.7`.\n   */\n  temperature: z\n    .number()\n    .optional()\n    .langgraph.metadata({\n      x_oap_ui_config: {\n        type: \"slider\",\n        default: 0.7,\n        min: 0,\n        max: 2,\n        step: 0.1,\n        description: \"Controls randomness (0 = deterministic, 2 = creative)\",\n      },\n    }),\n  /**\n   * The maximum number of tokens to generate.\n   * Defaults to `1000`.\n   */\n  maxTokens: z\n    .number()\n    .optional()\n    .langgraph.metadata({\n      x_oap_ui_config: {\n        type: \"number\",\n        default: 4000,\n        min: 1,\n        description: \"The maximum number of tokens to generate\",\n      },\n    }),\n  systemPrompt: z\n    .string()\n    .optional()\n    .langgraph.metadata({\n      x_oap_ui_config: {\n        type: \"textarea\",\n        placeholder: \"Enter a system prompt...\",\n        description: \"The system prompt to use in all generations\",\n      },\n    }),\n});\n\n// ENSURE YOU PASS THE GRAPH CONFIGURABLE SCHEMA TO THE StateGraph:\nconst workflow = new StateGraph(MyStateSchema, GraphConfiguration)\n```\n\n</details>\n\n### MCP Tools Config\n\nTo enable support for MCP tools in your agents with Open Agent Platform, you must add a field in your configurable fields with the type `mcp`. This field can have *any* key you want, but the value must be an object with these two keys:\n\n- `url`: The URL of the MCP server.\n- `tools`: An array of tool names to give your agent access to.\n\n#### Python\n\n<details>\n<summary>In Python, this looks like:</summary>\n\n```python\nclass MCPConfig(BaseModel):\n    url: Optional[str] = Field(\n        default=None,\n        optional=True,\n    )\n    \"\"\"The URL of the MCP server\"\"\"\n    tools: Optional[List[str]] = Field(\n        default=None,\n        optional=True,\n    )\n    \"\"\"The tools to make available to the LLM\"\"\"\n\n\nclass GraphConfigPydantic(BaseModel):\n    # The key (in this case it's `mcp_config`)\n    # can be any value you want.\n    mcp_config: Optional[MCPConfig] = Field(\n        default=None,\n        metadata={\n            \"x_oap_ui_config\": {\n                # Ensure the type is `mcp`\n                \"type\": \"mcp\",\n                # Here is where you would set the default tools.\n                # \"default\": {\n                #     \"tools\": [\"Math_Divide\", \"Math_Mod\"]\n                # }\n            }\n        }\n    )\n```\n\n</details>\n\n#### TypeScript\n\n<details>\n<summary>And in TypeScript, this looks like:</summary>\n\n```typescript\nexport const MCPConfig = z.object({\n  /**\n   * The MCP server URL.\n   */\n  url: z.string(),\n  /**\n   * The list of tools to provide to the LLM.\n   */\n  tools: z.array(z.string()),\n});\n\nexport const GraphConfiguration = z.object({\n  /**\n   * MCP configuration for tool selection. The key (in this case it's `mcpConfig`)\n   * can be any value you want.\n   */\n  mcpConfig: z\n    .lazy(() => MCPConfig)\n    .optional()\n    .langgraph.metadata({\n      x_oap_ui_config: {\n        // Ensure the type is `mcp`\n        type: \"mcp\",\n        // Add custom tools to default to here:\n        // default: {\n        //   tools: [\"Math_Divide\", \"Math_Mod\"]\n        // }\n      },\n    }),\n});\n```\n\n</details>\n\n### RAG Config\n\nTo enable support for using a LangConnect RAG server in your LangGraph agent, you must define a configurable field similar to the MCP config, but with its own unique type of `rag`, and the following fields in the object:\n\n- `rag_url`: The URL of the LangConnect RAG server.\n- `collections`: A list of collection IDs, containing the IDs of the collections to give your agent access to.\n\n#### Python\n\n<details>\n<summary>In Python, this looks like:</summary>\n\n```python\nclass RagConfig(BaseModel):\n    rag_url: Optional[str] = None\n    \"\"\"The URL of the rag server\"\"\"\n    collections: Optional[List[str]] = None\n    \"\"\"The collections to use for rag. Will be a list of collection IDs\"\"\"\n\n\nclass GraphConfigPydantic(BaseModel):\n    # Once again, the key (in this case it's `rag`)\n    # can be any value you want.\n    rag: Optional[RagConfig] = Field(\n        default=None,\n        optional=True,\n        metadata={\n            \"x_oap_ui_config\": {\n                # Ensure the type is `rag`\n                \"type\": \"rag\",\n                # Here is where you would set the default collection. Use collection IDs\n                # \"default\": {\n                #     \"collections\": [\n                #         \"fd4fac19-886c-4ac8-8a59-fff37d2b847f\",\n                #         \"659abb76-fdeb-428a-ac8f-03b111183e25\",\n                #     ]\n                # },\n            }\n        }\n    )\n```\n\n</details>\n\n#### TypeScript\n\n<details>\n<summary>And in TypeScript, this looks like:</summary>\n\n```typescript\nexport const RAGConfig = z.object({\n  /**\n   * The LangConnect RAG server URL.\n   */\n  rag_url: z.string(),\n  /**\n   * The collections to use for RAG. Will be an\n   * array of collection IDs\n   */\n  collections: z.string().array(),\n});\n\nexport const GraphConfiguration = z.object({\n  /**\n   * LangConnect RAG configuration. The key (in this case it's `rag`)\n   * can be any value you want.\n   */\n  rag: z\n    .lazy(() => RAGConfig)\n    .optional()\n    .langgraph.metadata({\n      x_oap_ui_config: {\n        // Ensure the type is `rag`\n        type: \"rag\",\n        // Here is where you would set the default collection. Use collection IDs\n        // default: {\n        //   collections: [\n        //     \"fd4fac19-886c-4ac8-8a59-fff37d2b847f\",\n        //     \"659abb76-fdeb-428a-ac8f-03b111183e25\",\n        //   ]\n        // }\n      },\n    }),\n});\n```\n\n</details>\n\n### Hidden Configurable Fields\n\nYou can hide configurable fields from the UI by setting the `type` of the `x_oap_ui_config` metadata to `hidden`. This will only hide the field from the UI, but it can still be set in the runtime configuration.\n\n> [!WARNING]\n> This does _not_ fully hide the field from the user. It can still be found by inspecting network requests, so do not use this for sensitive information.\n\n#### Python\n\n<details>\n<summary>In Python, this looks like:</summary>\n\n```python\nclass GraphConfigPydantic(BaseModel):\n    hidden_field: Optional[str] = Field(\n        metadata={\n            \"x_oap_ui_config\": {\n                # Ensure the type is `hidden`\n                \"type\": \"hidden\",\n            }\n        }\n    )\n```\n\n</details>\n\n#### TypeScript\n\n<details>\n<summary>And in TypeScript, this looks like:</summary>\n\n```typescript\nexport const GraphConfiguration = z.object({\n  hidden_field: z\n    .string()\n    .optional()\n    .langgraph.metadata({\n      x_oap_ui_config: {\n        // Ensure the type is `hidden`\n        type: \"hidden\",\n      },\n    }),\n});\n```\n\n</details>\n\n# Concepts/FAQ\n\n## Concepts\n\n### Agents\n\nAn agent is a custom configuration on-top of an existing LangGraph graph. This is the same concept as an `assistant`, in the LangGraph API.\n\n## FAQ\n\n### Do I need a backend server?\n\nOAP does not require a standalone backend server to be running in order for the web app to work. As long as you've added deployments from LangGraph Platform to your instance, it should work as expected!\n\nHowever, if you want to use the RAG features, you will need to have the LangConnect server running on its own. This is because the LangConnect RAG server is intended to be hosted independently from your LangGraph deployments. See the [LangConnect docs](https://github.com/langchain-ai/langconnect/blob/main/README.md) for more information.\n\n### How can I build my own agents?\n\nYes! See the [Building Your Own Agents](#building-your-own-agents) section for more information.\n\n### How can I use non-supabase auth?\n\nYes! It requires some modifications to be made to the code, but we've implemented authentication in a way which makes it easy to swap out with any other authentication provider. \n\n### How can I use non-langgraph agents?\n\nNo. All agents you intend to use with OAP must be LangGraph agents, deployed on LangGraph Platform.\n\n### Why is my agent's config is only showing string inputs, and not custom fields?\n\nFirst, ensure you're using the latest version of LangGraph. If running locally, make sure you're using the latest version of the LangGraph API, and CLI packages. If deploying, make sure you've published a revision after 05/14/2025. Then, check that you have the `x_oap_ui_config` metadata set on your configurable fields. If you have, check that your configurable object is defined using LangGraph Zod (if using TypeScript), as this is required for the Open Agent Platform to recognize & render your UI fields.\n\nIf it's still not working, confirm your `x_oap_ui_config` metadata has the proper fields set.\n"
  },
  {
    "name": "simple-evals",
    "url": "https://github.com/davidatoms/simple-evals",
    "description": null,
    "type": "fork",
    "updated_at": "2025-05-13T23:04:35Z",
    "readme": "# Overview\nThis repository contains a lightweight library for evaluating language models.\nWe are open sourcing it so we can be transparent about the accuracy numbers we're publishing alongside our latest models.\n\n## Benchmark Results\n\n| Model                        | Prompt        | MMLU   | GPQA [^8]   | MATH [^6]| HumanEval | MGSM[^5] | DROP[^5]<br>(F1, 3-shot) | SimpleQA\n|:----------------------------:|:-------------:|:------:|:------:|:--------:|:---------:|:------:|:--------------------------:|:---------:|\n| **o3**                         |               |        |        |          |           |        |                             |                      |           |\n| o3-high [^10]                | n/a [^7]      |  93.3  |  83.4  |   98.1   |  88.4     |  92.0  |  89.8                      |  48.6     |\n| o3 [^9] [^10]                | n/a           |  92.9  |  82.8  |   97.8   |  87.4     |  92.3  |  80.6                      |  49.4     |\n| o3-low [^10]                 | n/a           |  92.8  |  78.6  |   96.9   |  87.3     |  91.9  |  82.3                      |  49.4     |\n| **o4-mini**                    |               |        |        |          |           |        |                             |                      |\n| o4-mini-high [^9] [^10]      | n/a           |  90.3  |  81.3  |   98.2   |  99.3     |  93.5  |  78.1                      |  19.3     |\n| o4-mini [^9] [^10]           | n/a           |  90.0  |  77.6  |   97.5   |  97.3     |  93.7  |  77.7                      |  20.2     |\n| o4-mini-low [^10]            | n/a           |  89.5  |  73.6  |   96.2   |  95.9     |  93.0  |  76.0                      |  20.2     |\n| **o3-mini**                    |               |        |        |          |           |        |                             |                      |           |\n| o3-mini-high                 | n/a           |  86.9  |  77.2  |   97.9   |  97.6     |  92.0  |  80.6                      |  13.8     |\n| o3-mini                      | n/a           |  85.9  |  74.9  |   97.3   |  96.3     |  90.8  |  79.2                      |  13.4     |\n| o3-mini-low                  | n/a           |  84.9  |  67.6  |   95.8   |  94.5     |  89.4  |  77.6                      |  13.0     |\n| **o1**                         |               |        |        |          |           |        |                             |                      |\n|  o1                          | n/a           |  91.8  |  75.7  |   96.4   |    -      |  89.3  |  90.2                      |  42.6     |\n| o1-preview                   | n/a           |  90.8  |  73.3  |   85.5   |  92.4     |  90.8  |  74.8                      |  42.4     |\n| o1-mini                      | n/a           |  85.2  |  60.0  |   90.0   |  92.4     |  89.9  |  83.9                      |  07.6     |\n| **GPT-4.1**                            |               |        |        |          |           |        |                             |                      |           |\n| gpt-4.1-2025-04-14           | assistant [^2]|  90.2  |  66.3  |   82.1   |   94.5    |  86.9  |  79.4                      | 41.6      |\n| gpt-4.1-mini-2025-04-14      | assistant     |  87.5  |  65.0  |   81.4   |   93.8    |  88.2  |  81.0                      | 16.8      |\n| gpt-4.1-nano-2025-04-14      | assistant     |  80.1  |  50.3  |   62.3   |   87.0    |  73.0  |  82.2                      | 07.6      |\n| **GPT-4o**                     |               |        |        |          |           |        |                             |                      |           |\n| gpt-4o-2024-11-20            | assistant     |  85.7  |  46.0  |   68.5   |   90.2    |  90.3  |  81.5                      | 38.8      |\n| gpt-4o-2024-08-06            | assistant     |  88.7  |  53.1  |   75.9   |   90.2    |  90.0  |  79.8                      | 40.1      |\n| gpt-4o-2024-05-13            | assistant     |  87.2  |  49.9  |   76.6   |   91.0    |  89.9  |  83.7                      | 39.0      |\n| gpt-4o-mini-2024-07-18       | assistant     |  82.0  |  40.2  |   70.2   |   87.2    |  87.0  |  79.7                      | 09.5      |\n| **GPT-4.5-preview**          |               |        |        |          |           |        |                            |           |\n| gpt-4.5-preview-2025-02-27   | assistant     |  90.8  |  69.5  |   87.1   |   88.6    |  86.9  |  83.4                      | 62.5      |\n| **GPT-4 Turbo and GPT-4**    |               |        |        |          |           |        |                            |           |\n| gpt-4-turbo-2024-04-09       | assistant     |  86.7  |  49.3  |   73.4   |   88.2    |  89.6  |  86.0                      | 24.2      |\n| gpt-4-0125-preview           | assistant     |  85.4  |  41.4  |   64.5   |   86.6    |  85.1  |  81.5                      | n/a       |\n| gpt-4-1106-preview           | assistant     |  84.7  |  42.5  |   64.3   |   83.7    |  87.1  |  83.2                      | n/a       |\n| **Other Models (Reported)**   |               |        |        |        |           |        |                           |\n| [Claude 3.5 Sonnet](https://www.anthropic.com/news/claude-3-5-sonnet) | unknown |  88.3  |  59.4  |  71.1  |   92.0    | 91.6 | 87.1 |  28.9 |\n| [Claude 3 Opus](https://www.anthropic.com/news/claude-3-family) | unknown |  86.8  |  50.4  |  60.1  |   84.9    |   90.7   |  83.1 |  23.5 |\n| [Llama 3.1 405b](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md) | unknown |  88.6  |  50.7  |  73.8  |   89.0    | 91.6 |  84.8                   | n/a\n| [Llama 3.1 70b](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md) | unknown |  82.0  |  41.7  |  68.0  |   80.5    |  86.9  |  79.6                   | n/a\n| [Llama 3.1 8b](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md) | unknown |  68.4  |  30.4  |  51.9  |   72.6    |  68.9  |  59.5                   | n/a\n| [Grok 2](https://x.ai/blog/grok-2) | unknown | 87.5 | 56.0 | 76.1 | 88.4 | n/a | n/a | n/a\n| [Grok 2 mini](https://x.ai/blog/grok-2) | unknown | 86.2 | 51.0 | 73.0 | 85.7 | n/a | n/a | n/a\n| [Gemini 1.0 Ultra](https://goo.gle/GeminiV1-5) | unknown | 83.7 | n/a | 53.2 | 74.4 | 79.0 | 82.4 | n/a\n| [Gemini 1.5 Pro](https://goo.gle/GeminiV1-5) | unknown | 81.9 | n/a | 58.5 | 71.9 | 88.7 | 78.9 | n/a\n| [Gemini 1.5 Flash](https://goo.gle/GeminiV1-5) | unknown | 77.9 | 38.6 | 40.9 | 71.5 | 75.5 | 78.4 | n/a\n\n## Background\n\nEvals are sensitive to prompting, and there's significant variation in the formulations used in recent publications and libraries.\nSome use few-shot prompts or role playing prompts (\"You are an expert software programmer...\").\nThese approaches are carryovers from evaluating *base models* (rather than instruction/chat-tuned models) and from models that were worse at following instructions.\n\nFor this library, we are emphasizing the *zero-shot, chain-of-thought* setting, with simple instructions like \"Solve the following multiple choice problem\". We believe that this prompting technique is a better reflection of the models' performance in realistic usage.\n\n**We will not be actively maintaining this repository and monitoring PRs and Issues.** In particular, we're not accepting new evals. Here are the changes we might accept.\n- Bug fixes (hopefully not needed!)\n- Adding adapters for new models\n- Adding new rows to the table below with eval results, given new models and new system prompts.\n\nThis repository is NOT intended as a replacement for https://github.com/openai/evals, which is designed to be a comprehensive collection of a large number of evals.\n\n## Evals\n\nThis repository currently contains the following evals:\n\n- MMLU: Measuring Massive Multitask Language Understanding, reference: https://arxiv.org/abs/2009.03300, https://github.com/hendrycks/test, [MIT License](https://github.com/hendrycks/test/blob/master/LICENSE)\n- MATH: Measuring Mathematical Problem Solving With the MATH Dataset, reference: https://arxiv.org/abs/2103.03874, https://github.com/hendrycks/math, [MIT License](https://github.com/idavidrein/gpqa/blob/main/LICENSE)\n- GPQA: A Graduate-Level Google-Proof Q&A Benchmark, reference: https://arxiv.org/abs/2311.12022, https://github.com/idavidrein/gpqa/,  [MIT License](https://github.com/idavidrein/gpqa/blob/main/LICENSE)\n- DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs, reference: https://arxiv.org/abs/1903.00161, https://allenai.org/data/drop, [Apache License 2.0](https://github.com/allenai/allennlp-models/blob/main/LICENSE)\n- MGSM: Multilingual Grade School Math Benchmark (MGSM), Language Models are Multilingual Chain-of-Thought Reasoners, reference: https://arxiv.org/abs/2210.03057, https://github.com/google-research/url-nlp, [Creative Commons Attribution 4.0 International Public License (CC-BY)](https://github.com/google-research/url-nlp/blob/main/LICENSE)\n- HumanEval: Evaluating Large Language Models Trained on Code, reference https://arxiv.org/abs/2107.03374, https://github.com/openai/human-eval, [MIT License](https://github.com/openai/human-eval/blob/master/LICENSE)\n- SimpleQA: Measuring short-form factuality in large language models, reference: https://openai.com/index/introducing-simpleqa, [MIT License](https://github.com/openai/simple-evals/blob/main/LICENSE)\n- BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents, reference: https://openai.com/index/browsecomp, [MIT License](https://github.com/openai/simple-evals/blob/main/LICENSE)\n- HealthBench: Evaluating Large Language Models Towards Improved Human Health, reference: https://openai.com/index/healthbench, [MIT License](https://github.com/openai/simple-evals/blob/main/LICENSE)\n\n## Samplers\n\nWe have implemented sampling interfaces for the following language model APIs:\n\n- OpenAI: https://platform.openai.com/docs/overview\n- Claude: https://www.anthropic.com/api\n\nMake sure to set the `*_API_KEY` environment variables before using these APIs.\n\n## Setup\n\nDue to the optional dependencies, we're not providing a unified setup mechanism. Instead, we're providing instructions for each eval and sampler.\n\nFor [HumanEval](https://github.com/openai/human-eval/) (python programming)\n```bash\ngit clone https://github.com/openai/human-eval\npip install -e human-eval\n```\n\nFor the [OpenAI API](https://pypi.org/project/openai/):\n```bash\npip install openai\n```\n\nFor the [Anthropic API](https://docs.anthropic.com/claude/docs/quickstart-guide):\n```bash\npip install anthropic\n```\n\n## Running the evals\n```bash\npython -m simple-evals.simple_evals --list-models\n```\nThis will list all the models that you can evaluate.\n\nTo run the evaluations, you can use the following command:\n```bash\npython -m simple-evals.simple_evals --model <model_name> --examples <num_examples>\n```\nThis will launch evaluations through the OpenAI API.\n\n## Notes\n\n[^1]:chatgpt system message: \"You are ChatGPT, a large language model trained by OpenAI, based on the GPT-4 architecture.\\nKnowledge cutoff: 2023-12\\nCurrent date: 2024-04-01\"\n[^2]:assistant system message in [OpenAI API doc](https://platform.openai.com/docs/api-reference/introduction): \"You are a helpful assistant.\" .\n[^3]:claude-3 empty system message: suggested by Anthropic API doc, and we have done limited experiments due to [rate limit](https://docs.anthropic.com/claude/reference/rate-limits) issues, but we welcome PRs with alternative choices.\n[^4]:claude-3 lmsys system message: system message in LMSYS [Fast-chat open source code](https://github.com/lm-sys/FastChat/blob/7899355ebe32117fdae83985cf8ee476d2f4243f/fastchat/conversation.py#L894): \"The assistant is Claude, created by Anthropic. The current date is {{currentDateTime}}. Claude's knowledge base was last updated ... \". We have done limited experiments due to [rate limit](https://docs.anthropic.com/claude/reference/rate-limits) issues, but we welcome PRs with alternative choices.\n[^5]:We believe these evals are saturated for our newer models, but are reporting them for completeness.\n[^6]:For newer models (anything on or after o1) we evaluate on [MATH-500](https://github.com/openai/prm800k/tree/main/prm800k/math_splits), which is a newer, IID version of MATH.\n[^7]:o-series models do not support using a system prompt.\n[^8]:Includes an answer regex tweak for GPQA benchmark.\n[^9]:The default reasoning level for o3-mini is \"medium\".\n[^10]:These results are with no tools enabled for o3 or o4-mini\n\n## Legal Stuff\nBy contributing to evals, you are agreeing to make your evaluation logic and data under the same MIT license as this repository. You must have adequate rights to upload any data used in an eval. OpenAI reserves the right to use this data in future service improvements to our product. Contributions to OpenAI evals will be subject to our usual Usage Policies: https://platform.openai.com/docs/usage-policies.\n"
  },
  {
    "name": "build_tools",
    "url": "https://github.com/davidatoms/build_tools",
    "description": "Used to build ONLYOFFICE DocumentServer-related products",
    "type": "fork",
    "updated_at": "2025-05-13T12:47:17Z",
    "readme": "# build_tools\n\n## Overview\n\n**build_tools** allow you to automatically get and install all the components\nnecessary for the compilation process, all the dependencies required for the\n**ONLYOFFICE Document Server**, **Document Builder** and **Desktop Editors**\n correct work, as well as to get the latest version of\n**ONLYOFFICE products** source code and build all their components.\n\n**Important!**  We can only guarantee the correct work of the products built from\nthe `master` branch.\n\n## How to use - Linux\n\n**Note**: The solution has been tested on **Ubuntu 16.04**.\n\n### Installing dependencies\n\nYou might need to install **Python**, depending on your version of Ubuntu:\n\n```bash\nsudo apt-get install -y python\n```\n\n### Building ONLYOFFICE products source code\n\n1. Clone the build_tools repository:\n\n    ```bash\n    git clone https://github.com/ONLYOFFICE/build_tools.git\n    ```\n\n2. Go to the `build_tools/tools/linux` directory:\n\n    ```bash\n    cd build_tools/tools/linux\n    ```\n\n3. Run the `automate.py` script:\n\n    ```bash\n    ./automate.py\n    ```\n\nIf you run the script without any parameters this allows to build **ONLYOFFICE\nDocument Server**, **Document Builder** and **Desktop Editors**.\n\nThe result will be available in the `./out` directory.\n\nTo build **ONLYOFFICE** products separately run the script with the parameter\ncorresponding to the necessary product.\n\nIt\u2019s also possible to build several products at once as shown in the example\nbelow.\n\n**Example**: Building **Desktop Editors** and **Document Server**\n\n```bash\n./automate.py desktop server\n```\n\n### Using Docker\n\nYou can also build all **ONLYOFFICE products** at once using Docker.\nBuild the `onlyoffice-document-editors-builder` Docker image using the\nprovided `Dockerfile` and run the corresponding Docker container.\n\n```bash\nmkdir out\ndocker build --tag onlyoffice-document-editors-builder .\ndocker run -v $PWD/out:/build_tools/out onlyoffice-document-editors-builder\n```\n\nThe result will be available in the `./out` directory.\n\n### Building and running ONLYOFFICE products separately\n\n#### Document Builder\n\n##### Building Document Builder\n\n```bash\n./automate.py builder\n```\n\n##### Running Document Builder\n\n```bash\ncd ../../out/linux_64/onlyoffice/documentbuilder\n./docbuilder\n```\n\n#### Desktop Editors\n\n##### Building Desktop Editors\n\n```bash\n./automate.py desktop\n```\n\n##### Running Desktop Editors\n\n```bash\ncd ../../out/linux_64/onlyoffice/desktopeditors\nLD_LIBRARY_PATH=./ ./DesktopEditors\n```\n\n#### Document Server\n\n##### Building Document Server\n\n```bash\n./automate.py server\n```\n\n##### Installing and configuring Document Server dependencies\n\n**Document Server** uses **NGINX** as a web server and **PostgreSQL** as a database.\n**RabbitMQ** is also required for **Document Server** to work correctly.\n\n###### Installing and configuring NGINX\n\n1. Install NGINX:\n\n    ```bash\n    sudo apt-get install nginx\n    ```\n\n2. Disable the default website:\n\n    ```bash\n    sudo rm -f /etc/nginx/sites-enabled/default\n    ```\n\n3. Set up the new website. To do that create the `/etc/nginx/sites-available/onlyoffice-documentserver`\n   file with the following contents:\n\n    ```bash\n    map $http_host $this_host {\n      \"\" $host;\n      default $http_host;\n    }\n    map $http_x_forwarded_proto $the_scheme {\n      default $http_x_forwarded_proto;\n      \"\" $scheme;\n    }\n    map $http_x_forwarded_host $the_host {\n      default $http_x_forwarded_host;\n      \"\" $this_host;\n    }\n    map $http_upgrade $proxy_connection {\n      default upgrade;\n      \"\" close;\n    }\n    proxy_set_header Host $http_host;\n    proxy_set_header Upgrade $http_upgrade;\n    proxy_set_header Connection $proxy_connection;\n    proxy_set_header X-Forwarded-Host $the_host;\n    proxy_set_header X-Forwarded-Proto $the_scheme;\n    server {\n      listen 0.0.0.0:80;\n      listen [::]:80 default_server;\n      server_tokens off;\n      rewrite ^\\/OfficeWeb(\\/apps\\/.*)$ /web-apps$1 redirect;\n      location / {\n        proxy_pass http://localhost:8000;\n        proxy_http_version 1.1;\n      }\n    }\n    ```\n\n4. Add the symlink to the newly created website to the\n   `/etc/nginx/sites-available` directory:\n\n    ```bash\n    sudo ln -s /etc/nginx/sites-available/onlyoffice-documentserver /etc/nginx/sites-enabled/onlyoffice-documentserver\n    ```\n\n5. Restart NGINX to apply the changes:\n\n    ```bash\n    sudo nginx -s reload\n    ```\n\n###### Installing and configuring PostgreSQL\n\n1. Install PostgreSQL:\n\n    ```bash\n    sudo apt-get install postgresql\n    ```\n\n2. Create the PostgreSQL database and user:\n\n    **Note**: The created database must have **onlyoffice** both for user and password.\n\n    ```bash\n    sudo -i -u postgres psql -c \"CREATE USER onlyoffice WITH PASSWORD 'onlyoffice';\"\n    sudo -i -u postgres psql -c \"CREATE DATABASE onlyoffice OWNER onlyoffice;\"\n    ```\n\n3. Configure the database:\n\n    ```bash\n    psql -hlocalhost -Uonlyoffice -d onlyoffice -f ../../out/linux_64/onlyoffice/documentserver/server/schema/postgresql/createdb.sql\n    ```\n\n**Note**: Upon that, you will be asked to provide a password for the **onlyoffice**\nPostgreSQL user. Please enter the **onlyoffice** password.\n\n###### Installing RabbitMQ\n\n```bash\nsudo apt-get install rabbitmq-server\n```\n\n###### Generate fonts data\n\n```bash\ncd out/linux_64/onlyoffice/documentserver/\nmkdir fonts\nLD_LIBRARY_PATH=${PWD}/server/FileConverter/bin server/tools/allfontsgen \\\n  --input=\"${PWD}/core-fonts\" \\\n  --allfonts-web=\"${PWD}/sdkjs/common/AllFonts.js\" \\\n  --allfonts=\"${PWD}/server/FileConverter/bin/AllFonts.js\" \\\n  --images=\"${PWD}/sdkjs/common/Images\" \\\n  --selection=\"${PWD}/server/FileConverter/bin/font_selection.bin\" \\\n  --output-web='fonts' \\\n  --use-system=\"true\"\n```\n\n###### Generate presentation themes\n\n```bash\ncd out/linux_64/onlyoffice/documentserver/\nLD_LIBRARY_PATH=${PWD}/server/FileConverter/bin server/tools/allthemesgen \\\n  --converter-dir=\"${PWD}/server/FileConverter/bin\"\\\n  --src=\"${PWD}/sdkjs/slide/themes\"\\\n  --output=\"${PWD}/sdkjs/common/Images\"\n```\n\n##### Running Document Server\n\n**Note**: All **Document Server** components run as foreground processes. Thus\nyou need separate terminal consoles to run them or specific tools which will\nallow to run foreground processes in background mode.\n\n1. Start the **FileConverter** service:\n\n    ```bash\n    cd out/linux_64/onlyoffice/documentserver/server/FileConverter\n    LD_LIBRARY_PATH=$PWD/bin \\\n    NODE_ENV=development-linux \\\n    NODE_CONFIG_DIR=$PWD/../Common/config \\\n    ./converter\n    ```\n\n2. Start the **DocService** service:\n\n    ```bash\n    cd out/linux_64/onlyoffice/documentserver/server/DocService\n    NODE_ENV=development-linux \\\n    NODE_CONFIG_DIR=$PWD/../Common/config \\\n    ./docservice\n    ```\n"
  },
  {
    "name": "davidatoms",
    "url": "https://github.com/davidatoms/davidatoms",
    "description": "Github Readme Profile",
    "type": "original",
    "updated_at": "2025-05-13T12:02:44Z",
    "readme": "# David Adams Automatic GitHub Readme\n\n<p align=\"left\"><b>Last Updated:</b> <!-- last_updated starts -->May 13, 2025 at 12:02 (133/365 (0.364) of the year)<!-- last_updated ends -->\n</p>\n\n<p align=\"left\">\n  <img src=\"https://img.shields.io/badge/Python-3776AB?style=flat&logo=python&logoColor=white\" />\n  <img src=\"https://img.shields.io/badge/Go-00ADD8?style=flat&logo=go&logoColor=white\" />\n  <img src=\"https://img.shields.io/badge/Rust-000000?style=flat&logo=rust&logoColor=white\" />\n  <img src=\"https://img.shields.io/badge/React-20232A?style=flat&logo=react&logoColor=61DAFB\" />\n  <img src=\"https://img.shields.io/badge/Bash-4EAA25?style=flat&logo=gnu-bash&logoColor=white\" />\n</p>\n\nI am passionate about a better future through innovation and investments. \n\n## Recent Repositories\n<!-- recent_repos starts -->\n[**DesktopEditors**](https://github.com/davidatoms/DesktopEditors) - ONLYOFFICE Desktop Editors is a free office suite combining text, spreadsheet, and presentation editors for creating, viewing, and editing Office Open XML documents locally without an internet connection.\n\n[**script-lab**](https://github.com/davidatoms/script-lab) - Script Lab is a tool for experimenting with the Office JavaScript API and quickly prototyping Office Add-ins without leaving Excel, Outlook, Word, or PowerPoint.\n\n[**codex**](https://github.com/davidatoms/codex) - Codex is a lightweight coding agent that runs in your terminal, providing ChatGPT-level reasoning and the ability to execute code, manipulate files, and iterate on changes with version control.\n\n[**zero**](https://github.com/davidatoms/zero) - Zero is an open-source AI-powered email solution that allows users to self-host their own email app, integrate with external providers like Gmail, and leverage AI agents to modernize the email experience.\n\n[**void**](https://github.com/davidatoms/void) - Void is an open-source Cursor alternative that allows you to use AI agents on your codebase, checkpoint and visualize changes, and bring any model or host locally without retaining your data.\n\n[**Thinking-Claude**](https://github.com/davidatoms/Thinking-Claude) - Thinking Claude is a project that enhances Claude's reasoning and transparency by providing instructions for in-depth thinking and a browser extension for neatly organizing Claude's thought process.\n\n[**cursor-talk-to-figma-mcp**](https://github.com/davidatoms/cursor-talk-to-figma-mcp) - Cursor Talk To Figma MCP\n\n[**arxiv-mcp-server**](https://github.com/davidatoms/arxiv-mcp-server) - The ArXiv MCP Server enables AI assistants to search and access arXiv research papers through a simple Model Context Protocol (MCP) interface, providing seamless integration with arXiv's repository.\n\n[**python-sdk**](https://github.com/davidatoms/python-sdk) - The MCP Python SDK is a Python implementation of the Model Context Protocol (MCP), enabling developers to build MCP servers that expose data, functionality, and interaction patterns to LLM applications in a standardized way.\n\n[**mcp-agent**](https://github.com/davidatoms/mcp-agent) - mcp-agent is a framework for building composable AI agents using Model Context Protocol, implementing patterns like Augmented LLM, Parallel, Router, Intent-Classifier, and OpenAI's Swarm.\n<!-- recent_repos ends -->\n\n<br>\n\n![Star this repository](https://img.shields.io/badge/Star%20this%20repository-FFDD00?style=flat&logo=github&logoColor=white)\n![Profile Views](https://komarev.com/ghpvc/?username=davidatoms&style=flat&color=blue&label=Views)\n"
  }
]