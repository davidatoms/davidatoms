[
  {
    "name": "DesktopEditors",
    "url": "https://github.com/davidatoms/DesktopEditors",
    "description": "Open-source office suite pack that comprises all the tools you need to work with documents, spreadsheets, presentations, PDFs, and PDF forms on Windows, Linux, and macOS",
    "type": "fork",
    "updated_at": "2025-05-13T11:59:38Z",
    "readme": "[![License](https://img.shields.io/badge/License-GNU%20AGPL%20V3-green.svg?style=flat)](https://www.gnu.org/licenses/agpl-3.0.en.html)\n![Platforms Windows | macOS | Linux](https://img.shields.io/badge/Platforms-Windows%20%7C%20macOS%20%7C%20Linux-lightgrey.svg?style=flat) ![Release](https://img.shields.io/badge/Release-v8.3.0-blue.svg?style=flat)\n\n## Overview\n\n[ONLYOFFICE Desktop Editors][4] is a free office suite that combines text, spreadsheet and presentation editors allowing to create, view and edit documents stored on your Windows/Linux PC or Mac without an Internet connection. It is fully compatible with Office Open XML formats: .docx, .xlsx, .pptx.\n\n## Components\n\nONLYOFFICE Desktop Editors contain the following components:\n\n* [desktop-apps](https://github.com/ONLYOFFICE/desktop-apps) - the frontend for ONLYOFFICE Desktop Editors which is used to build the program interface for the operating system selected.\n* [desktop-sdk](https://github.com/ONLYOFFICE/desktop-sdk) - SDK which is a core part of ONLYOFFICE Desktop Editors.\n* [core](https://github.com/ONLYOFFICE/core) - server core components for [ONLYOFFICE Document Server][2] which is a part of ONLYOFFICE Desktop Editors and is used to enable the conversion between the most popular office document formats (DOC, DOCX, ODT, RTF, TXT, PDF, HTML, EPUB, XPS, DjVu, XLS, XLSX, ODS, CSV, PPT, PPTX, ODP).\n* [sdkjs](https://github.com/ONLYOFFICE/sdkjs) - JavaScript SDK for the [ONLYOFFICE Document Server][2] which is a part of ONLYOFFICE Desktop Editors and contains API for all the included components client-side interaction.\n* [web-apps](https://github.com/ONLYOFFICE/web-apps) - the frontend for [ONLYOFFICE Document Server][2] which is a part of ONLYOFFICE Desktop Editors that allows the user to create, edit, save and export text, spreadsheet and presentation documents using the common interface of a document editor.\n* [dictionaries](https://github.com/ONLYOFFICE/dictionaries) - the dictionaries of various languages used for spellchecking in ONLYOFFICE Desktop Editors.\n\n## Plugins\n\nONLYOFFICE Desktop Editors offer support for plugins allowing developers to add specific features to the editors that are not directly related to the OOXML format. For more information see [our API](https://api.onlyoffice.com/plugin/basic) or visit github [plugins repo](https://github.com/ONLYOFFICE/onlyoffice.github.io).\n\n## Functionality\n\nONLYOFFICE Desktop Editors include the following editors:\n\n* [ONLYOFFICE Document Editor](https://www.onlyoffice.com/document-editor.aspx?utm_source=GitHub&utm_medium=social&utm_campaign=GitHubDesktop)\n* [ONLYOFFICE Spreadsheet Editor](https://www.onlyoffice.com/spreadsheet-editor.aspx?utm_source=GitHub&utm_medium=social&utm_campaign=GitHubDesktop)\n* [ONLYOFFICE Presentation Editor](https://www.onlyoffice.com/presentation-editor.aspx?utm_source=GitHub&utm_medium=social&utm_campaign=GitHubDesktop)\n* [ONLYOFFICE Form Creator](https://www.onlyoffice.com/form-creator.aspx?utm_source=GitHub&utm_medium=social&utm_campaign=GitHubDesktop)\n* [ONLYOFFICE PDF editor, reader & converter](https://www.onlyoffice.com/pdf-reader.aspx?utm_source=GitHub&utm_medium=social&utm_campaign=GitHubDesktop)\n\nThe editors allow you to create, edit, save and export text, spreadsheet and presentation documents.\n\n## Installation methods\n\n* Deb, rpm, exe, dmg on the [official website](https://www.onlyoffice.com/download-desktop.aspx?utm_source=GitHub&utm_medium=social&utm_campaign=GitHubDesktop)\n* Snap package. Get on [snapcraft.io](https://snapcraft.io/onlyoffice-desktopeditors). The official [source code](https://github.com/ONLYOFFICE/snap-desktopeditors) for ONLYOFFICE Desktop Editors Snap package\n* Flatpak. Get on [flathub.org](https://flathub.org/apps/details/org.onlyoffice.desktopeditors). The official [source code](https://github.com/flathub/org.onlyoffice.desktopeditors) for ONLYOFFICE Desktop Editors Flatpak\n* AppImage.  Get on [AppImageHub](https://appimage.github.io/ONLYOFFICE/). The official [source code](https://github.com/ONLYOFFICE/appimage-desktopeditors) for ONLYOFFICE Desktop Editors AppImage\n\n## License\n\nONLYOFFICE Desktop Editors is licensed under the GNU Affero Public License, version 3.0. See [LICENSE](https://onlyo.co/38YZGJh) for more information.\n\n## How to Build\n\nInstructions for building ONLYOFFICE Desktop Editors are in [build_tools](https://github.com/ONLYOFFICE/build_tools#desktop-editors).\n\n## User Feedback and Support\n\nIf you have any problems with or questions about ONLYOFFICE Desktop Editors, please visit our official forum to find answers to your questions: [forum.onlyoffice.com][1] or you can ask and answer ONLYOFFICE development questions on [Stack Overflow][3].\n\n  [1]: https://forum.onlyoffice.com\n  [2]: https://github.com/ONLYOFFICE/DocumentServer\n  [3]: https://stackoverflow.com/questions/tagged/onlyoffice\n  [4]: https://www.onlyoffice.com/desktop.aspx?utm_source=github&utm_medium=cpc&utm_campaign=GitHubDesktop\n"
  },
  {
    "name": "script-lab",
    "url": "https://github.com/davidatoms/script-lab",
    "description": "[Archived - see the readme for more info] Create, run, and share your code directly from Office",
    "type": "fork",
    "updated_at": "2025-05-12T02:05:53Z",
    "readme": "# Script Lab, a Microsoft Garage project\n\nExperiment with the Office JavaScript API without leaving Excel, Outlook, Word, or PowerPoint!\n\n> [!IMPORTANT]\n> This repo has been archived because the codebase was moved to a Microsoft-internal source. However, the Script Lab tool is still available.\n\n**Get [Script Lab](https://appsource.microsoft.com/product/office/WA104380862) in Excel, Word, and PowerPoint, or [Script Lab for Outlook](https://appsource.microsoft.com/product/office/WA200001603), free from Microsoft AppSource**.\n\n[Read the blog post](https://devblogs.microsoft.com/microsoft365dev/update-on-the-future-of-the-script-lab-office-add-in/) on the future of Script Lab.\n\n## Topics\n\n- [What is Script Lab?](#what-is-script-lab)\n- [Get Started](#get-started)\n- [Import someone else's snippet, or export your own](#import-someone-elses-snippet-or-export-your-own)\n- [Report a bug, or suggest a feature](#report-a-bug-or-suggest-a-feature)\n- [Stay up-to-date](#stay-up-to-date)\n- [Contribute to Script Lab](#contribute-to-script-lab)\n- [Rate and review](#rate-and-review)\n- [Articles & FAQs](#articles--faqs)\n- [External media coverage](#external-media-coverage)\n- [Code of Conduct](#code-of-conduct)\n\n## What is Script Lab?\n\nWouldn't it be crazy if you could launch Excel, click to open a small code window, and then instantly start writing and executing JavaScript that interacts with your spreadsheet?\n\nScript lab is a tool for anyone who wants to learn about writing Office Add-ins for Excel, Outlook, Word, or PowerPoint. The focus is the Office JavaScript API, which is the technology you need for building Office Add-ins that run across platforms. Maybe you're an experienced Office developer and you want to quickly prototype a feature for your add-in. Or maybe you've never tried writing code for Office and you just want to play with a sample and tweak it to learn more. Either way, Script Lab is for you.\n\nScript Lab has three main features:\n\n- **Code** in a pane beside your spreadsheet.\n    - IntelliSense is there while you type so you can easily discover and use the Office JavaScript objects and methods. Script Lab uses the Monaco editor, the same tech that powers VS Code, so it's beautiful and lightweight.\n    - Samples are pre-installed with Script Lab, so you don't have to start from scratch\n    - Your can use any TypeScript features like arrow functions, template strings, and async/await (i.e., a good chunk of ES6 and ES7 features). But it's not only script: your snippets can also use HTML, CSS, and references to external libraries and data on the web.\n- **Run** the code in another pane beside the editor.\n    - Execution can include logic, API calls to Office, UI in the pane, and even output to a console.\n    - Every time you make a code change you can refresh the editor and run the new version in seconds.\n- **Share** your snippets.\n    - If you create a snippet you'd like to share, you can copy it to your clipboard, save it as a GitHub gist. Then send the gist link to someone else to use on their computer.\n    - The Import feature lets you load other people's snippets.\n\nScript Lab works in Office 2013 or later on Windows, Office on the web, or Office on Mac.\n\nYou can get [Script Lab](https://appsource.microsoft.com/product/office/WA104380862) right now for free from Microsoft AppSource! It works for Excel, Word, and PowerPoint.\n\nIf you're interested in developing for Outlook, you can get [Script Lab for Outlook](https://appsource.microsoft.com/product/office/WA200001603), also free from AppSource.\n\nScript Lab is a Microsoft Garage project that began at a hackathon. You can read the story of the original Script Lab creation on the [Garage website](https://www.microsoft.com/garage/profiles/script-lab/).\n\nHere's a 1-minute teaser video:\n\n[![Script Lab teaser video showing Script Lab being used in Excel to make charts, Word, and Powerpoint Online.](.github/images/screenshot-wide-youtube.png 'Script Lab teaser video')](https://aka.ms/scriptlabvideo)\n\n## Get Started\n\n[Explore Office JavaScript API using Script Lab](https://learn.microsoft.com/en-us/office/dev/add-ins/overview/explore-with-script-lab)\n\nThis 10-minute demo explains how to use the main features:\n\n[![Michael Saunders demos Script Lab](.github/images/demoscreenshot-youtube.png 'Michael Saunders demos Script Lab')](https://youtu.be/V85_97G7VA4)\n\n## Import someone else's snippet, or export your own\n\nScript Lab is built around sharing. If someone gives you a URL to a GitHub Gist, simply open Script Lab, use the hamburger menu at the top left to see the menu, and choose \"Import\" category (either on the left or top, depending on the available screen space). Then, enter the URL of the Gist, and click the \"Import\" button at the bottom of the screen. In just these few clicks, you will be able to view and run someone else's snippet!\n\n![Import tab within the \"Hamburger\" menu showing a text box to import a URL, GitHub gist ID, or snippet YAML](.github/images/import-snippet.jpg)\n\nConversely, to share _your_ snippet with someone, choose the \"Copy to clipboard\" button with the open snippet. You can share as a public [GitHub Gist](https://help.github.com/articles/about-gists/), or you can paste the snippet from the clipboard, and share it from there.\n\n### Size restrictions\n\nScript Lab is designed for you to play with small code samples. Generally, a snippet should be at most a few hundred lines and a few thousand characters.\n\nYour snippet can use hard-coded data. A small amount of data (say, a few hundred characters) is OK to hard code in Script Lab. However, for larger pieces of data, we recommend that you store those externally then load them at runtime with a command like `fetch`.\n\nKeep your snippets and hard-coded data small since storing several large snippets could exceed Script Lab's storage and cause issues when loading Script Lab.\n\n## Report a bug, or suggest a feature\n\nTo report a bug, [create a new issue](https://github.com/OfficeDev/office-js/issues). Please provide as much detail as you can, tell us: the operating system, the Office build number, and your browser (if you're using Office on the web).\n\nIf you have a suggestion for a feature, please feel free to file it under \"issues\" as well, and we will tag it appropriately. The more detail, the better! (see more at [CONTRIBUTING.md](CONTRIBUTING.md)).\n\nIf you have a **question**, please ask it on [Stack Overflow](https://stackoverflow.com/questions/tagged/office-js) and tag your questions with `office-js` and `scriptlab`.\n\n## Stay up-to-date\n\n- Follow [@OfficeDev](https://twitter.com/OfficeDev) on Twitter\n- Follow [Script Lab](https://medium.com/script-lab) articles on [medium.com](https://medium.com/script-lab)\n- Join our Office Developer program at [developer.microsoft.com](https://developer.microsoft.com/microsoft-365/dev-program)\n\n## Contribute to Script Lab\n\nThere are a bunch of ways you can contribute to Script Lab:\n\n- File bugs & suggestions (see [Report a bug, or suggest a feature](#report-a-bug-or-suggest-a-feature)).\n- Contribute new samples, or improve existing one. Please submit a pull request to the [office-js-snippets repo](https://github.com/OfficeDev/office-js-snippets/pulls); more info in the [README](https://github.com/OfficeDev/office-js-snippets/blob/master/README.md) of that repo.\n- Spread the word! Whether through writing a blog post ([examples](#external-media-coverage)), recording a video, tweeting about us, or sharing snippets with colleagues or the [StackOverflow](https://stackoverflow.com/questions/tagged/office-js) community -- we want more of the world to use Script Lab!\n- Help improve the documentation. If you feel like this README or the [CONTRIBUTING.md doc](CONTRIBUTING.md) could use more details, please send a pull request!\n\n## Rate and review\n\nLeave a star-rating and (optionally) a review blurb for Script Lab on the [Office Store review page](https://store.office.com/writereview.aspx?assetid=WA104380862).\n\nWe'd prefer you [report issues on GitHub](https://github.com/OfficeDev/script-lab/issues/new).\n\n## Articles & FAQs\n\n- Announcing Script Lab React (January 1, 2019): [Official blog post](https://developer.microsoft.com/office/blogs/announcing-script-lab-react/)\n- Script Lab overview: [\"You can write JavaScript in Excel!\"](https://medium.com/script-lab/you-can-write-javascript-in-excel-4ba588a948bd)\n- Project history & the technology behind it: [Episode 127 on the Office 365 Developer Podcast](https://www.microsoft.com/microsoft-365/blog/2017/04/20/episode-127-new-script-lab-office-add-michael-zlatkovsky-bhargav-krishna-office-365-developer-podcast/)\n\n## External media coverage\n\n- August 29, 2017: [\"Start Developing in OfficeJS Today with Script\u00a0Lab\"](http://theofficecontext.com/2017/08/29/start-developing-in-officejs-today-with-script-lab/) _by David Craig at theofficecontext.com_\n- July 30, 2017: _[German]_ [\"Eigene Script Lab Scripte in Office Online testen\"](http://www.excel-ticker.de/eigene-script-lab-scripte-in-office-online-testen/). (Auto-translation: [\"Test your own Script Lab scripts in Office Online\"](http://www.microsofttranslator.com/bv.aspx?from=&to=en&a=http%3A%2F%2Fwww.excel-ticker.de%2Feigene-script-lab-scripte-in-office-online-testen%2F)) _by [Mourad Louha](https://twitter.com/maninweb)_\n- June 14, 2017: _[Portuguese]_ [\"Script Lab: Novo add-in da Microsoft\"](https://medium.com/leonardo-xavier/script-lab-novo-add-in-da-microsoft-f8aee5bf0dd2). (Auto-translation: [\"Script Lab: New Microsoft add-in\"](https://translate.google.com/translate?sl=auto&tl=en&js=y&prev=_t&hl=en&ie=UTF-8&u=https%3A%2F%2Fmedium.com%2Fleonardo-xavier%2Fscript-lab-novo-add-in-da-microsoft-f8aee5bf0dd2&edit-text=&act=url)), by _[Leonardo Xavier](https://medium.com/leonardo-xavier)_.\n- May 1, 2017: _[German]_ [\"Prototyping von Microsoft Office JavaScript Add-Ins mit Script Lab\"](http://www.excel-ticker.de/prototyping-von-microsoft-office-javascript-add-ins-mit-script-lab/). (Auto-translation: [\"Prototyping Microsoft Office JavaScript add-ins with Script Lab\"](http://www.microsofttranslator.com/bv.aspx?&lo=TP&from=de&to=en&a=http%3A%2F%2Fwww.excel-ticker.de%2Fprototyping-von-microsoft-office-javascript-add-ins-mit-script-lab%2F)), _by [Mourad Louha](https://twitter.com/maninweb)_\n- May 1, 2017: _[Portuguese]_ [\"Microsoft lan\u00e7a o Script Lab\"](http://mlf.net.br/blog/microsoft-lanca-o-script-labs/). (Auto-translation: [\"Microsoft Launches Script Lab\"](http://www.microsofttranslator.com/bv.aspx?from=pt&to=en&a=http%3A%2F%2Fmlf.net.br%2Fblog%2Fmicrosoft-lanca-o-script-labs%2F)), _by Felipe Costa Gualberto_.\n- April 18, 2017: [\"Microsoft Garage Releases Script Lab\"](https://winbuzzer.com/2017/04/18/microsoft-garage-releases-script-lab-tool-test-javascript-apis-inside-office-suite-xcxwbn/) _by Ryan Maskell at winbuzzer.com_\n\n## Code of conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information, see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n"
  },
  {
    "name": "codex",
    "url": "https://github.com/davidatoms/codex",
    "description": "Lightweight coding agent that runs in your terminal",
    "type": "fork",
    "updated_at": "2025-05-11T07:30:56Z",
    "readme": "<h1 align=\"center\">OpenAI Codex CLI</h1>\n<p align=\"center\">Lightweight coding agent that runs in your terminal</p>\n\n<p align=\"center\"><code>npm i -g @openai/codex</code></p>\n\n![Codex demo GIF using: codex \"explain this codebase to me\"](./.github/demo.gif)\n\n---\n\n<details>\n<summary><strong>Table of contents</strong></summary>\n\n<!-- Begin ToC -->\n\n- [Experimental technology disclaimer](#experimental-technology-disclaimer)\n- [Quickstart](#quickstart)\n- [Why Codex?](#why-codex)\n- [Security model & permissions](#security-model--permissions)\n  - [Platform sandboxing details](#platform-sandboxing-details)\n- [System requirements](#system-requirements)\n- [CLI reference](#cli-reference)\n- [Memory & project docs](#memory--project-docs)\n- [Non-interactive / CI mode](#non-interactive--ci-mode)\n- [Tracing / verbose logging](#tracing--verbose-logging)\n- [Recipes](#recipes)\n- [Installation](#installation)\n- [Configuration guide](#configuration-guide)\n  - [Basic configuration parameters](#basic-configuration-parameters)\n  - [Custom AI provider configuration](#custom-ai-provider-configuration)\n  - [History configuration](#history-configuration)\n  - [Configuration examples](#configuration-examples)\n  - [Full configuration example](#full-configuration-example)\n  - [Custom instructions](#custom-instructions)\n  - [Environment variables setup](#environment-variables-setup)\n- [FAQ](#faq)\n- [Zero data retention (ZDR) usage](#zero-data-retention-zdr-usage)\n- [Codex open source fund](#codex-open-source-fund)\n- [Contributing](#contributing)\n  - [Development workflow](#development-workflow)\n  - [Git hooks with Husky](#git-hooks-with-husky)\n  - [Debugging](#debugging)\n  - [Writing high-impact code changes](#writing-high-impact-code-changes)\n  - [Opening a pull request](#opening-a-pull-request)\n  - [Review process](#review-process)\n  - [Community values](#community-values)\n  - [Getting help](#getting-help)\n  - [Contributor license agreement (CLA)](#contributor-license-agreement-cla)\n    - [Quick fixes](#quick-fixes)\n  - [Releasing `codex`](#releasing-codex)\n  - [Alternative build options](#alternative-build-options)\n    - [Nix flake development](#nix-flake-development)\n- [Security & responsible AI](#security--responsible-ai)\n- [License](#license)\n\n<!-- End ToC -->\n\n</details>\n\n---\n\n## Experimental technology disclaimer\n\nCodex CLI is an experimental project under active development. It is not yet stable, may contain bugs, incomplete features, or undergo breaking changes. We're building it in the open with the community and welcome:\n\n- Bug reports\n- Feature requests\n- Pull requests\n- Good vibes\n\nHelp us improve by filing issues or submitting PRs (see the section below for how to contribute)!\n\n## Quickstart\n\nInstall globally:\n\n```shell\nnpm install -g @openai/codex\n```\n\nNext, set your OpenAI API key as an environment variable:\n\n```shell\nexport OPENAI_API_KEY=\"your-api-key-here\"\n```\n\n> **Note:** This command sets the key only for your current terminal session. You can add the `export` line to your shell's configuration file (e.g., `~/.zshrc`) but we recommend setting for the session. **Tip:** You can also place your API key into a `.env` file at the root of your project:\n>\n> ```env\n> OPENAI_API_KEY=your-api-key-here\n> ```\n>\n> The CLI will automatically load variables from `.env` (via `dotenv/config`).\n\n<details>\n<summary><strong>Use <code>--provider</code> to use other models</strong></summary>\n\n> Codex also allows you to use other providers that support the OpenAI Chat Completions API. You can set the provider in the config file or use the `--provider` flag. The possible options for `--provider` are:\n>\n> - openai (default)\n> - openrouter\n> - azure\n> - gemini\n> - ollama\n> - mistral\n> - deepseek\n> - xai\n> - groq\n> - arceeai\n> - any other provider that is compatible with the OpenAI API\n>\n> If you use a provider other than OpenAI, you will need to set the API key for the provider in the config file or in the environment variable as:\n>\n> ```shell\n> export <provider>_API_KEY=\"your-api-key-here\"\n> ```\n>\n> If you use a provider not listed above, you must also set the base URL for the provider:\n>\n> ```shell\n> export <provider>_BASE_URL=\"https://your-provider-api-base-url\"\n> ```\n\n</details>\n<br />\n\nRun interactively:\n\n```shell\ncodex\n```\n\nOr, run with a prompt as input (and optionally in `Full Auto` mode):\n\n```shell\ncodex \"explain this codebase to me\"\n```\n\n```shell\ncodex --approval-mode full-auto \"create the fanciest todo-list app\"\n```\n\nThat's it - Codex will scaffold a file, run it inside a sandbox, install any\nmissing dependencies, and show you the live result. Approve the changes and\nthey'll be committed to your working directory.\n\n---\n\n## Why Codex?\n\nCodex CLI is built for developers who already **live in the terminal** and want\nChatGPT-level reasoning **plus** the power to actually run code, manipulate\nfiles, and iterate - all under version control. In short, it's _chat-driven\ndevelopment_ that understands and executes your repo.\n\n- **Zero setup** - bring your OpenAI API key and it just works!\n- **Full auto-approval, while safe + secure** by running network-disabled and directory-sandboxed\n- **Multimodal** - pass in screenshots or diagrams to implement features \u2728\n\nAnd it's **fully open-source** so you can see and contribute to how it develops!\n\n---\n\n## Security model & permissions\n\nCodex lets you decide _how much autonomy_ the agent receives and auto-approval policy via the\n`--approval-mode` flag (or the interactive onboarding prompt):\n\n| Mode                      | What the agent may do without asking                                                                | Still requires approval                                                                         |\n| ------------------------- | --------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------- |\n| **Suggest** <br>(default) | <li>Read any file in the repo                                                                       | <li>**All** file writes/patches<li> **Any** arbitrary shell commands (aside from reading files) |\n| **Auto Edit**             | <li>Read **and** apply-patch writes to files                                                        | <li>**All** shell commands                                                                      |\n| **Full Auto**             | <li>Read/write files <li> Execute shell commands (network disabled, writes limited to your workdir) | -                                                                                               |\n\nIn **Full Auto** every command is run **network-disabled** and confined to the\ncurrent working directory (plus temporary files) for defense-in-depth. Codex\nwill also show a warning/confirmation if you start in **auto-edit** or\n**full-auto** while the directory is _not_ tracked by Git, so you always have a\nsafety net.\n\nComing soon: you'll be able to whitelist specific commands to auto-execute with\nthe network enabled, once we're confident in additional safeguards.\n\n### Platform sandboxing details\n\nThe hardening mechanism Codex uses depends on your OS:\n\n- **macOS 12+** - commands are wrapped with **Apple Seatbelt** (`sandbox-exec`).\n\n  - Everything is placed in a read-only jail except for a small set of\n    writable roots (`$PWD`, `$TMPDIR`, `~/.codex`, etc.).\n  - Outbound network is _fully blocked_ by default - even if a child process\n    tries to `curl` somewhere it will fail.\n\n- **Linux** - there is no sandboxing by default.\n  We recommend using Docker for sandboxing, where Codex launches itself inside a **minimal\n  container image** and mounts your repo _read/write_ at the same path. A\n  custom `iptables`/`ipset` firewall script denies all egress except the\n  OpenAI API. This gives you deterministic, reproducible runs without needing\n  root on the host. You can use the [`run_in_container.sh`](./codex-cli/scripts/run_in_container.sh) script to set up the sandbox.\n\n---\n\n## System requirements\n\n| Requirement                 | Details                                                         |\n| --------------------------- | --------------------------------------------------------------- |\n| Operating systems           | macOS 12+, Ubuntu 20.04+/Debian 10+, or Windows 11 **via WSL2** |\n| Node.js                     | **22 or newer** (LTS recommended)                               |\n| Git (optional, recommended) | 2.23+ for built-in PR helpers                                   |\n| RAM                         | 4-GB minimum (8-GB recommended)                                 |\n\n> Never run `sudo npm install -g`; fix npm permissions instead.\n\n---\n\n## CLI reference\n\n| Command                              | Purpose                             | Example                              |\n| ------------------------------------ | ----------------------------------- | ------------------------------------ |\n| `codex`                              | Interactive REPL                    | `codex`                              |\n| `codex \"...\"`                        | Initial prompt for interactive REPL | `codex \"fix lint errors\"`            |\n| `codex -q \"...\"`                     | Non-interactive \"quiet mode\"        | `codex -q --json \"explain utils.ts\"` |\n| `codex completion <bash\\|zsh\\|fish>` | Print shell completion script       | `codex completion bash`              |\n\nKey flags: `--model/-m`, `--approval-mode/-a`, `--quiet/-q`, and `--notify`.\n\n---\n\n## Memory & project docs\n\nYou can give Codex extra instructions and guidance using `AGENTS.md` files. Codex looks for `AGENTS.md` files in the following places, and merges them top-down:\n\n1. `~/.codex/AGENTS.md` - personal global guidance\n2. `AGENTS.md` at repo root - shared project notes\n3. `AGENTS.md` in the current working directory - sub-folder/feature specifics\n\nDisable loading of these files with `--no-project-doc` or the environment variable `CODEX_DISABLE_PROJECT_DOC=1`.\n\n---\n\n## Non-interactive / CI mode\n\nRun Codex head-less in pipelines. Example GitHub Action step:\n\n```yaml\n- name: Update changelog via Codex\n  run: |\n    npm install -g @openai/codex\n    export OPENAI_API_KEY=\"${{ secrets.OPENAI_KEY }}\"\n    codex -a auto-edit --quiet \"update CHANGELOG for next release\"\n```\n\nSet `CODEX_QUIET_MODE=1` to silence interactive UI noise.\n\n## Tracing / verbose logging\n\nSetting the environment variable `DEBUG=true` prints full API request and response details:\n\n```shell\nDEBUG=true codex\n```\n\n---\n\n## Recipes\n\nBelow are a few bite-size examples you can copy-paste. Replace the text in quotes with your own task. See the [prompting guide](https://github.com/openai/codex/blob/main/codex-cli/examples/prompting_guide.md) for more tips and usage patterns.\n\n| \u2728  | What you type                                                                   | What happens                                                               |\n| --- | ------------------------------------------------------------------------------- | -------------------------------------------------------------------------- |\n| 1   | `codex \"Refactor the Dashboard component to React Hooks\"`                       | Codex rewrites the class component, runs `npm test`, and shows the diff.   |\n| 2   | `codex \"Generate SQL migrations for adding a users table\"`                      | Infers your ORM, creates migration files, and runs them in a sandboxed DB. |\n| 3   | `codex \"Write unit tests for utils/date.ts\"`                                    | Generates tests, executes them, and iterates until they pass.              |\n| 4   | `codex \"Bulk-rename *.jpeg -> *.jpg with git mv\"`                               | Safely renames files and updates imports/usages.                           |\n| 5   | `codex \"Explain what this regex does: ^(?=.*[A-Z]).{8,}$\"`                      | Outputs a step-by-step human explanation.                                  |\n| 6   | `codex \"Carefully review this repo, and propose 3 high impact well-scoped PRs\"` | Suggests impactful PRs in the current codebase.                            |\n| 7   | `codex \"Look for vulnerabilities and create a security review report\"`          | Finds and explains security bugs.                                          |\n\n---\n\n## Installation\n\n<details open>\n<summary><strong>From npm (Recommended)</strong></summary>\n\n```bash\nnpm install -g @openai/codex\n# or\nyarn global add @openai/codex\n# or\nbun install -g @openai/codex\n# or\npnpm add -g @openai/codex\n```\n\n</details>\n\n<details>\n<summary><strong>Build from source</strong></summary>\n\n```bash\n# Clone the repository and navigate to the CLI package\ngit clone https://github.com/openai/codex.git\ncd codex/codex-cli\n\n# Enable corepack\ncorepack enable\n\n# Install dependencies and build\npnpm install\npnpm build\n\n# Linux-only: download prebuilt sandboxing binaries (requires gh and zstd).\n./scripts/install_native_deps.sh\n\n# Get the usage and the options\nnode ./dist/cli.js --help\n\n# Run the locally-built CLI directly\nnode ./dist/cli.js\n\n# Or link the command globally for convenience\npnpm link\n```\n\n</details>\n\n---\n\n## Configuration guide\n\nCodex configuration files can be placed in the `~/.codex/` directory, supporting both YAML and JSON formats.\n\n### Basic configuration parameters\n\n| Parameter           | Type    | Default    | Description                      | Available Options                                                                              |\n| ------------------- | ------- | ---------- | -------------------------------- | ---------------------------------------------------------------------------------------------- |\n| `model`             | string  | `o4-mini`  | AI model to use                  | Any model name supporting OpenAI API                                                           |\n| `approvalMode`      | string  | `suggest`  | AI assistant's permission mode   | `suggest` (suggestions only)<br>`auto-edit` (automatic edits)<br>`full-auto` (fully automatic) |\n| `fullAutoErrorMode` | string  | `ask-user` | Error handling in full-auto mode | `ask-user` (prompt for user input)<br>`ignore-and-continue` (ignore and proceed)               |\n| `notify`            | boolean | `true`     | Enable desktop notifications     | `true`/`false`                                                                                 |\n\n### Custom AI provider configuration\n\nIn the `providers` object, you can configure multiple AI service providers. Each provider requires the following parameters:\n\n| Parameter | Type   | Description                             | Example                       |\n| --------- | ------ | --------------------------------------- | ----------------------------- |\n| `name`    | string | Display name of the provider            | `\"OpenAI\"`                    |\n| `baseURL` | string | API service URL                         | `\"https://api.openai.com/v1\"` |\n| `envKey`  | string | Environment variable name (for API key) | `\"OPENAI_API_KEY\"`            |\n\n### History configuration\n\nIn the `history` object, you can configure conversation history settings:\n\n| Parameter           | Type    | Description                                            | Example Value |\n| ------------------- | ------- | ------------------------------------------------------ | ------------- |\n| `maxSize`           | number  | Maximum number of history entries to save              | `1000`        |\n| `saveHistory`       | boolean | Whether to save history                                | `true`        |\n| `sensitivePatterns` | array   | Patterns of sensitive information to filter in history | `[]`          |\n\n### Configuration examples\n\n1. YAML format (save as `~/.codex/config.yaml`):\n\n```yaml\nmodel: o4-mini\napprovalMode: suggest\nfullAutoErrorMode: ask-user\nnotify: true\n```\n\n2. JSON format (save as `~/.codex/config.json`):\n\n```json\n{\n  \"model\": \"o4-mini\",\n  \"approvalMode\": \"suggest\",\n  \"fullAutoErrorMode\": \"ask-user\",\n  \"notify\": true\n}\n```\n\n### Full configuration example\n\nBelow is a comprehensive example of `config.json` with multiple custom providers:\n\n```json\n{\n  \"model\": \"o4-mini\",\n  \"provider\": \"openai\",\n  \"providers\": {\n    \"openai\": {\n      \"name\": \"OpenAI\",\n      \"baseURL\": \"https://api.openai.com/v1\",\n      \"envKey\": \"OPENAI_API_KEY\"\n    },\n    \"azure\": {\n      \"name\": \"AzureOpenAI\",\n      \"baseURL\": \"https://YOUR_PROJECT_NAME.openai.azure.com/openai\",\n      \"envKey\": \"AZURE_OPENAI_API_KEY\"\n    },\n    \"openrouter\": {\n      \"name\": \"OpenRouter\",\n      \"baseURL\": \"https://openrouter.ai/api/v1\",\n      \"envKey\": \"OPENROUTER_API_KEY\"\n    },\n    \"gemini\": {\n      \"name\": \"Gemini\",\n      \"baseURL\": \"https://generativelanguage.googleapis.com/v1beta/openai\",\n      \"envKey\": \"GEMINI_API_KEY\"\n    },\n    \"ollama\": {\n      \"name\": \"Ollama\",\n      \"baseURL\": \"http://localhost:11434/v1\",\n      \"envKey\": \"OLLAMA_API_KEY\"\n    },\n    \"mistral\": {\n      \"name\": \"Mistral\",\n      \"baseURL\": \"https://api.mistral.ai/v1\",\n      \"envKey\": \"MISTRAL_API_KEY\"\n    },\n    \"deepseek\": {\n      \"name\": \"DeepSeek\",\n      \"baseURL\": \"https://api.deepseek.com\",\n      \"envKey\": \"DEEPSEEK_API_KEY\"\n    },\n    \"xai\": {\n      \"name\": \"xAI\",\n      \"baseURL\": \"https://api.x.ai/v1\",\n      \"envKey\": \"XAI_API_KEY\"\n    },\n    \"groq\": {\n      \"name\": \"Groq\",\n      \"baseURL\": \"https://api.groq.com/openai/v1\",\n      \"envKey\": \"GROQ_API_KEY\"\n    },\n    \"arceeai\": {\n      \"name\": \"ArceeAI\",\n      \"baseURL\": \"https://conductor.arcee.ai/v1\",\n      \"envKey\": \"ARCEEAI_API_KEY\"\n    }\n  },\n  \"history\": {\n    \"maxSize\": 1000,\n    \"saveHistory\": true,\n    \"sensitivePatterns\": []\n  }\n}\n```\n\n### Custom instructions\n\nYou can create a `~/.codex/AGENTS.md` file to define custom guidance for the agent:\n\n```markdown\n- Always respond with emojis\n- Only use git commands when explicitly requested\n```\n\n### Environment variables setup\n\nFor each AI provider, you need to set the corresponding API key in your environment variables. For example:\n\n```bash\n# OpenAI\nexport OPENAI_API_KEY=\"your-api-key-here\"\n\n# Azure OpenAI\nexport AZURE_OPENAI_API_KEY=\"your-azure-api-key-here\"\nexport AZURE_OPENAI_API_VERSION=\"2025-03-01-preview\" (Optional)\n\n# OpenRouter\nexport OPENROUTER_API_KEY=\"your-openrouter-key-here\"\n\n# Similarly for other providers\n```\n\n---\n\n## FAQ\n\n<details>\n<summary>OpenAI released a model called Codex in 2021 - is this related?</summary>\n\nIn 2021, OpenAI released Codex, an AI system designed to generate code from natural language prompts. That original Codex model was deprecated as of March 2023 and is separate from the CLI tool.\n\n</details>\n\n<details>\n<summary>Which models are supported?</summary>\n\nAny model available with [Responses API](https://platform.openai.com/docs/api-reference/responses). The default is `o4-mini`, but pass `--model gpt-4.1` or set `model: gpt-4.1` in your config file to override.\n\n</details>\n<details>\n<summary>Why does <code>o3</code> or <code>o4-mini</code> not work for me?</summary>\n\nIt's possible that your [API account needs to be verified](https://help.openai.com/en/articles/10910291-api-organization-verification) in order to start streaming responses and seeing chain of thought summaries from the API. If you're still running into issues, please let us know!\n\n</details>\n\n<details>\n<summary>How do I stop Codex from editing my files?</summary>\n\nCodex runs model-generated commands in a sandbox. If a proposed command or file change doesn't look right, you can simply type **n** to deny the command or give the model feedback.\n\n</details>\n<details>\n<summary>Does it work on Windows?</summary>\n\nNot directly. It requires [Windows Subsystem for Linux (WSL2)](https://learn.microsoft.com/en-us/windows/wsl/install) - Codex has been tested on macOS and Linux with Node 22.\n\n</details>\n\n---\n\n## Zero data retention (ZDR) usage\n\nCodex CLI **does** support OpenAI organizations with [Zero Data Retention (ZDR)](https://platform.openai.com/docs/guides/your-data#zero-data-retention) enabled. If your OpenAI organization has Zero Data Retention enabled and you still encounter errors such as:\n\n```\nOpenAI rejected the request. Error details: Status: 400, Code: unsupported_parameter, Type: invalid_request_error, Message: 400 Previous response cannot be used for this organization due to Zero Data Retention.\n```\n\nYou may need to upgrade to a more recent version with: `npm i -g @openai/codex@latest`\n\n---\n\n## Codex open source fund\n\nWe're excited to launch a **$1 million initiative** supporting open source projects that use Codex CLI and other OpenAI models.\n\n- Grants are awarded up to **$25,000** API credits.\n- Applications are reviewed **on a rolling basis**.\n\n**Interested? [Apply here](https://openai.com/form/codex-open-source-fund/).**\n\n---\n\n## Contributing\n\nThis project is under active development and the code will likely change pretty significantly. We'll update this message once that's complete!\n\nMore broadly we welcome contributions - whether you are opening your very first pull request or you're a seasoned maintainer. At the same time we care about reliability and long-term maintainability, so the bar for merging code is intentionally **high**. The guidelines below spell out what \"high-quality\" means in practice and should make the whole process transparent and friendly.\n\n### Development workflow\n\n- Create a _topic branch_ from `main` - e.g. `feat/interactive-prompt`.\n- Keep your changes focused. Multiple unrelated fixes should be opened as separate PRs.\n- Use `pnpm test:watch` during development for super-fast feedback.\n- We use **Vitest** for unit tests, **ESLint** + **Prettier** for style, and **TypeScript** for type-checking.\n- Before pushing, run the full test/type/lint suite:\n\n### Git hooks with Husky\n\nThis project uses [Husky](https://typicode.github.io/husky/) to enforce code quality checks:\n\n- **Pre-commit hook**: Automatically runs lint-staged to format and lint files before committing\n- **Pre-push hook**: Runs tests and type checking before pushing to the remote\n\nThese hooks help maintain code quality and prevent pushing code with failing tests. For more details, see [HUSKY.md](./codex-cli/HUSKY.md).\n\n```bash\npnpm test && pnpm run lint && pnpm run typecheck\n```\n\n- If you have **not** yet signed the Contributor License Agreement (CLA), add a PR comment containing the exact text\n\n  ```text\n  I have read the CLA Document and I hereby sign the CLA\n  ```\n\n  The CLA-Assistant bot will turn the PR status green once all authors have signed.\n\n```bash\n# Watch mode (tests rerun on change)\npnpm test:watch\n\n# Type-check without emitting files\npnpm typecheck\n\n# Automatically fix lint + prettier issues\npnpm lint:fix\npnpm format:fix\n```\n\n### Debugging\n\nTo debug the CLI with a visual debugger, do the following in the `codex-cli` folder:\n\n- Run `pnpm run build` to build the CLI, which will generate `cli.js.map` alongside `cli.js` in the `dist` folder.\n- Run the CLI with `node --inspect-brk ./dist/cli.js` The program then waits until a debugger is attached before proceeding. Options:\n  - In VS Code, choose **Debug: Attach to Node Process** from the command palette and choose the option in the dropdown with debug port `9229` (likely the first option)\n  - Go to <chrome://inspect> in Chrome and find **localhost:9229** and click **trace**\n\n### Writing high-impact code changes\n\n1. **Start with an issue.** Open a new one or comment on an existing discussion so we can agree on the solution before code is written.\n2. **Add or update tests.** Every new feature or bug-fix should come with test coverage that fails before your change and passes afterwards. 100% coverage is not required, but aim for meaningful assertions.\n3. **Document behaviour.** If your change affects user-facing behaviour, update the README, inline help (`codex --help`), or relevant example projects.\n4. **Keep commits atomic.** Each commit should compile and the tests should pass. This makes reviews and potential rollbacks easier.\n\n### Opening a pull request\n\n- Fill in the PR template (or include similar information) - **What? Why? How?**\n- Run **all** checks locally (`npm test && npm run lint && npm run typecheck`). CI failures that could have been caught locally slow down the process.\n- Make sure your branch is up-to-date with `main` and that you have resolved merge conflicts.\n- Mark the PR as **Ready for review** only when you believe it is in a merge-able state.\n\n### Review process\n\n1. One maintainer will be assigned as a primary reviewer.\n2. We may ask for changes - please do not take this personally. We value the work, we just also value consistency and long-term maintainability.\n3. When there is consensus that the PR meets the bar, a maintainer will squash-and-merge.\n\n### Community values\n\n- **Be kind and inclusive.** Treat others with respect; we follow the [Contributor Covenant](https://www.contributor-covenant.org/).\n- **Assume good intent.** Written communication is hard - err on the side of generosity.\n- **Teach & learn.** If you spot something confusing, open an issue or PR with improvements.\n\n### Getting help\n\nIf you run into problems setting up the project, would like feedback on an idea, or just want to say _hi_ - please open a Discussion or jump into the relevant issue. We are happy to help.\n\nTogether we can make Codex CLI an incredible tool. **Happy hacking!** :rocket:\n\n### Contributor license agreement (CLA)\n\nAll contributors **must** accept the CLA. The process is lightweight:\n\n1. Open your pull request.\n2. Paste the following comment (or reply `recheck` if you've signed before):\n\n   ```text\n   I have read the CLA Document and I hereby sign the CLA\n   ```\n\n3. The CLA-Assistant bot records your signature in the repo and marks the status check as passed.\n\nNo special Git commands, email attachments, or commit footers required.\n\n#### Quick fixes\n\n| Scenario          | Command                                          |\n| ----------------- | ------------------------------------------------ |\n| Amend last commit | `git commit --amend -s --no-edit && git push -f` |\n\nThe **DCO check** blocks merges until every commit in the PR carries the footer (with squash this is just the one).\n\n### Releasing `codex`\n\nTo publish a new version of the CLI, run the following in the `codex-cli` folder to stage the release in a temporary directory:\n\n```\npnpm stage-release\n```\n\nNote you can specify the folder for the staged release:\n\n```\nRELEASE_DIR=$(mktemp -d)\npnpm stage-release \"$RELEASE_DIR\"\n```\n\nGo to the folder where the release is staged and verify that it works as intended. If so, run the following from the temp folder:\n\n```\ncd \"$RELEASE_DIR\"\nnpm publish\n```\n\n### Alternative build options\n\n#### Nix flake development\n\nPrerequisite: Nix >= 2.4 with flakes enabled (`experimental-features = nix-command flakes` in `~/.config/nix/nix.conf`).\n\nEnter a Nix development shell:\n\n```bash\nnix develop\n```\n\nThis shell includes Node.js, installs dependencies, builds the CLI, and provides a `codex` command alias.\n\nBuild and run the CLI directly:\n\n```bash\nnix build\n./result/bin/codex --help\n```\n\nRun the CLI via the flake app:\n\n```bash\nnix run .#codex\n```\n\n---\n\n## Security & responsible AI\n\nHave you discovered a vulnerability or have concerns about model output? Please e-mail **security@openai.com** and we will respond promptly.\n\n---\n\n## License\n\nThis repository is licensed under the [Apache-2.0 License](LICENSE).\n"
  },
  {
    "name": "zero",
    "url": "https://github.com/davidatoms/zero",
    "description": "Experience email the way you want with 0 \u2013 the first open source email app that puts your privacy and safety first. Join the discord: https://discord.gg/0email",
    "type": "fork",
    "updated_at": "2025-05-10T19:02:20Z",
    "readme": "<p align=\"center\">\n  <picture>\n    <source srcset=\"apps/mail/public/white-icon.svg\" media=\"(prefers-color-scheme: dark)\">\n    <img src=\"apps/mail/public/black-icon.svg\" alt=\"Zero Logo\" width=\"64\" style=\"background-color: #000; padding: 10px;\"/>\n  </picture>\n</p>\n\n# Zero\n\n[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Fnizzyabi%2FMail0&env=DATABASE_URL,BETTER_AUTH_SECRET,BETTER_AUTH_URL,BETTER_AUTH_TRUSTED_ORIGINS,GOOGLE_CLIENT_ID,GOOGLE_CLIENT_SECRET,GOOGLE_REDIRECT_URI,GITHUB_CLIENT_ID,GITHUB_CLIENT_SECRET,GITHUB_REDIRECT_URI&envDescription=For%20more%20info%20on%20setting%20up%20your%20API%20keys%2C%20checkout%20the%20Readme%20below&envLink=https%3A%2F%2Fgithub.com%2Fnizzyabi%2FMail0%2Fblob%2Fmain%2FREADME.md&project-name=0&repository-name=0&redirect-url=0.email&demo-title=0&demo-description=An%20open%20source%20email%20app&demo-url=0.email)\n\nAn Open-Source Gmail Alternative for the Future of Email\n\n## What is Zero?\n\nZero is an open-source AI email solution that gives users the power to **self-host** their own email app while also integrating external services like Gmail and other email providers. Our goal is to modernize and improve emails through AI agents to truly modernize emails.\n\n## Why Zero?\n\nMost email services today are either **closed-source**, **data-hungry**, or **too complex to self-host**.\n0.email is different:\n\n- \u2705 **Open-Source** \u2013 No hidden agendas, fully transparent.\n- \ud83e\uddbe **AI Driven** - Enhance your emails with Agents & LLMs.\n- \ud83d\udd12 **Data Privacy First** \u2013 Your emails, your data. Zero does not track, collect, or sell your data in any way. Please note: while we integrate with external services, the data passed through them is not under our control and falls under their respective privacy policies and terms of service.\n- \u2699\ufe0f **Self-Hosting Freedom** \u2013 Run your own email app with ease.\n- \ud83d\udcec **Unified Inbox** \u2013 Connect multiple email providers like Gmail, Outlook, and more.\n- \ud83c\udfa8 **Customizable UI & Features** \u2013 Tailor your email experience the way you want it.\n- \ud83d\ude80 **Developer-Friendly** \u2013 Built with extensibility and integrations in mind.\n\n## Tech Stack\n\nZero is built with modern and reliable technologies:\n\n- **Frontend**: Next.js, React, TypeScript, TailwindCSS, Shadcn UI\n- **Backend**: Node.js, Drizzle ORM\n- **Database**: PostgreSQL\n- **Authentication**: Better Auth, Google OAuth\n<!-- - **Testing**: Jest, React Testing Library -->\n\n## Getting Started\n\n### Prerequisites\n\n**Required Versions:**\n\n- [Node.js](https://nodejs.org/en/download) (v18 or higher)\n- [Bun](https://bun.sh) (v1.2 or higher)\n- [Docker](https://docs.docker.com/engine/install/) (v20 or higher)\n\nBefore running the application, you'll need to set up services and configure environment variables. For more details on environment variables, see the [Environment Variables](#environment-variables) section.\n\n### Setup Options\n\nYou can set up Zero in two ways:\n\n<details open>\n<summary><b>Option 1: Standard Setup (Recommended)</b></summary>\n\n#### Quick Start Guide\n\n1. **Clone and Install**\n\n   ```bash\n   # Clone the repository\n   git clone https://github.com/Mail-0/Zero.git\n   cd Zero\n\n   # Install dependencies\n   bun install\n\n   # Start database locally\n   bun docker:up\n   ```\n\n2. **Set Up Environment**\n\n   - Copy `.env.example` to `.env` in project root\n     ```bash\n     cp .env.example .env\n     ```\n   - Configure your environment variables (see below)\n   - Start the database with the provided docker compose setup: `bun docker:up`\n   - Initialize the database: `bun db:push`\n\n3. **Start the App**\n\n   ```bash\n   bun dev\n   ```\n\n4. **Open in Browser**\n\n   Visit [http://localhost:3000](http://localhost:3000)\n   </details>\n\n<details>\n<summary><b>Option 2: Dev Container Setup (For VS Code Users)</b></summary>\n\nThis option uses VS Code's Dev Containers feature to provide a fully configured development environment with all dependencies pre-installed. It's great for ensuring everyone on the team has the same setup.\n\n1. **Prerequisites**\n\n   - [Docker](https://docs.docker.com/get-docker/)\n   - [VS Code](https://code.visualstudio.com/) or compatible editor\n   - [Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers)\n\n2. **Open in Dev Container**\n\n   - Clone the repository: `git clone https://github.com/Mail-0/Zero.git`\n   - Open the folder in VS Code\n   - When prompted, click \"Reopen in Container\" or run the \"Dev Containers: Open Folder in Container\" command\n   - VS Code will build and start the dev container (this may take a few minutes the first time)\n\n3. **Access the App**\n\n   - The app will be available at [http://localhost:3000](http://localhost:3000)\n\n4. **Troubleshooting**\n   - If you encounter issues with the container, try rebuilding it using the \"Dev Containers: Rebuild Container\" command\n   - For dependency issues inside the container:\n     `bash\nrm -rf node_modules\nrm bun.lockb\nbun install\n`\n     </details>\n\n### Environment Setup\n\n1. **Better Auth Setup**\n\n   - Open the `.env` file and change the BETTER_AUTH_SECRET to a random string. (Use `openssl rand -hex 32` to generate a 32 character string)\n\n     ```env\n     BETTER_AUTH_SECRET=your_secret_key\n     ```\n\n2. **Google OAuth Setup** (Required for Gmail integration)\n\n   - Go to [Google Cloud Console](https://console.cloud.google.com)\n   - Create a new project\n   - Add the following APIs in your Google Cloud Project: [People API](https://console.cloud.google.com/apis/library/people.googleapis.com), [Gmail API](https://console.cloud.google.com/apis/library/gmail.googleapis.com)\n     - Use the links above and click 'Enable' or\n     - Go to 'APIs and Services' > 'Enable APIs and Services' > Search for 'Google People API' and click 'Enable'\n     - Go to 'APIs and Services' > 'Enable APIs and Services' > Search for 'Gmail API' and click 'Enable'\n   - Enable the Google OAuth2 API\n   - Create OAuth 2.0 credentials (Web application type)\n   - Add authorized redirect URIs:\n     - Development:\n       - `http://localhost:3000/api/auth/callback/google`\n     - Production:\n       - `https://your-production-url/api/auth/callback/google`\n   - Add to `.env`:\n\n     ```env\n     GOOGLE_CLIENT_ID=your_client_id\n     GOOGLE_CLIENT_SECRET=your_client_secret\n     ```\n\n   - Add yourself as a test user:\n\n     - Go to [`Audience`](https://console.cloud.google.com/auth/audience)\n     - Under 'Test users' click 'Add Users'\n     - Add your email and click 'Save'\n\n> [!WARNING]\n> The authorized redirect URIs in Google Cloud Console must match **exactly** what you configure in the `.env`, including the protocol (http/https), domain, and path - these are provided above.\n\n### Environment Variables\n\nCopy `.env.example` located in the project folder to `.env` in the same folder and configure the following variables:\n\n```env\n# Auth\nBETTER_AUTH_SECRET=     # Required: Secret key for authentication\n\n# Google OAuth (Required for Gmail integration)\nGOOGLE_CLIENT_ID=       # Required for Gmail integration\nGOOGLE_CLIENT_SECRET=   # Required for Gmail integration\n\n# Database\nDATABASE_URL=           # Required: PostgreSQL connection string for backend connection\n\n# Redis\nREDIS_URL=              # Redis URL for caching (http://localhost:8079 for local dev)\nREDIS_TOKEN=            # Redis token (upstash-local-token for local dev)\n```\n\nFor local development a connection string example is provided in the `.env.example` file located in the same folder as the database.\n\n### Database Setup\n\nZero uses PostgreSQL for storing data. Here's how to set it up:\n\n1. **Start the Database**\n\n   Run this command to start a local PostgreSQL instance:\n\n   ```bash\n   bun docker:up\n   ```\n\n   This creates a database with:\n\n   - Name: `zerodotemail`\n   - Username: `postgres`\n   - Password: `postgres`\n   - Port: `5432`\n\n2. **Set Up Database Connection**\n\n   Make sure your database connection string is in `.env` file.\n\n   For local development use:\n\n   ```\n   DATABASE_URL=\"postgresql://postgres:postgres@localhost:5432/zerodotemail\"\n   ```\n\n3. **Database Commands**\n\n   - **Set up database tables**:\n\n     ```bash\n     bun db:push\n     ```\n\n   - **Create migration files** (after schema changes):\n\n     ```bash\n     bun db:generate\n     ```\n\n   - **Apply migrations**:\n\n     ```bash\n     bun db:migrate\n     ```\n\n   - **View database content**:\n     ```bash\n     bun db:studio\n     ```\n     > If you run `bun dev` in your terminal, the studio command should be automatically running with the app.\n\n## Contribute\n\nPlease refer to the [contributing guide](.github/CONTRIBUTING.md).\n\nIf you'd like to help with translating Zero to other languages, check out our [translation guide](.github/TRANSLATION.md).\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Mail-0/Zero&type=Timeline)](https://www.star-history.com/#Mail-0/Zero&Timeline)\n\n## This project wouldn't be possible without these awesome companies\n\n<div style=\"display: flex; justify-content: center;\">\n  <a href=\"https://vercel.com\" style=\"text-decoration: none;\">\n    <img src=\"public/vercel.png\" alt=\"Vercel\" width=\"96\"/>\n  </a>\n  <a href=\"https://better-auth.com\" style=\"text-decoration: none;\">\n    <img src=\"public/better-auth.png\" alt=\"Better Auth\" width=\"96\"/>\n  </a>\n  <a href=\"https://orm.drizzle.team\" style=\"text-decoration: none;\">\n    <img src=\"public/drizzle-orm.png\" alt=\"Drizzle ORM\" width=\"96\"/>\n  </a>\n  <a href=\"https://coderabbit.com\" style=\"text-decoration: none;\">\n    <img src=\"public/coderabbit.png\" alt=\"Coderabbit AI\" width=\"96\"/>\n  </a>\n</div>\n\n## \ud83e\udd0d The team\n\nCurious who makes Zero? Here are our [contributors and maintainers](https://0.email/contributors)\n"
  },
  {
    "name": "void",
    "url": "https://github.com/davidatoms/void",
    "description": null,
    "type": "fork",
    "updated_at": "2025-05-10T07:22:04Z",
    "readme": "# Welcome to Void.\n\n<div align=\"center\">\n\t<img\n\t\tsrc=\"./src/vs/workbench/browser/parts/editor/media/slice_of_void.png\"\n\t \talt=\"Void Welcome\"\n\t\twidth=\"300\"\n\t \theight=\"300\"\n\t/>\n</div>\n\nVoid is the open-source Cursor alternative.\n\nUse AI agents on your codebase, checkpoint and visualize changes, and bring any model or host locally. Void sends messages directly to providers without retaining your data.\n\nThis repo contains the full sourcecode for Void. If you're new, welcome!\n\n- \ud83e\udded [Website](https://voideditor.com)\n\n- \ud83d\udc4b [Discord](https://discord.gg/RSNjgaugJs)\n\n- \ud83d\ude99 [Project Board](https://github.com/orgs/voideditor/projects/2)\n\n\n## Contributing\n\n1. To get started working on Void, check out our Project Board! You can also see [HOW_TO_CONTRIBUTE](https://github.com/voideditor/void/blob/main/HOW_TO_CONTRIBUTE.md).\n\n2. Feel free to attend a casual weekly meeting in our Discord channel!\n\n\n## Reference\n\nVoid is a fork of the [vscode](https://github.com/microsoft/vscode) repository. For a guide to the codebase, see [VOID_CODEBASE_GUIDE](https://github.com/voideditor/void/blob/main/VOID_CODEBASE_GUIDE.md).\n\n## Support\nYou can always reach us in our Discord server or contact us via email: hello@voideditor.com.\n"
  },
  {
    "name": "Thinking-Claude",
    "url": "https://github.com/davidatoms/Thinking-Claude",
    "description": "Let your Claude able to think",
    "type": "fork",
    "updated_at": "2025-05-09T20:13:02Z",
    "readme": "# Thinking Claude\n\nLet Claude think comprehensively before responding!\n\n> **A super quick reminder:**\n> Thinking claude **is not aimed for benchmarks or huge leaps in math or something**, since those are pre-determined by the base model (new Claude-3.5 Sonnet).\n> I only want to explore how further we could reach with Claude's \"deep mindset\". That said, when using it in your daily tasks, you will find Claude's inner monolog (thinking process) very very fun and interesting.\n\n## Demo\n\n> It is supposed to work on both `Free` and `Pro` versions of [Claude Web App](https://claude.ai/) with `Claude 3.5 Sonnet` model.\n\nHere is a demo of using the latest [Thinking Cluade Chrome extension](https://github.com/richards199999/Thinking-Claude/releases/download/chrome-extension-v3.2.3/thinking-claude-chrome-extension-v3.2.3.zip) (click to download v3.2.3 ) installed in Chrome with the chat of Claude (check [Browser Extension](https://github.com/richards199999/Thinking-Claude?tab=readme-ov-file#browser-extension) for more) featured with an instruction selector:\n\nhttps://github.com/user-attachments/assets/afa0f64f-53e5-45bc-9ad8-0641b29d2b77\n\nuse in project with legacy extension:\n\nhttps://github.com/user-attachments/assets/88ff0c75-c51b-42b9-a042-00d47053795a\n\n## Overview\n\nThis project consists of two main components:\n\n1. **Thinking Protocol**: A comprehensive set of instructions that guides Claude to think deeply and systematically before responding\n2. **Browser Extension**: A tool that makes Claude's thinking process more readable and manageable in the browser interface\n\n## Project Structure\n\n```bash\nthinking-claude/\n\u251c\u2500\u2500 extensions/\n\u2502   \u251c\u2500\u2500 chrome/          # Current version of Chrome extension\n\u2502   \u251c\u2500\u2500 chrome_v0/       # Legacy Chrome extension (deprecated)\n\u2502   \u251c\u2500\u2500 firefox/         # Firefox extension (in development)\n\u2502   \u2514\u2500\u2500 changelog.md\n\u251c\u2500\u2500 model_instructions/\n\u2502   \u251c\u2500\u2500 changelog.md\n\u2502   \u251c\u2500\u2500 v5.1-extensive-20241201.md\n\u2502   \u251c\u2500\u2500 v5.1-20241125.md\n\u2502   \u251c\u2500\u2500 v5-lite-20241124.md\n\u2502   \u251c\u2500\u2500 v4-20241118.md\n\u2502   \u251c\u2500\u2500 v4-lite-20241118.md\n\u2502   \u2514\u2500\u2500 v3.5-20241113.md\n\u251c\u2500\u2500 .github/             # GitHub configurations and workflows\n\u251c\u2500\u2500 .husky/             # Git hooks for development\n\u251c\u2500\u2500 LICENSE\n\u2514\u2500\u2500 README.md\n```\n\nThe project is organized into two main components:\n\n- `extensions/`: Browser extension implementations\n\n  - `chrome/`: Current version with modern architecture and features\n  - `chrome_v0/`: Legacy version (deprecated)\n  - `firefox/`: Firefox version (in development)\n\n- `model_instructions/`: Thinking protocols for different versions\n  - Contains versioned instruction sets\n  - Each version brings improvements to Claude's thinking process\n\n## Thinking Protocol\n\nThe thinking protocol instructs Claude to follow a natural, thorough thought process before providing responses.\n\n## Browser Extension\n\nThe browser extension makes Claude's thinking process easier to read and use! It automatically organizes Claude's thoughts into neat, collapsible sections.\n\n### Features\n\n- \ud83c\udfaf Makes Claude's thinking process easy to read\n- \ud83d\udd04 Fold and unfold different parts of Claude's thoughts\n- \ud83d\udccb Copy any part with just one click\n- \u26a1 Works automatically with new messages\n- \ud83c\udfa8 Clean, modern design that's easy on the eyes\n\n### \ud83d\ude80 Quick Install Guide\n\n1. **Chrome Users (May be Outdated)**\n\n   - Install directly from the [Chrome Web Store](https://chromewebstore.google.com/detail/thinking-claude/ncjafpbbndpggfhfgjngkcimeaciahpo)\n\n2. **Manual Installation\uff08Recommended - latest v3.2.3\uff09**\n   - Download the latest version from our [Releases Page](https://github.com/richards199999/Thinking-Claude/releases)\n   - Unzip the file\n   - Open Chrome and go to `chrome://extensions/`\n   - Turn on \"Developer mode\" (top right corner)\n   - Click \"Load unpacked\" and select the unzipped folder `dist`\n\n\ud83d\udc49 Want more details? Check out our [Extension Guide](extensions/chrome/README.md) for:\n\n- Step-by-step installation instructions\n- Development setup\n- Advanced features and usage\n- Troubleshooting tips\n\n### \ud83c\udf89 Getting Started\n\nOnce installed, just:\n\n1. Visit [Claude.ai](https://claude.ai)\n2. Click on the `Choose style` selector in the bottom of input box -> click on `Create & Edit Styles` -> click on `Create Custom Style` -> click on `Describe style manually` -> click on `Start from scratch` -> click on `Use custom instructions (advanced)` -> paste the content of the desired instruction set from `model_instructions/` folder\n3. Start chatting with Claude\n4. That's it! The extension will automatically make Claude's thinking process more readable\n\n## Why Use Thinking Claude?\n\n- **Better Reasoning**: Get more thorough and well-thought-out responses\n- **Transparency**: See how Claude arrives at its conclusions\n- **Improved Organization**: Manage long conversations more effectively\n- **Quality Control**: Benefit from built-in verification steps\n\n## Contributing\n\nContributions are welcome! Feel free to:\n\n- Submit bug reports\n- Propose new features\n- Create pull requests\n\n## License\n\nMIT License - feel free to use and modify as needed.\n\n## Acknowledgments\n\nSpecial thanks to [@lumpinif](https://github.com/lumpinif) and Claude for the extension!\n"
  },
  {
    "name": "cursor-talk-to-figma-mcp",
    "url": "https://github.com/davidatoms/cursor-talk-to-figma-mcp",
    "description": "Cursor Talk To Figma MCP",
    "type": "fork",
    "updated_at": "2025-05-09T05:39:37Z",
    "readme": ""
  },
  {
    "name": "arxiv-mcp-server",
    "url": "https://github.com/davidatoms/arxiv-mcp-server",
    "description": "A Model Context Protocol server for searching and analyzing arXiv papers",
    "type": "fork",
    "updated_at": "2025-05-09T05:38:22Z",
    "readme": "[![Twitter Follow](https://img.shields.io/twitter/follow/JoeBlazick?style=social)](https://twitter.com/JoeBlazick)\n[![smithery badge](https://smithery.ai/badge/arxiv-mcp-server)](https://smithery.ai/server/arxiv-mcp-server)\n[![Python Version](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)\n[![Tests](https://github.com/blazickjp/arxiv-mcp-server/actions/workflows/tests.yml/badge.svg)](https://github.com/blazickjp/arxiv-mcp-server/actions/workflows/tests.yml)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/arxiv-mcp-server.svg)](https://pypi.org/project/arxiv-mcp-server/)\n[![PyPI Version](https://img.shields.io/pypi/v/arxiv-mcp-server.svg)](https://pypi.org/project/arxiv-mcp-server/)\n\n# ArXiv MCP Server\n\n> \ud83d\udd0d Enable AI assistants to search and access arXiv papers through a simple MCP interface.\n\nThe ArXiv MCP Server provides a bridge between AI assistants and arXiv's research repository through the Model Context Protocol (MCP). It allows AI models to search for papers and access their content in a programmatic way.\n\n<div align=\"center\">\n  \n\ud83e\udd1d **[Contribute](https://github.com/blazickjp/arxiv-mcp-server/blob/main/CONTRIBUTING.md)** \u2022 \n\ud83d\udcdd **[Report Bug](https://github.com/blazickjp/arxiv-mcp-server/issues)**\n\n<a href=\"https://www.pulsemcp.com/servers/blazickjp-arxiv-mcp-server\"><img src=\"https://www.pulsemcp.com/badge/top-pick/blazickjp-arxiv-mcp-server\" width=\"400\" alt=\"Pulse MCP Badge\"></a>\n</div>\n\n## \u2728 Core Features\n\n- \ud83d\udd0e **Paper Search**: Query arXiv papers with filters for date ranges and categories\n- \ud83d\udcc4 **Paper Access**: Download and read paper content\n- \ud83d\udccb **Paper Listing**: View all downloaded papers\n- \ud83d\uddc3\ufe0f **Local Storage**: Papers are saved locally for faster access\n- \ud83d\udcdd **Prompts**: A Set of Research Prompts\n\n## \ud83d\ude80 Quick Start\n\n### Installing via Smithery\n\nTo install ArXiv Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/arxiv-mcp-server):\n\n```bash\nnpx -y @smithery/cli install arxiv-mcp-server --client claude\n```\n\n### Installing Manually\nInstall using uv:\n\n```bash\nuv tool install arxiv-mcp-server\n```\n\nFor development:\n\n```bash\n# Clone and set up development environment\ngit clone https://github.com/blazickjp/arxiv-mcp-server.git\ncd arxiv-mcp-server\n\n# Create and activate virtual environment\nuv venv\nsource .venv/bin/activate\n\n# Install with test dependencies\nuv pip install -e \".[test]\"\n```\n\n### \ud83d\udd0c MCP Integration\n\nAdd this configuration to your MCP client config file:\n\n```json\n{\n    \"mcpServers\": {\n        \"arxiv-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"tool\",\n                \"run\",\n                \"arxiv-mcp-server\",\n                \"--storage-path\", \"/path/to/paper/storage\"\n            ]\n        }\n    }\n}\n```\n\nFor Development:\n\n```json\n{\n    \"mcpServers\": {\n        \"arxiv-mcp-server\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"--directory\",\n                \"path/to/cloned/arxiv-mcp-server\",\n                \"run\",\n                \"arxiv-mcp-server\",\n                \"--storage-path\", \"/path/to/paper/storage\"\n            ]\n        }\n    }\n}\n```\n\n## \ud83d\udca1 Available Tools\n\nThe server provides four main tools:\n\n### 1. Paper Search\nSearch for papers with optional filters:\n\n```python\nresult = await call_tool(\"search_papers\", {\n    \"query\": \"transformer architecture\",\n    \"max_results\": 10,\n    \"date_from\": \"2023-01-01\",\n    \"categories\": [\"cs.AI\", \"cs.LG\"]\n})\n```\n\n### 2. Paper Download\nDownload a paper by its arXiv ID:\n\n```python\nresult = await call_tool(\"download_paper\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\n### 3. List Papers\nView all downloaded papers:\n\n```python\nresult = await call_tool(\"list_papers\", {})\n```\n\n### 4. Read Paper\nAccess the content of a downloaded paper:\n\n```python\nresult = await call_tool(\"read_paper\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\n## \ud83d\udcdd Research Prompts\n\nThe server offers specialized prompts to help analyze academic papers:\n\n### Paper Analysis Prompt\nA comprehensive workflow for analyzing academic papers that only requires a paper ID:\n\n```python\nresult = await call_prompt(\"deep-paper-analysis\", {\n    \"paper_id\": \"2401.12345\"\n})\n```\n\nThis prompt includes:\n- Detailed instructions for using available tools (list_papers, download_paper, read_paper, search_papers)\n- A systematic workflow for paper analysis\n- Comprehensive analysis structure covering:\n  - Executive summary\n  - Research context\n  - Methodology analysis\n  - Results evaluation\n  - Practical and theoretical implications\n  - Future research directions\n  - Broader impacts\n\n## \u2699\ufe0f Configuration\n\nConfigure through environment variables:\n\n| Variable | Purpose | Default |\n|----------|---------|---------|\n| `ARXIV_STORAGE_PATH` | Paper storage location | ~/.arxiv-mcp-server/papers |\n\n## \ud83e\uddea Testing\n\nRun the test suite:\n\n```bash\npython -m pytest\n```\n\n## \ud83d\udcc4 License\n\nReleased under the MIT License. See the LICENSE file for details.\n\n---\n\n<div align=\"center\">\n\nMade with \u2764\ufe0f by the Pearl Labs Team\n\n<a href=\"https://glama.ai/mcp/servers/04dtxi5i5n\"><img width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/04dtxi5i5n/badge\" alt=\"ArXiv Server MCP server\" /></a>\n</div>\n"
  },
  {
    "name": "python-sdk",
    "url": "https://github.com/davidatoms/python-sdk",
    "description": "The official Python SDK for Model Context Protocol servers and clients",
    "type": "fork",
    "updated_at": "2025-05-09T05:34:21Z",
    "readme": "# MCP Python SDK\n\n<div align=\"center\">\n\n<strong>Python implementation of the Model Context Protocol (MCP)</strong>\n\n[![PyPI][pypi-badge]][pypi-url]\n[![MIT licensed][mit-badge]][mit-url]\n[![Python Version][python-badge]][python-url]\n[![Documentation][docs-badge]][docs-url]\n[![Specification][spec-badge]][spec-url]\n[![GitHub Discussions][discussions-badge]][discussions-url]\n\n</div>\n\n<!-- omit in toc -->\n## Table of Contents\n\n- [MCP Python SDK](#mcp-python-sdk)\n  - [Overview](#overview)\n  - [Installation](#installation)\n    - [Adding MCP to your python project](#adding-mcp-to-your-python-project)\n    - [Running the standalone MCP development tools](#running-the-standalone-mcp-development-tools)\n  - [Quickstart](#quickstart)\n  - [What is MCP?](#what-is-mcp)\n  - [Core Concepts](#core-concepts)\n    - [Server](#server)\n    - [Resources](#resources)\n    - [Tools](#tools)\n    - [Prompts](#prompts)\n    - [Images](#images)\n    - [Context](#context)\n  - [Running Your Server](#running-your-server)\n    - [Development Mode](#development-mode)\n    - [Claude Desktop Integration](#claude-desktop-integration)\n    - [Direct Execution](#direct-execution)\n    - [Mounting to an Existing ASGI Server](#mounting-to-an-existing-asgi-server)\n  - [Examples](#examples)\n    - [Echo Server](#echo-server)\n    - [SQLite Explorer](#sqlite-explorer)\n  - [Advanced Usage](#advanced-usage)\n    - [Low-Level Server](#low-level-server)\n    - [Writing MCP Clients](#writing-mcp-clients)\n    - [MCP Primitives](#mcp-primitives)\n    - [Server Capabilities](#server-capabilities)\n  - [Documentation](#documentation)\n  - [Contributing](#contributing)\n  - [License](#license)\n\n[pypi-badge]: https://img.shields.io/pypi/v/mcp.svg\n[pypi-url]: https://pypi.org/project/mcp/\n[mit-badge]: https://img.shields.io/pypi/l/mcp.svg\n[mit-url]: https://github.com/modelcontextprotocol/python-sdk/blob/main/LICENSE\n[python-badge]: https://img.shields.io/pypi/pyversions/mcp.svg\n[python-url]: https://www.python.org/downloads/\n[docs-badge]: https://img.shields.io/badge/docs-modelcontextprotocol.io-blue.svg\n[docs-url]: https://modelcontextprotocol.io\n[spec-badge]: https://img.shields.io/badge/spec-spec.modelcontextprotocol.io-blue.svg\n[spec-url]: https://spec.modelcontextprotocol.io\n[discussions-badge]: https://img.shields.io/github/discussions/modelcontextprotocol/python-sdk\n[discussions-url]: https://github.com/modelcontextprotocol/python-sdk/discussions\n\n## Overview\n\nThe Model Context Protocol allows applications to provide context for LLMs in a standardized way, separating the concerns of providing context from the actual LLM interaction. This Python SDK implements the full MCP specification, making it easy to:\n\n- Build MCP clients that can connect to any MCP server\n- Create MCP servers that expose resources, prompts and tools\n- Use standard transports like stdio, SSE, and Streamable HTTP\n- Handle all MCP protocol messages and lifecycle events\n\n## Installation\n\n### Adding MCP to your python project\n\nWe recommend using [uv](https://docs.astral.sh/uv/) to manage your Python projects. \n\nIf you haven't created a uv-managed project yet, create one:\n\n   ```bash\n   uv init mcp-server-demo\n   cd mcp-server-demo\n   ```\n\n   Then add MCP to your project dependencies:\n\n   ```bash\n   uv add \"mcp[cli]\"\n   ```\n\nAlternatively, for projects using pip for dependencies:\n```bash\npip install \"mcp[cli]\"\n```\n\n### Running the standalone MCP development tools\n\nTo run the mcp command with uv:\n\n```bash\nuv run mcp\n```\n\n## Quickstart\n\nLet's create a simple MCP server that exposes a calculator tool and some data:\n\n```python\n# server.py\nfrom mcp.server.fastmcp import FastMCP\n\n# Create an MCP server\nmcp = FastMCP(\"Demo\")\n\n\n# Add an addition tool\n@mcp.tool()\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n\n# Add a dynamic greeting resource\n@mcp.resource(\"greeting://{name}\")\ndef get_greeting(name: str) -> str:\n    \"\"\"Get a personalized greeting\"\"\"\n    return f\"Hello, {name}!\"\n```\n\nYou can install this server in [Claude Desktop](https://claude.ai/download) and interact with it right away by running:\n```bash\nmcp install server.py\n```\n\nAlternatively, you can test it with the MCP Inspector:\n```bash\nmcp dev server.py\n```\n\n## What is MCP?\n\nThe [Model Context Protocol (MCP)](https://modelcontextprotocol.io) lets you build servers that expose data and functionality to LLM applications in a secure, standardized way. Think of it like a web API, but specifically designed for LLM interactions. MCP servers can:\n\n- Expose data through **Resources** (think of these sort of like GET endpoints; they are used to load information into the LLM's context)\n- Provide functionality through **Tools** (sort of like POST endpoints; they are used to execute code or otherwise produce a side effect)\n- Define interaction patterns through **Prompts** (reusable templates for LLM interactions)\n- And more!\n\n## Core Concepts\n\n### Server\n\nThe FastMCP server is your core interface to the MCP protocol. It handles connection management, protocol compliance, and message routing:\n\n```python\n# Add lifespan support for startup/shutdown with strong typing\nfrom contextlib import asynccontextmanager\nfrom collections.abc import AsyncIterator\nfrom dataclasses import dataclass\n\nfrom fake_database import Database  # Replace with your actual DB type\n\nfrom mcp.server.fastmcp import Context, FastMCP\n\n# Create a named server\nmcp = FastMCP(\"My App\")\n\n# Specify dependencies for deployment and development\nmcp = FastMCP(\"My App\", dependencies=[\"pandas\", \"numpy\"])\n\n\n@dataclass\nclass AppContext:\n    db: Database\n\n\n@asynccontextmanager\nasync def app_lifespan(server: FastMCP) -> AsyncIterator[AppContext]:\n    \"\"\"Manage application lifecycle with type-safe context\"\"\"\n    # Initialize on startup\n    db = await Database.connect()\n    try:\n        yield AppContext(db=db)\n    finally:\n        # Cleanup on shutdown\n        await db.disconnect()\n\n\n# Pass lifespan to server\nmcp = FastMCP(\"My App\", lifespan=app_lifespan)\n\n\n# Access type-safe lifespan context in tools\n@mcp.tool()\ndef query_db(ctx: Context) -> str:\n    \"\"\"Tool that uses initialized resources\"\"\"\n    db = ctx.request_context.lifespan_context.db\n    return db.query()\n```\n\n### Resources\n\nResources are how you expose data to LLMs. They're similar to GET endpoints in a REST API - they provide data but shouldn't perform significant computation or have side effects:\n\n```python\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"My App\")\n\n\n@mcp.resource(\"config://app\")\ndef get_config() -> str:\n    \"\"\"Static configuration data\"\"\"\n    return \"App configuration here\"\n\n\n@mcp.resource(\"users://{user_id}/profile\")\ndef get_user_profile(user_id: str) -> str:\n    \"\"\"Dynamic user data\"\"\"\n    return f\"Profile data for user {user_id}\"\n```\n\n### Tools\n\nTools let LLMs take actions through your server. Unlike resources, tools are expected to perform computation and have side effects:\n\n```python\nimport httpx\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"My App\")\n\n\n@mcp.tool()\ndef calculate_bmi(weight_kg: float, height_m: float) -> float:\n    \"\"\"Calculate BMI given weight in kg and height in meters\"\"\"\n    return weight_kg / (height_m**2)\n\n\n@mcp.tool()\nasync def fetch_weather(city: str) -> str:\n    \"\"\"Fetch current weather for a city\"\"\"\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"https://api.weather.com/{city}\")\n        return response.text\n```\n\n### Prompts\n\nPrompts are reusable templates that help LLMs interact with your server effectively:\n\n```python\nfrom mcp.server.fastmcp import FastMCP\nfrom mcp.server.fastmcp.prompts import base\n\nmcp = FastMCP(\"My App\")\n\n\n@mcp.prompt()\ndef review_code(code: str) -> str:\n    return f\"Please review this code:\\n\\n{code}\"\n\n\n@mcp.prompt()\ndef debug_error(error: str) -> list[base.Message]:\n    return [\n        base.UserMessage(\"I'm seeing this error:\"),\n        base.UserMessage(error),\n        base.AssistantMessage(\"I'll help debug that. What have you tried so far?\"),\n    ]\n```\n\n### Images\n\nFastMCP provides an `Image` class that automatically handles image data:\n\n```python\nfrom mcp.server.fastmcp import FastMCP, Image\nfrom PIL import Image as PILImage\n\nmcp = FastMCP(\"My App\")\n\n\n@mcp.tool()\ndef create_thumbnail(image_path: str) -> Image:\n    \"\"\"Create a thumbnail from an image\"\"\"\n    img = PILImage.open(image_path)\n    img.thumbnail((100, 100))\n    return Image(data=img.tobytes(), format=\"png\")\n```\n\n### Context\n\nThe Context object gives your tools and resources access to MCP capabilities:\n\n```python\nfrom mcp.server.fastmcp import FastMCP, Context\n\nmcp = FastMCP(\"My App\")\n\n\n@mcp.tool()\nasync def long_task(files: list[str], ctx: Context) -> str:\n    \"\"\"Process multiple files with progress tracking\"\"\"\n    for i, file in enumerate(files):\n        ctx.info(f\"Processing {file}\")\n        await ctx.report_progress(i, len(files))\n        data, mime_type = await ctx.read_resource(f\"file://{file}\")\n    return \"Processing complete\"\n```\n\n### Authentication\n\nAuthentication can be used by servers that want to expose tools accessing protected resources.\n\n`mcp.server.auth` implements an OAuth 2.0 server interface, which servers can use by\nproviding an implementation of the `OAuthServerProvider` protocol.\n\n```\nmcp = FastMCP(\"My App\",\n        auth_provider=MyOAuthServerProvider(),\n        auth=AuthSettings(\n            issuer_url=\"https://myapp.com\",\n            revocation_options=RevocationOptions(\n                enabled=True,\n            ),\n            client_registration_options=ClientRegistrationOptions(\n                enabled=True,\n                valid_scopes=[\"myscope\", \"myotherscope\"],\n                default_scopes=[\"myscope\"],\n            ),\n            required_scopes=[\"myscope\"],\n        ),\n)\n```\n\nSee [OAuthServerProvider](src/mcp/server/auth/provider.py) for more details.\n\n## Running Your Server\n\n### Development Mode\n\nThe fastest way to test and debug your server is with the MCP Inspector:\n\n```bash\nmcp dev server.py\n\n# Add dependencies\nmcp dev server.py --with pandas --with numpy\n\n# Mount local code\nmcp dev server.py --with-editable .\n```\n\n### Claude Desktop Integration\n\nOnce your server is ready, install it in Claude Desktop:\n\n```bash\nmcp install server.py\n\n# Custom name\nmcp install server.py --name \"My Analytics Server\"\n\n# Environment variables\nmcp install server.py -v API_KEY=abc123 -v DB_URL=postgres://...\nmcp install server.py -f .env\n```\n\n### Direct Execution\n\nFor advanced scenarios like custom deployments:\n\n```python\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"My App\")\n\nif __name__ == \"__main__\":\n    mcp.run()\n```\n\nRun it with:\n```bash\npython server.py\n# or\nmcp run server.py\n```\n\n### Streamable HTTP Transport\n\n> **Note**: Streamable HTTP transport is superseding SSE transport for production deployments.\n\n```python\nfrom mcp.server.fastmcp import FastMCP\n\n# Stateful server (maintains session state)\nmcp = FastMCP(\"StatefulServer\")\n\n# Stateless server (no session persistence)\nmcp = FastMCP(\"StatelessServer\", stateless_http=True)\n\n# Run server with streamable_http transport\nmcp.run(transport=\"streamable-http\")\n```\n\nYou can mount multiple FastMCP servers in a FastAPI application:\n\n```python\n# echo.py\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(name=\"EchoServer\", stateless_http=True)\n\n\n@mcp.tool(description=\"A simple echo tool\")\ndef echo(message: str) -> str:\n    return f\"Echo: {message}\"\n```\n\n```python\n# math.py\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(name=\"MathServer\", stateless_http=True)\n\n\n@mcp.tool(description=\"A simple add tool\")\ndef add_two(n: int) -> str:\n    return n + 2\n```\n\n```python\n# main.py\nfrom fastapi import FastAPI\nfrom mcp.echo import echo\nfrom mcp.math import math\n\n\napp = FastAPI()\n\n# Use the session manager's lifespan\napp = FastAPI(lifespan=lambda app: echo.mcp.session_manager.run())\napp.mount(\"/echo\", echo.mcp.streamable_http_app())\napp.mount(\"/math\", math.mcp.streamable_http_app())\n```\n\nFor low level server with Streamable HTTP implementations, see:\n- Stateful server: [`examples/servers/simple-streamablehttp/`](examples/servers/simple-streamablehttp/)\n- Stateless server: [`examples/servers/simple-streamablehttp-stateless/`](examples/servers/simple-streamablehttp-stateless/)\n\n\n\nThe streamable HTTP transport supports:\n- Stateful and stateless operation modes\n- Resumability with event stores\n- JSON or SSE response formats  \n- Better scalability for multi-node deployments\n\n\n### Mounting to an Existing ASGI Server\n\n> **Note**: SSE transport is being superseded by [Streamable HTTP transport](https://modelcontextprotocol.io/specification/2025-03-26/basic/transports#streamable-http).\n\nYou can mount the SSE server to an existing ASGI server using the `sse_app` method. This allows you to integrate the SSE server with other ASGI applications.\n\n```python\nfrom starlette.applications import Starlette\nfrom starlette.routing import Mount, Host\nfrom mcp.server.fastmcp import FastMCP\n\n\nmcp = FastMCP(\"My App\")\n\n# Mount the SSE server to the existing ASGI server\napp = Starlette(\n    routes=[\n        Mount('/', app=mcp.sse_app()),\n    ]\n)\n\n# or dynamically mount as host\napp.router.routes.append(Host('mcp.acme.corp', app=mcp.sse_app()))\n```\n\nWhen mounting multiple MCP servers under different paths, you can configure the mount path in several ways:\n\n```python\nfrom starlette.applications import Starlette\nfrom starlette.routing import Mount\nfrom mcp.server.fastmcp import FastMCP\n\n# Create multiple MCP servers\ngithub_mcp = FastMCP(\"GitHub API\")\nbrowser_mcp = FastMCP(\"Browser\")\ncurl_mcp = FastMCP(\"Curl\")\nsearch_mcp = FastMCP(\"Search\")\n\n# Method 1: Configure mount paths via settings (recommended for persistent configuration)\ngithub_mcp.settings.mount_path = \"/github\"\nbrowser_mcp.settings.mount_path = \"/browser\"\n\n# Method 2: Pass mount path directly to sse_app (preferred for ad-hoc mounting)\n# This approach doesn't modify the server's settings permanently\n\n# Create Starlette app with multiple mounted servers\napp = Starlette(\n    routes=[\n        # Using settings-based configuration\n        Mount(\"/github\", app=github_mcp.sse_app()),\n        Mount(\"/browser\", app=browser_mcp.sse_app()),\n        # Using direct mount path parameter\n        Mount(\"/curl\", app=curl_mcp.sse_app(\"/curl\")),\n        Mount(\"/search\", app=search_mcp.sse_app(\"/search\")),\n    ]\n)\n\n# Method 3: For direct execution, you can also pass the mount path to run()\nif __name__ == \"__main__\":\n    search_mcp.run(transport=\"sse\", mount_path=\"/search\")\n```\n\nFor more information on mounting applications in Starlette, see the [Starlette documentation](https://www.starlette.io/routing/#submounting-routes).\n\n## Examples\n\n### Echo Server\n\nA simple server demonstrating resources, tools, and prompts:\n\n```python\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Echo\")\n\n\n@mcp.resource(\"echo://{message}\")\ndef echo_resource(message: str) -> str:\n    \"\"\"Echo a message as a resource\"\"\"\n    return f\"Resource echo: {message}\"\n\n\n@mcp.tool()\ndef echo_tool(message: str) -> str:\n    \"\"\"Echo a message as a tool\"\"\"\n    return f\"Tool echo: {message}\"\n\n\n@mcp.prompt()\ndef echo_prompt(message: str) -> str:\n    \"\"\"Create an echo prompt\"\"\"\n    return f\"Please process this message: {message}\"\n```\n\n### SQLite Explorer\n\nA more complex example showing database integration:\n\n```python\nimport sqlite3\n\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"SQLite Explorer\")\n\n\n@mcp.resource(\"schema://main\")\ndef get_schema() -> str:\n    \"\"\"Provide the database schema as a resource\"\"\"\n    conn = sqlite3.connect(\"database.db\")\n    schema = conn.execute(\"SELECT sql FROM sqlite_master WHERE type='table'\").fetchall()\n    return \"\\n\".join(sql[0] for sql in schema if sql[0])\n\n\n@mcp.tool()\ndef query_data(sql: str) -> str:\n    \"\"\"Execute SQL queries safely\"\"\"\n    conn = sqlite3.connect(\"database.db\")\n    try:\n        result = conn.execute(sql).fetchall()\n        return \"\\n\".join(str(row) for row in result)\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n```\n\n## Advanced Usage\n\n### Low-Level Server\n\nFor more control, you can use the low-level server implementation directly. This gives you full access to the protocol and allows you to customize every aspect of your server, including lifecycle management through the lifespan API:\n\n```python\nfrom contextlib import asynccontextmanager\nfrom collections.abc import AsyncIterator\n\nfrom fake_database import Database  # Replace with your actual DB type\n\nfrom mcp.server import Server\n\n\n@asynccontextmanager\nasync def server_lifespan(server: Server) -> AsyncIterator[dict]:\n    \"\"\"Manage server startup and shutdown lifecycle.\"\"\"\n    # Initialize resources on startup\n    db = await Database.connect()\n    try:\n        yield {\"db\": db}\n    finally:\n        # Clean up on shutdown\n        await db.disconnect()\n\n\n# Pass lifespan to server\nserver = Server(\"example-server\", lifespan=server_lifespan)\n\n\n# Access lifespan context in handlers\n@server.call_tool()\nasync def query_db(name: str, arguments: dict) -> list:\n    ctx = server.request_context\n    db = ctx.lifespan_context[\"db\"]\n    return await db.query(arguments[\"query\"])\n```\n\nThe lifespan API provides:\n- A way to initialize resources when the server starts and clean them up when it stops\n- Access to initialized resources through the request context in handlers\n- Type-safe context passing between lifespan and request handlers\n\n```python\nimport mcp.server.stdio\nimport mcp.types as types\nfrom mcp.server.lowlevel import NotificationOptions, Server\nfrom mcp.server.models import InitializationOptions\n\n# Create a server instance\nserver = Server(\"example-server\")\n\n\n@server.list_prompts()\nasync def handle_list_prompts() -> list[types.Prompt]:\n    return [\n        types.Prompt(\n            name=\"example-prompt\",\n            description=\"An example prompt template\",\n            arguments=[\n                types.PromptArgument(\n                    name=\"arg1\", description=\"Example argument\", required=True\n                )\n            ],\n        )\n    ]\n\n\n@server.get_prompt()\nasync def handle_get_prompt(\n    name: str, arguments: dict[str, str] | None\n) -> types.GetPromptResult:\n    if name != \"example-prompt\":\n        raise ValueError(f\"Unknown prompt: {name}\")\n\n    return types.GetPromptResult(\n        description=\"Example prompt\",\n        messages=[\n            types.PromptMessage(\n                role=\"user\",\n                content=types.TextContent(type=\"text\", text=\"Example prompt text\"),\n            )\n        ],\n    )\n\n\nasync def run():\n    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):\n        await server.run(\n            read_stream,\n            write_stream,\n            InitializationOptions(\n                server_name=\"example\",\n                server_version=\"0.1.0\",\n                capabilities=server.get_capabilities(\n                    notification_options=NotificationOptions(),\n                    experimental_capabilities={},\n                ),\n            ),\n        )\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    asyncio.run(run())\n```\n\n### Writing MCP Clients\n\nThe SDK provides a high-level client interface for connecting to MCP servers using various [transports](https://modelcontextprotocol.io/specification/2025-03-26/basic/transports):\n\n```python\nfrom mcp import ClientSession, StdioServerParameters, types\nfrom mcp.client.stdio import stdio_client\n\n# Create server parameters for stdio connection\nserver_params = StdioServerParameters(\n    command=\"python\",  # Executable\n    args=[\"example_server.py\"],  # Optional command line arguments\n    env=None,  # Optional environment variables\n)\n\n\n# Optional: create a sampling callback\nasync def handle_sampling_message(\n    message: types.CreateMessageRequestParams,\n) -> types.CreateMessageResult:\n    return types.CreateMessageResult(\n        role=\"assistant\",\n        content=types.TextContent(\n            type=\"text\",\n            text=\"Hello, world! from model\",\n        ),\n        model=\"gpt-3.5-turbo\",\n        stopReason=\"endTurn\",\n    )\n\n\nasync def run():\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(\n            read, write, sampling_callback=handle_sampling_message\n        ) as session:\n            # Initialize the connection\n            await session.initialize()\n\n            # List available prompts\n            prompts = await session.list_prompts()\n\n            # Get a prompt\n            prompt = await session.get_prompt(\n                \"example-prompt\", arguments={\"arg1\": \"value\"}\n            )\n\n            # List available resources\n            resources = await session.list_resources()\n\n            # List available tools\n            tools = await session.list_tools()\n\n            # Read a resource\n            content, mime_type = await session.read_resource(\"file://some/path\")\n\n            # Call a tool\n            result = await session.call_tool(\"tool-name\", arguments={\"arg1\": \"value\"})\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    asyncio.run(run())\n```\n\nClients can also connect using [Streamable HTTP transport](https://modelcontextprotocol.io/specification/2025-03-26/basic/transports#streamable-http):\n\n```python\nfrom mcp.client.streamable_http import streamablehttp_client\nfrom mcp import ClientSession\n\n\nasync def main():\n    # Connect to a streamable HTTP server\n    async with streamablehttp_client(\"example/mcp\") as (\n        read_stream,\n        write_stream,\n        _,\n    ):\n        # Create a session using the client streams\n        async with ClientSession(read_stream, write_stream) as session:\n            # Initialize the connection\n            await session.initialize()\n            # Call a tool\n            tool_result = await session.call_tool(\"echo\", {\"message\": \"hello\"})\n```\n\n### MCP Primitives\n\nThe MCP protocol defines three core primitives that servers can implement:\n\n| Primitive | Control               | Description                                         | Example Use                  |\n|-----------|-----------------------|-----------------------------------------------------|------------------------------|\n| Prompts   | User-controlled       | Interactive templates invoked by user choice        | Slash commands, menu options |\n| Resources | Application-controlled| Contextual data managed by the client application   | File contents, API responses |\n| Tools     | Model-controlled      | Functions exposed to the LLM to take actions        | API calls, data updates      |\n\n### Server Capabilities\n\nMCP servers declare capabilities during initialization:\n\n| Capability  | Feature Flag                 | Description                        |\n|-------------|------------------------------|------------------------------------|\n| `prompts`   | `listChanged`                | Prompt template management         |\n| `resources` | `subscribe`<br/>`listChanged`| Resource exposure and updates      |\n| `tools`     | `listChanged`                | Tool discovery and execution       |\n| `logging`   | -                            | Server logging configuration       |\n| `completion`| -                            | Argument completion suggestions    |\n\n## Documentation\n\n- [Model Context Protocol documentation](https://modelcontextprotocol.io)\n- [Model Context Protocol specification](https://spec.modelcontextprotocol.io)\n- [Officially supported servers](https://github.com/modelcontextprotocol/servers)\n\n## Contributing\n\nWe are passionate about supporting contributors of all levels of experience and would love to see you get involved in the project. See the [contributing guide](CONTRIBUTING.md) to get started.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n"
  },
  {
    "name": "mcp-agent",
    "url": "https://github.com/davidatoms/mcp-agent",
    "description": "Build effective agents using Model Context Protocol and simple workflow patterns",
    "type": "fork",
    "updated_at": "2025-05-09T05:26:53Z",
    "readme": "<p align=\"center\">\n  <img src=\"https://github.com/user-attachments/assets/6f4e40c4-dc88-47b6-b965-5856b69416d2\" alt=\"Logo\" width=\"300\" />\n</p>\n\n<p align=\"center\">\n  <em>Build effective agents with Model Context Protocol using simple, composable patterns.</em>\n\n<p align=\"center\">\n  <a href=\"https://github.com/lastmile-ai/mcp-agent/tree/main/examples\" target=\"_blank\"><strong>Examples</strong></a>\n  |\n  <a href=\"https://www.anthropic.com/research/building-effective-agents\" target=\"_blank\"><strong>Building Effective Agents</strong></a>\n  |\n  <a href=\"https://modelcontextprotocol.io/introduction\" target=\"_blank\"><strong>MCP</strong></a>\n</p>\n\n<p align=\"center\">\n<a href=\"https://pypi.org/project/mcp-agent/\"><img src=\"https://img.shields.io/pypi/v/mcp-agent?color=%2334D058&label=pypi\" /></a>\n<a href=\"https://github.com/lastmile-ai/mcp-agent/issues\"><img src=\"https://img.shields.io/github/issues-raw/lastmile-ai/mcp-agent\" /></a>\n<a href=\"https://lmai.link/discord/mcp-agent\"><img src=\"https://shields.io/discord/1089284610329952357\" alt=\"discord\" /></a>\n<img alt=\"Pepy Total Downloads\" src=\"https://img.shields.io/pepy/dt/mcp-agent?label=pypi%20%7C%20downloads\"/>\n<a href=\"https://github.com/lastmile-ai/mcp-agent/blob/main/LICENSE\"><img src=\"https://img.shields.io/pypi/l/mcp-agent\" /></a>\n</p>\n\n## Overview\n\n**`mcp-agent`** is a simple, composable framework to build agents using [Model Context Protocol](https://modelcontextprotocol.io/introduction).\n\n**Inspiration**: Anthropic announced 2 foundational updates for AI application developers:\n\n1. [Model Context Protocol](https://www.anthropic.com/news/model-context-protocol) - a standardized interface to let any software be accessible to AI assistants via MCP servers.\n2. [Building Effective Agents](https://www.anthropic.com/research/building-effective-agents) - a seminal writeup on simple, composable patterns for building production-ready AI agents.\n\n`mcp-agent` puts these two foundational pieces into an AI application framework:\n\n1. It handles the pesky business of managing the lifecycle of MCP server connections so you don't have to.\n2. It implements every pattern described in Building Effective Agents, and does so in a _composable_ way, allowing you to chain these patterns together.\n3. **Bonus**: It implements [OpenAI's Swarm](https://github.com/openai/swarm) pattern for multi-agent orchestration, but in a model-agnostic way.\n\nAltogether, this is the simplest and easiest way to build robust agent applications. Much like MCP, this project is in early development.\nWe welcome all kinds of [contributions](/CONTRIBUTING.md), feedback and your help in growing this to become a new standard.\n\n## Get Started\n\nWe recommend using [uv](https://docs.astral.sh/uv/) to manage your Python projects:\n\n```bash\nuv add \"mcp-agent\"\n```\n\nAlternatively:\n\n```bash\npip install mcp-agent\n```\n\n### Quickstart\n\n> [!TIP]\n> The [`examples`](/examples) directory has several example applications to get started with.\n> To run an example, clone this repo, then:\n>\n> ```bash\n> cd examples/basic/mcp_basic_agent # Or any other example\n> cp mcp_agent.secrets.yaml.example mcp_agent.secrets.yaml # Update API keys\n> uv run main.py\n> ```\n\nHere is a basic \"finder\" agent that uses the fetch and filesystem servers to look up a file, read a blog and write a tweet. [Example link](./examples/basic/mcp_basic_agent/):\n\n<details open>\n<summary>finder_agent.py</summary>\n\n```python\nimport asyncio\nimport os\n\nfrom mcp_agent.app import MCPApp\nfrom mcp_agent.agents.agent import Agent\nfrom mcp_agent.workflows.llm.augmented_llm_openai import OpenAIAugmentedLLM\n\napp = MCPApp(name=\"hello_world_agent\")\n\nasync def example_usage():\n    async with app.run() as mcp_agent_app:\n        logger = mcp_agent_app.logger\n        # This agent can read the filesystem or fetch URLs\n        finder_agent = Agent(\n            name=\"finder\",\n            instruction=\"\"\"You can read local files or fetch URLs.\n                Return the requested information when asked.\"\"\",\n            server_names=[\"fetch\", \"filesystem\"], # MCP servers this Agent can use\n        )\n\n        async with finder_agent:\n            # Automatically initializes the MCP servers and adds their tools for LLM use\n            tools = await finder_agent.list_tools()\n            logger.info(f\"Tools available:\", data=tools)\n\n            # Attach an OpenAI LLM to the agent (defaults to GPT-4o)\n            llm = await finder_agent.attach_llm(OpenAIAugmentedLLM)\n\n            # This will perform a file lookup and read using the filesystem server\n            result = await llm.generate_str(\n                message=\"Show me what's in README.md verbatim\"\n            )\n            logger.info(f\"README.md contents: {result}\")\n\n            # Uses the fetch server to fetch the content from URL\n            result = await llm.generate_str(\n                message=\"Print the first two paragraphs from https://www.anthropic.com/research/building-effective-agents\"\n            )\n            logger.info(f\"Blog intro: {result}\")\n\n            # Multi-turn interactions by default\n            result = await llm.generate_str(\"Summarize that in a 128-char tweet\")\n            logger.info(f\"Tweet: {result}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(example_usage())\n\n```\n\n</details>\n\n<details>\n<summary>mcp_agent.config.yaml</summary>\n\n```yaml\nexecution_engine: asyncio\nlogger:\n  transports: [console] # You can use [file, console] for both\n  level: debug\n  path: \"logs/mcp-agent.jsonl\" # Used for file transport\n  # For dynamic log filenames:\n  # path_settings:\n  #   path_pattern: \"logs/mcp-agent-{unique_id}.jsonl\"\n  #   unique_id: \"timestamp\"  # Or \"session_id\"\n  #   timestamp_format: \"%Y%m%d_%H%M%S\"\n\nmcp:\n  servers:\n    fetch:\n      command: \"uvx\"\n      args: [\"mcp-server-fetch\"]\n    filesystem:\n      command: \"npx\"\n      args:\n        [\n          \"-y\",\n          \"@modelcontextprotocol/server-filesystem\",\n          \"<add_your_directories>\",\n        ]\n\nopenai:\n  # Secrets (API keys, etc.) are stored in an mcp_agent.secrets.yaml file which can be gitignored\n  default_model: gpt-4o\n```\n\n</details>\n\n<details>\n<summary>Agent output</summary>\n<img width=\"2398\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/eaa60fdf-bcc6-460b-926e-6fa8534e9089\" />\n</details>\n\n## Table of Contents\n\n- [Why use mcp-agent?](#why-use-mcp-agent)\n- [Example Applications](#examples)\n  - [Claude Desktop](#claude-desktop)\n  - [Streamlit](#streamlit)\n    - [Gmail Agent](#gmail-agent)\n    - [RAG](#simple-rag-chatbot)\n  - [Marimo](#marimo)\n  - [Python](#python)\n    - [Swarm (CLI)](#swarm)\n- [Core Concepts](#core-components)\n- [Workflows Patterns](#workflows)\n  - [Augmented LLM](#augmentedllm)\n  - [Parallel](#parallel)\n  - [Router](#router)\n  - [Intent-Classifier](#intentclassifier)\n  - [Orchestrator-Workers](#orchestrator-workers)\n  - [Evaluator-Optimizer](#evaluator-optimizer)\n  - [OpenAI Swarm](#swarm-1)\n- [Advanced](#advanced)\n  - [Composing multiple workflows](#composability)\n  - [Signaling and Human input](#signaling-and-human-input)\n  - [App Config](#app-config)\n  - [MCP Server Management](#mcp-server-management)\n- [Contributing](#contributing)\n- [Roadmap](#roadmap)\n- [FAQs](#faqs)\n\n## Why use `mcp-agent`?\n\nThere are too many AI frameworks out there already. But `mcp-agent` is the only one that is purpose-built for a shared protocol - [MCP](https://modelcontextprotocol.io/introduction). It is also the most lightweight, and is closer to an agent pattern library than a framework.\n\nAs [more services become MCP-aware](https://github.com/punkpeye/awesome-mcp-servers), you can use mcp-agent to build robust and controllable AI agents that can leverage those services out-of-the-box.\n\n## Examples\n\nBefore we go into the core concepts of mcp-agent, let's show what you can build with it.\n\nIn short, you can build any kind of AI application with mcp-agent: multi-agent collaborative workflows, human-in-the-loop workflows, RAG pipelines and more.\n\n### Claude Desktop\n\nYou can integrate mcp-agent apps into MCP clients like Claude Desktop.\n\n#### mcp-agent server\n\nThis app wraps an mcp-agent application inside an MCP server, and exposes that server to Claude Desktop.\nThe app exposes agents and workflows that Claude Desktop can invoke to service of the user's request.\n\nhttps://github.com/user-attachments/assets/7807cffd-dba7-4f0c-9c70-9482fd7e0699\n\nThis demo shows a multi-agent evaluation task where each agent evaluates aspects of an input poem, and\nthen an aggregator summarizes their findings into a final response.\n\n**Details**: Starting from a user's request over text, the application:\n\n- dynamically defines agents to do the job\n- uses the appropriate workflow to orchestrate those agents (in this case the Parallel workflow)\n\n**Link to code**: [examples/basic/mcp_agent_server](./examples/basic/mcp_agent_server)\n\n> [!NOTE]\n> Huge thanks to [Jerron Lim (@StreetLamb)](https://github.com/StreetLamb)\n> for developing and contributing this example!\n\n### Streamlit\n\nYou can deploy mcp-agent apps using Streamlit.\n\n#### Gmail agent\n\nThis app is able to perform read and write actions on gmail using text prompts -- i.e. read, delete, send emails, mark as read/unread, etc.\nIt uses an MCP server for Gmail.\n\nhttps://github.com/user-attachments/assets/54899cac-de24-4102-bd7e-4b2022c956e3\n\n**Link to code**: [gmail-mcp-server](https://github.com/jasonsum/gmail-mcp-server/blob/add-mcp-agent-streamlit/streamlit_app.py)\n\n> [!NOTE]\n> Huge thanks to [Jason Summer (@jasonsum)](https://github.com/jasonsum)\n> for developing and contributing this example!\n\n#### Simple RAG Chatbot\n\nThis app uses a Qdrant vector database (via an MCP server) to do Q&A over a corpus of text.\n\nhttps://github.com/user-attachments/assets/f4dcd227-cae9-4a59-aa9e-0eceeb4acaf4\n\n**Link to code**: [examples/usecases/streamlit_mcp_rag_agent](./examples/usecases/streamlit_mcp_rag_agent/)\n\n> [!NOTE]\n> Huge thanks to [Jerron Lim (@StreetLamb)](https://github.com/StreetLamb)\n> for developing and contributing this example!\n\n### Marimo\n\n[Marimo](https://github.com/marimo-team/marimo) is a reactive Python notebook that replaces Jupyter and Streamlit.\nHere's the \"file finder\" agent from [Quickstart](#quickstart) implemented in Marimo:\n\n<img src=\"https://github.com/user-attachments/assets/139a95a5-e3ac-4ea7-9c8f-bad6577e8597\" width=\"400\"/>\n\n**Link to code**: [examples/usecases/marimo_mcp_basic_agent](./examples/usecases/marimo_mcp_basic_agent/)\n\n> [!NOTE]\n> Huge thanks to [Akshay Agrawal (@akshayka)](https://github.com/akshayka)\n> for developing and contributing this example!\n\n### Python\n\nYou can write mcp-agent apps as Python scripts or Jupyter notebooks.\n\n#### Swarm\n\nThis example demonstrates a multi-agent setup for handling different customer service requests in an airline context using the Swarm workflow pattern. The agents can triage requests, handle flight modifications, cancellations, and lost baggage cases.\n\nhttps://github.com/user-attachments/assets/b314d75d-7945-4de6-965b-7f21eb14a8bd\n\n**Link to code**: [examples/workflows/workflow_swarm](./examples/workflows/workflow_swarm/)\n\n## Core Components\n\nThe following are the building blocks of the mcp-agent framework:\n\n- **[MCPApp](./src/mcp_agent/app.py)**: global state and app configuration\n- **MCP server management**: [`gen_client`](./src/mcp_agent/mcp/gen_client.py) and [`MCPConnectionManager`](./src/mcp_agent/mcp/mcp_connection_manager.py) to easily connect to MCP servers.\n- **[Agent](./src/mcp_agent/agents/agent.py)**: An Agent is an entity that has access to a set of MCP servers and exposes them to an LLM as tool calls. It has a name and purpose (instruction).\n- **[AugmentedLLM](./src/mcp_agent/workflows/llm/augmented_llm.py)**: An LLM that is enhanced with tools provided from a collection of MCP servers. Every Workflow pattern described below is an `AugmentedLLM` itself, allowing you to compose and chain them together.\n\nEverything in the framework is a derivative of these core capabilities.\n\n## Workflows\n\nmcp-agent provides implementations for every pattern in Anthropic\u2019s [Building Effective Agents](https://www.anthropic.com/research/building-effective-agents), as well as the OpenAI [Swarm](https://github.com/openai/swarm) pattern.\nEach pattern is model-agnostic, and exposed as an `AugmentedLLM`, making everything very composable.\n\n### AugmentedLLM\n\n[AugmentedLLM](./src/mcp_agent/workflows/llm/augmented_llm.py) is an LLM that has access to MCP servers and functions via Agents.\n\nLLM providers implement the AugmentedLLM interface to expose 3 functions:\n\n- `generate`: Generate message(s) given a prompt, possibly over multiple iterations and making tool calls as needed.\n- `generate_str`: Calls `generate` and returns result as a string output.\n- `generate_structured`: Uses [Instructor](https://github.com/instructor-ai/instructor) to return the generated result as a Pydantic model.\n\nAdditionally, `AugmentedLLM` has memory, to keep track of long or short-term history.\n\n<details>\n<summary>Example</summary>\n\n```python\nfrom mcp_agent.agents.agent import Agent\nfrom mcp_agent.workflows.llm.augmented_llm_anthropic import AnthropicAugmentedLLM\n\nfinder_agent = Agent(\n    name=\"finder\",\n    instruction=\"You are an agent with filesystem + fetch access. Return the requested file or URL contents.\",\n    server_names=[\"fetch\", \"filesystem\"],\n)\n\nasync with finder_agent:\n   llm = await finder_agent.attach_llm(AnthropicAugmentedLLM)\n\n   result = await llm.generate_str(\n      message=\"Print the first 2 paragraphs of https://www.anthropic.com/research/building-effective-agents\",\n      # Can override model, tokens and other defaults\n   )\n   logger.info(f\"Result: {result}\")\n\n   # Multi-turn conversation\n   result = await llm.generate_str(\n      message=\"Summarize those paragraphs in a 128 character tweet\",\n   )\n   logger.info(f\"Result: {result}\")\n```\n\n</details>\n\n### [Parallel](src/mcp_agent/workflows/parallel/parallel_llm.py)\n\n![Parallel workflow (Image credit: Anthropic)](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F406bb032ca007fd1624f261af717d70e6ca86286-2401x1000.png&w=3840&q=75)\n\nFan-out tasks to multiple sub-agents and fan-in the results. Each subtask is an AugmentedLLM, as is the overall Parallel workflow, meaning each subtask can optionally be a more complex workflow itself.\n\n> [!NOTE]\n>\n> **[Link to full example](examples/workflows/workflow_parallel/main.py)**\n\n<details>\n<summary>Example</summary>\n\n```python\nproofreader = Agent(name=\"proofreader\", instruction=\"Review grammar...\")\nfact_checker = Agent(name=\"fact_checker\", instruction=\"Check factual consistency...\")\nstyle_enforcer = Agent(name=\"style_enforcer\", instruction=\"Enforce style guidelines...\")\n\ngrader = Agent(name=\"grader\", instruction=\"Combine feedback into a structured report.\")\n\nparallel = ParallelLLM(\n    fan_in_agent=grader,\n    fan_out_agents=[proofreader, fact_checker, style_enforcer],\n    llm_factory=OpenAIAugmentedLLM,\n)\n\nresult = await parallel.generate_str(\"Student short story submission: ...\", RequestParams(model=\"gpt4-o\"))\n```\n\n</details>\n\n### [Router](src/mcp_agent/workflows/router/)\n\n![Router workflow (Image credit: Anthropic)](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F5c0c0e9fe4def0b584c04d37849941da55e5e71c-2401x1000.png&w=3840&q=75)\n\nGiven an input, route to the `top_k` most relevant categories. A category can be an Agent, an MCP server or a regular function.\n\nmcp-agent provides several router implementations, including:\n\n- [`EmbeddingRouter`](src/mcp_agent/workflows/router/router_embedding.py): uses embedding models for classification\n- [`LLMRouter`](src/mcp_agent/workflows/router/router_llm.py): uses LLMs for classification\n\n> [!NOTE]\n>\n> **[Link to full example](examples/workflows/workflow_router/main.py)**\n\n<details>\n<summary>Example</summary>\n\n```python\ndef print_hello_world:\n     print(\"Hello, world!\")\n\nfinder_agent = Agent(name=\"finder\", server_names=[\"fetch\", \"filesystem\"])\nwriter_agent = Agent(name=\"writer\", server_names=[\"filesystem\"])\n\nllm = OpenAIAugmentedLLM()\nrouter = LLMRouter(\n    llm=llm,\n    agents=[finder_agent, writer_agent],\n    functions=[print_hello_world],\n)\n\nresults = await router.route( # Also available: route_to_agent, route_to_server\n    request=\"Find and print the contents of README.md verbatim\",\n    top_k=1\n)\nchosen_agent = results[0].result\nasync with chosen_agent:\n    ...\n```\n\n</details>\n\n### [IntentClassifier](src/mcp_agent/workflows/intent_classifier/)\n\nA close sibling of Router, the Intent Classifier pattern identifies the `top_k` Intents that most closely match a given input.\nJust like a Router, mcp-agent provides both an [embedding](src/mcp_agent/workflows/intent_classifier/intent_classifier_embedding.py) and [LLM-based](src/mcp_agent/workflows/intent_classifier/intent_classifier_llm.py) intent classifier.\n\n### [Evaluator-Optimizer](src/mcp_agent/workflows/evaluator_optimizer/evaluator_optimizer.py)\n\n![Evaluator-optimizer workflow (Image credit: Anthropic)](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F14f51e6406ccb29e695da48b17017e899a6119c7-2401x1000.png&w=3840&q=75)\n\nOne LLM (the \u201coptimizer\u201d) refines a response, another (the \u201cevaluator\u201d) critiques it until a response exceeds a quality criteria.\n\n> [!NOTE]\n>\n> **[Link to full example](examples/workflows/workflow_evaluator_optimizer/main.py)**\n\n<details>\n<summary>Example</summary>\n\n```python\noptimizer = Agent(name=\"cover_letter_writer\", server_names=[\"fetch\"], instruction=\"Generate a cover letter ...\")\nevaluator = Agent(name=\"critiquer\", instruction=\"Evaluate clarity, specificity, relevance...\")\n\nllm = EvaluatorOptimizerLLM(\n    optimizer=optimizer,\n    evaluator=evaluator,\n    llm_factory=OpenAIAugmentedLLM,\n    min_rating=QualityRating.EXCELLENT, # Keep iterating until the minimum quality bar is reached\n)\n\nresult = await eo_llm.generate_str(\"Write a job cover letter for an AI framework developer role at LastMile AI.\")\nprint(\"Final refined cover letter:\", result)\n```\n\n</details>\n\n### [Orchestrator-workers](src/mcp_agent/workflows/orchestrator/orchestrator.py)\n\n![Orchestrator workflow (Image credit: Anthropic)](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F8985fc683fae4780fb34eab1365ab78c7e51bc8e-2401x1000.png&w=3840&q=75)\n\nA higher-level LLM generates a plan, then assigns them to sub-agents, and synthesizes the results.\nThe Orchestrator workflow automatically parallelizes steps that can be done in parallel, and blocks on dependencies.\n\n> [!NOTE]\n>\n> **[Link to full example](examples/workflows/workflow_orchestrator_worker/main.py)**\n\n<details>\n<summary>Example</summary>\n\n```python\nfinder_agent = Agent(name=\"finder\", server_names=[\"fetch\", \"filesystem\"])\nwriter_agent = Agent(name=\"writer\", server_names=[\"filesystem\"])\nproofreader = Agent(name=\"proofreader\", ...)\nfact_checker = Agent(name=\"fact_checker\", ...)\nstyle_enforcer = Agent(name=\"style_enforcer\", instructions=\"Use APA style guide from ...\", server_names=[\"fetch\"])\n\norchestrator = Orchestrator(\n    llm_factory=AnthropicAugmentedLLM,\n    available_agents=[finder_agent, writer_agent, proofreader, fact_checker, style_enforcer],\n)\n\ntask = \"Load short_story.md, evaluate it, produce a graded_report.md with multiple feedback aspects.\"\nresult = await orchestrator.generate_str(task, RequestParams(model=\"gpt-4o\"))\nprint(result)\n```\n\n</details>\n\n### [Swarm](src/mcp_agent/workflows/swarm/swarm.py)\n\nOpenAI has an experimental multi-agent pattern called [Swarm](https://github.com/openai/swarm), which we provide a model-agnostic reference implementation for in mcp-agent.\n\n<img src=\"https://github.com/openai/swarm/blob/main/assets/swarm_diagram.png?raw=true\" width=500 />\n\nThe mcp-agent Swarm pattern works seamlessly with MCP servers, and is exposed as an `AugmentedLLM`, allowing for composability with other patterns above.\n\n> [!NOTE]\n>\n> **[Link to full example](examples/workflows/workflow_swarm/main.py)**\n\n<details>\n<summary>Example</summary>\n\n```python\ntriage_agent = SwarmAgent(...)\nflight_mod_agent = SwarmAgent(...)\nlost_baggage_agent = SwarmAgent(...)\n\n# The triage agent decides whether to route to flight_mod_agent or lost_baggage_agent\nswarm = AnthropicSwarm(agent=triage_agent, context_variables={...})\n\ntest_input = \"My bag was not delivered!\"\nresult = await swarm.generate_str(test_input)\nprint(\"Result:\", result)\n```\n\n</details>\n\n## Advanced\n\n### Composability\n\nAn example of composability is using an [Evaluator-Optimizer](#evaluator-optimizer) workflow as the planner LLM inside\nthe [Orchestrator](#orchestrator-workers) workflow. Generating a high-quality plan to execute is important for robust behavior, and an evaluator-optimizer can help ensure that.\n\nDoing so is seamless in mcp-agent, because each workflow is implemented as an `AugmentedLLM`.\n\n<details>\n<summary>Example</summary>\n\n```python\noptimizer = Agent(name=\"plan_optimizer\", server_names=[...], instruction=\"Generate a plan given an objective ...\")\nevaluator = Agent(name=\"plan_evaluator\", instruction=\"Evaluate logic, ordering and precision of plan......\")\n\nplanner_llm = EvaluatorOptimizerLLM(\n    optimizer=optimizer,\n    evaluator=evaluator,\n    llm_factory=OpenAIAugmentedLLM,\n    min_rating=QualityRating.EXCELLENT,\n)\n\norchestrator = Orchestrator(\n    llm_factory=AnthropicAugmentedLLM,\n    available_agents=[finder_agent, writer_agent, proofreader, fact_checker, style_enforcer],\n    planner=planner_llm # It's that simple\n)\n\n...\n```\n\n</details>\n\n### Signaling and Human Input\n\n**Signaling**: The framework can pause/resume tasks. The agent or LLM might \u201csignal\u201d that it needs user input, so the workflow awaits. A developer may signal during a workflow to seek approval or review before continuing with a workflow.\n\n**Human Input**: If an Agent has a `human_input_callback`, the LLM can call a `__human_input__` tool to request user input mid-workflow.\n\n<details>\n<summary>Example</summary>\n\nThe [Swarm example](examples/workflows/workflow_swarm/main.py) shows this in action.\n\n```python\nfrom mcp_agent.human_input.handler import console_input_callback\n\nlost_baggage = SwarmAgent(\n    name=\"Lost baggage traversal\",\n    instruction=lambda context_variables: f\"\"\"\n        {\n        FLY_AIR_AGENT_PROMPT.format(\n            customer_context=context_variables.get(\"customer_context\", \"None\"),\n            flight_context=context_variables.get(\"flight_context\", \"None\"),\n        )\n    }\\n Lost baggage policy: policies/lost_baggage_policy.md\"\"\",\n    functions=[\n        escalate_to_agent,\n        initiate_baggage_search,\n        transfer_to_triage,\n        case_resolved,\n    ],\n    server_names=[\"fetch\", \"filesystem\"],\n    human_input_callback=console_input_callback, # Request input from the console\n)\n```\n\n</details>\n\n### App Config\n\nCreate an [`mcp_agent.config.yaml`](/schema/mcp-agent.config.schema.json) and a gitignored [`mcp_agent.secrets.yaml`](./examples/basic/mcp_basic_agent/mcp_agent.secrets.yaml.example) to define MCP app configuration. This controls logging, execution, LLM provider APIs, and MCP server configuration.\n\n### MCP server management\n\nmcp-agent makes it trivial to connect to MCP servers. Create an [`mcp_agent.config.yaml`](/schema/mcp-agent.config.schema.json) to define server configuration under the `mcp` section:\n\n```yaml\nmcp:\n  servers:\n    fetch:\n      command: \"uvx\"\n      args: [\"mcp-server-fetch\"]\n      description: \"Fetch content at URLs from the world wide web\"\n```\n\n#### [`gen_client`](src/mcp_agent/mcp/gen_client.py)\n\nManage the lifecycle of an MCP server within an async context manager:\n\n```python\nfrom mcp_agent.mcp.gen_client import gen_client\n\nasync with gen_client(\"fetch\") as fetch_client:\n    # Fetch server is initialized and ready to use\n    result = await fetch_client.list_tools()\n\n# Fetch server is automatically disconnected/shutdown\n```\n\nThe gen_client function makes it easy to spin up connections to MCP servers.\n\n#### Persistent server connections\n\nIn many cases, you want an MCP server to stay online for persistent use (e.g. in a multi-step tool use workflow).\nFor persistent connections, use:\n\n- [`connect`](<(src/mcp_agent/mcp/gen_client.py)>) and [`disconnect`](src/mcp_agent/mcp/gen_client.py)\n\n```python\nfrom mcp_agent.mcp.gen_client import connect, disconnect\n\nfetch_client = None\ntry:\n     fetch_client = connect(\"fetch\")\n     result = await fetch_client.list_tools()\nfinally:\n     disconnect(\"fetch\")\n```\n\n- [`MCPConnectionManager`](src/mcp_agent/mcp/mcp_connection_manager.py)\n  For even more fine-grained control over server connections, you can use the MCPConnectionManager.\n\n<details>\n<summary>Example</summary>\n\n```python\nfrom mcp_agent.context import get_current_context\nfrom mcp_agent.mcp.mcp_connection_manager import MCPConnectionManager\n\ncontext = get_current_context()\nconnection_manager = MCPConnectionManager(context.server_registry)\n\nasync with connection_manager:\nfetch_client = await connection_manager.get_server(\"fetch\") # Initializes fetch server\nresult = fetch_client.list_tool()\nfetch_client2 = await connection_manager.get_server(\"fetch\") # Reuses same server connection\n\n# All servers managed by connection manager are automatically disconnected/shut down\n```\n\n</details>\n\n#### MCP Server Aggregator\n\n[`MCPAggregator`](src/mcp_agent/mcp/mcp_aggregator.py) acts as a \"server-of-servers\".\nIt provides a single MCP server interface for interacting with multiple MCP servers.\nThis allows you to expose tools from multiple servers to LLM applications.\n\n<details>\n<summary>Example</summary>\n\n```python\nfrom mcp_agent.mcp.mcp_aggregator import MCPAggregator\n\naggregator = await MCPAggregator.create(server_names=[\"fetch\", \"filesystem\"])\n\nasync with aggregator:\n   # combined list of tools exposed by 'fetch' and 'filesystem' servers\n   tools = await aggregator.list_tools()\n\n   # namespacing -- invokes the 'fetch' server to call the 'fetch' tool\n   fetch_result = await aggregator.call_tool(name=\"fetch-fetch\", arguments={\"url\": \"https://www.anthropic.com/research/building-effective-agents\"})\n\n   # no namespacing -- first server in the aggregator exposing that tool wins\n   read_file_result = await aggregator.call_tool(name=\"read_file\", arguments={})\n```\n\n</details>\n\n## Contributing\n\nWe welcome any and all kinds of contributions. Please see the [CONTRIBUTING guidelines](./CONTRIBUTING.md) to get started.\n\n### Special Mentions\n\nThere have already been incredible community contributors who are driving this project forward:\n\n- [Shaun Smith (@evalstate)](https://github.com/evalstate) -- who has been leading the charge on countless complex improvements, both to `mcp-agent` and generally to the MCP ecosystem.\n- [Jerron Lim (@StreetLamb)](https://github.com/StreetLamb) -- who has contributed countless hours and excellent examples, and great ideas to the project.\n- [Jason Summer (@jasonsum)](https://github.com/jasonsum) -- for identifying several issues and adapting his Gmail MCP server to work with mcp-agent\n\n## Roadmap\n\nWe will be adding a detailed roadmap (ideally driven by your feedback). The current set of priorities include:\n\n- **Durable Execution** -- allow workflows to pause/resume and serialize state so they can be replayed or be paused indefinitely. We are working on integrating [Temporal](./src/mcp_agent/executor/temporal.py) for this purpose.\n- **Memory** -- adding support for long-term memory\n- **Streaming** -- Support streaming listeners for iterative progress\n- **Additional MCP capabilities** -- Expand beyond tool calls to support:\n  - Resources\n  - Prompts\n  - Notifications\n\n## FAQs\n\n### What are the core benefits of using mcp-agent?\n\nmcp-agent provides a streamlined approach to building AI agents using capabilities exposed by **MCP** (Model Context Protocol) servers.\n\nMCP is quite low-level, and this framework handles the mechanics of connecting to servers, working with LLMs, handling external signals (like human input) and supporting persistent state via durable execution. That lets you, the developer, focus on the core business logic of your AI application.\n\nCore benefits:\n\n- \ud83e\udd1d **Interoperability**: ensures that any tool exposed by any number of MCP servers can seamlessly plug in to your agents.\n- \u26d3\ufe0f **Composability & Cutstomizability**: Implements well-defined workflows, but in a composable way that enables compound workflows, and allows full customization across model provider, logging, orchestrator, etc.\n- \ud83d\udcbb **Programmatic control flow**: Keeps things simple as developers just write code instead of thinking in graphs, nodes and edges. For branching logic, you write `if` statements. For cycles, use `while` loops.\n- \ud83d\udd90\ufe0f **Human Input & Signals**: Supports pausing workflows for external signals, such as human input, which are exposed as tool calls an Agent can make.\n\n### Do you need an MCP client to use mcp-agent?\n\nNo, you can use mcp-agent anywhere, since it handles MCPClient creation for you. This allows you to leverage MCP servers outside of MCP hosts like Claude Desktop.\n\nHere's all the ways you can set up your mcp-agent application:\n\n#### MCP-Agent Server\n\nYou can expose mcp-agent applications as MCP servers themselves (see [example](./examples/basic/mcp_agent_server)), allowing MCP clients to interface with sophisticated AI workflows using the standard tools API of MCP servers. This is effectively a server-of-servers.\n\n#### MCP Client or Host\n\nYou can embed mcp-agent in an MCP client directly to manage the orchestration across multiple MCP servers.\n\n#### Standalone\n\nYou can use mcp-agent applications in a standalone fashion (i.e. they aren't part of an MCP client). The [`examples`](/examples/) are all standalone applications.\n\n### Tell me a fun fact\n\nI debated naming this project _silsila_ (\u0633\u0644\u0633\u0644\u06c1), which means chain of events in Urdu. mcp-agent is more matter-of-fact, but there's still an easter egg in the project paying homage to silsila.\n"
  }
]