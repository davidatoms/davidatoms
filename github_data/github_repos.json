[
  {
    "name": "hacks-leaks-and-revelations",
    "url": "https://github.com/davidatoms/hacks-leaks-and-revelations",
    "description": "Code that goes along with the Hacks, Leaks, and Revelations book",
    "type": "fork",
    "updated_at": "2025-05-26T09:10:27Z",
    "readme": "# Hacks, Leaks, and Revelations\n\nThis git repository contains the code that goes along with the book [Hacks, Leaks, and Revelations](https://hacksandleaks.com/).\n\n_If you find any errors in this repo, please let me know. You can contact me at micah@micahflee.com._\n\n## Part 1: Sources and Datasets\n\n### Chapter 1: Protecting Sources and Yourself\n\n- [Exercise 1-1: Encrypt Your Internal Disk](./chapter-1/exercise-1-1.md)\n- [Exercise 1-2: Encrypt a USB Disk](./chapter-1/exercise-1-2.md)\n- [Exercise 1-3: Install and Use Dangerzone](./chapter-1/exercise-1-3.md)\n\n### Chapter 2: Acquiring Datasets\n\n- [Exercise 2-1: Download BlueLeaks](./chapter-2/exercise-2-1.md)\n- [Exercise 2-2: Installing and Using Signal](./chapter-2/exercise-2-2.md)\n- [Exercise 2-3: Play with Tor and OnionShare](./chapter-2/exercise-2-3.md)\n\n## Part 2: Tools of the Trade\n\n### Chapter 3: The Command Line Interface\n\n- [Exercise 3-1 for Windows Users: Install Ubuntu in Windows](./chapter-3/exercise-3-1.md)\n- [Exercise 3-2 for Mac Users: Manage Packages with Homebrew](./chapter-3/exercise-3-2.md)\n- [Exercise 3-3 for Windows and Linux Users: Manage Packages with Apt](./chapter-3/exercise-3-3.md)\n- [Exercise 3-4: Practice Using the CLI with cURL](./chapter-3/exercise-3-4.md)\n- [Exercise 3-5: Install a Text Editor](./chapter-3/exercise-3-5.md)\n- [Exercise 3-6: Write Your First Shell Script](./chapter-3/exercise-3-6.md)\n- [Exercise 3-7: Clone the Book's Git Repository](./chapter-3/exercise-3-7.md)\n\n### Chapter 4: Exploring Datasets in the Terminal\n\n- [Exercise 4-1: Accessing the BlueLeaks Dataset](./chapter-4/exercise-4-1.md)\n- [Exercise 4-2: Exploring BlueLeaks on the Command Line](./chapter-4/exercise-4-2.md)\n- [Exercise 4-3: Finding Revelations with Grep](./chapter-4/exercise-4-3.md)\n- [Exercise 4-4: Setting Up Your First VPS](./chapter-4/exercise-4-4.md)\n- [Exercise 4-5: Exploring the Oath Keepers Dataset Remotely](./chapter-4/exercise-4-5.md)\n\n### Chapter 5: Docker, Aleph, and Making Datasets Searchable\n\n- [Exercise 5-1 for Windows and Mac users: Install Docker Desktop](./chapter-5/exercise-5-1.md)\n- [Exercise 5-2 for Linux users: Install Docker CE](./chapter-5/exercise-5-2.md)\n- [Exercise 5-3: Run a WordPress Site with Docker Compose](./chapter-5/exercise-5-3.md)\n- [Exercise 5-4: Run Aleph Locally in Linux Containers](./chapter-5/exercise-5-4.md)\n- [Exercise 5-5: Add Part of BlueLeaks to Aleph](./chapter-5/exercise-5-5.md)\n\n### Chapter 6: Reading Other People's Emails\n\n- [Exercise 6-1: Download Email Dumps](./chapter-6/exercise-6-1.md)\n- [Exercise 6-2: Configure Thunderbird for Email Dumps](./chapter-6/exercise-6-2.md)\n- [Exercise 6-3: Import EML Files Into Thunderbird](./chapter-6/exercise-6-3.md)\n- [Exercise 6-4: Import MBOX Files Into Thunderbird](./chapter-6/exercise-6-4.md)\n- [Exercise 6-5: Import PST Files Into Thunderbird](./chapter-6/exercise-6-5.md)\n\n## Part 3: Python Programming\n\n### Chapter 7: An Introduction to Python\n\n- [Exercise 7-1: Install Python](./chapter-7/exercise-7-1.md)\n- [Exercise 7-2: Your First Python Script](./chapter-7/exercise-7-2.md)\n- [Exercise 7-3: Practice the Basics](./chapter-7/exercise-7-3.md)\n- [Exercise 7-4: Practice Loops and Control Flow](./chapter-7/exercise-7-4.md)\n- [Exercise 7-5: Practice Writing Functions](./chapter-7/exercise-7-5.py)\n\n### Chapter 8: Working With Data in Python\n\n- [Exercise 8-1: Traverse the Filesystem](./chapter-8/exercise-8-1.md)\n- [Exercise 8-2: Find the Largest Files in BlueLeaks](./chapter-8/exercise-8-2.md)\n- [Exercise 8-3: Practice Command Line Arguments with Click](./chapter-8/exercise-8-3.md)\n- [Exercise 8-4: Find the Largest Files in Any Dataset](./chapter-8/exercise-8-4.md)\n- [Exercise 8-5: Map Out the CSVs in BlueLeaks](./chapter-8/exercise-8-5.md)\n- [Exercise 8-6: Practice Reading and Writing Files](./chapter-8/exercise-8-6.md)\n\n## Part 4: Structured Data\n\n### Chapter 9: BlueLeaks and the CSV File Format\n\n- [Exercise 9-1: Make BlueLeaks CSVs More Readable](./chapter-9/exercise-9-1.md)\n- [Exercise 9-2: Make Bulk Emails Readable](./chapter-9/exercise-9-2.md)\n- [Exercise 9-3: Make a CSV of BlueLeaks Sites](./chapter-9/exercise-9-3.md)\n\n### Chapter 10: BlueLeaks Explorer\n\n- [Exercise 10-1: Install BlueLeaks Explorer](./chapter-10/exercise-10-1.md)\n- [Exercise 10-2: Finish Building the Structure for JRIC](./chapter-10/exercise-10-2.md)\n\n### Chapter 11: Parler, January 6, and the JSON File Format\n\n- [Exercise 11-1: Download the Parler Video Metadata](./chapter-11/exercise-11-1.md)\n- [Exercise 11-2: Write a Script to Filter for Videos with GPS From January 6, 2021](./chapter-11/exercise-11-2.md)\n- [Exercise 11-3: Update the Script to Filter for Insurrection Videos](./chapter-11/exercise-11-4.md)\n- [Exercise 11-4: Update the Script to Create KML Files to Visualize](./chapter-11/exercise-11-5.md)\n\n### Chapter 12: Epik Fail and SQL Databases\n\n- [Exercise 12-1: Create and Test a MySQL Server Using Docker and Adminer](./chapter-12/exercise-12-1.md)\n- [Exercise 12-2: Query Your SQL Database](./chapter-12/exercise-12-2.md)\n- [Exercise 12-3: Install and Test the Command Line MySQL Client](./chapter-12/exercise-12-3.md)\n- [Exercise 12-4: Download and Extract Part of the Epik Dataset](./chapter-12/exercise-12-4.md)\n- [Exercise 12-5: Import Epik Data Into MySQL](./chapter-12/exercise-12-5.md)\n\n## Part 5: Case Studies\n\n### Chapter 13: Pandemic Profiteers and COVID-19 Disinformation\n\n- [create-aflds-patients-csv.py](./chapter-13/create-aflds-patients-csv.py)\n- [create-ravkoo-csv.py](./chapter-13/create-ravkoo-csv.py)\n- [create-ravkoo-categories-csv.py](./chapter-13/create-ravkoo-categories-csv.py)\n- [create-cadence-partners-csv.py](./chapter-13/create-cadence-partners-csv.py)\n- [create-cities-csv.py](./chapter-13/create-cities-csv.py)\n- [create-ages-csv.py](./chapter-13/create-ages-csv.py)\n- [The Intercept: Network of Right-Wing Health Care Providers Is Making Millions Off Hydroxychloroquine and Ivermectin, Hacked Data Reveals](https://theintercept.com/2021/09/28/covid-telehealth-hydroxychloroquine-ivermectin-hacked/)\n\n### Chapter 14: Neo-Nazis and Their Chat Rooms\n\n- [Discord Analysis](./chapter-14/discord-analysis/README.md)\n- [The Intercept: How Right-Wing Extremists Stalk, Dox, and Harass Their Enemies](https://theintercept.com/2017/09/06/how-right-wing-extremists-stalk-dox-and-harass-their-enemies/)\n- [Unicorn Riot's Discord Leaks](https://discordleaks.unicornriot.ninja/)\n\n## Appendixes\n\n### Appendix A: Using the Windows Subsystem for Linux\n\n- [Official WSL Documentation](https://learn.microsoft.com/en-us/windows/wsl/)\n\n### Appendix B: Scraping the Web\n\n- [HTTPX example script](./appendix-b/httpx-example.py)\n- [Beautiful Soup example script](./appendix-b/bs4-example.py)\n- [Selenium example script](./appendix-b/selenium-example.py)\n\n# Licenses\n\nAll of the source code in this repository is licensed [GPLv3](./LICENSE).\n\nAll of the human language text in this repository is licensed [CC BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/)."
  },
  {
    "name": "voxelize",
    "url": "https://github.com/davidatoms/voxelize",
    "description": ":mushroom: Build your own voxel games with Voxelize! Multiplayer, optimized, highly customizable full stack library.",
    "type": "fork",
    "updated_at": "2025-05-26T08:50:08Z",
    "readme": "<a href=\"https://shaoruu.io\">\n  <p align=\"center\">\n    <img src=\"examples/client/src/assets/logo-circle.png\" width=\"100px\" height=\"100px\" />\n  </p>\n  <h1 align=\"center\">Voxelize</h1>\n</a>\n\n<p align=\"center\">A multiplayer, <i>super fast</i>, voxel engine in your browser!</p>\n\n<p align=\"center\">\n  <a href=\"https://discord.gg/9483RZtWVU\">\n  <img alt=\"Discord Server\" src=\"https://img.shields.io/discord/1229328337713762355?label=Discord&logo=Discord&style=for-the-badge\">\n  </a>\n  <img src=\"https://img.shields.io/npm/v/@voxelize/core?logo=npm&style=for-the-badge\">\n  <img src=\"https://img.shields.io/crates/v/voxelize?style=for-the-badge\"/>\n</p>\n\n<a href=\"https://shaoruu.io\">\n  <p align=\"center\">\n  LIVE DEMO\n  </p>\n</a>\n\n![](/assets/Screenshot%202024-02-19%20at%201.37.53\u202fAM.png)\n![](/assets/Screen%20Shot%202022-07-13%20at%201.01.08%20AM.png)\n![](/assets/minejs.png)\n![](/assets/Screen%20Shot%202022-07-19%20at%209.54.24%20PM.png)\n![](/assets/Screen%20Shot%202022-07-31%20at%2011.58.11%20PM.png)\n![](</assets/Screen%20Shot%202022-07-22%20at%208.01.48%20PM%20(2).png>)\n\n## Disclaimer\n\nThis is purely a passionate project. The v0 of this engine, [mc.js](https://github.com/shaoruu/mc.js), was <i>brutally</i> taken down by Microsoft by a DMCA strike with some false claims (claimed that I was collecting actual MC user information even though mc.js wasn't deployed anywhere), so although inspired, I have to clarify that this voxel engine is NOT affiliated with Minecraft, nor does it have any intention collecting existing Minecraft user information (or from any licensed voxel engines). This engine is simply made out of passion, and the textures and assets used in the game are all either licensed for free use or hand-drawn by me. I am a big fan of Minecraft, so Mojang/Microsoft, if you see this, let's work together instead of taking me down :) (Minecraft web demo?)\n\n[@shaoruu](https://github.com/shaoruu)\n\n## Features\n\n- Define custom blocks with custom static or dynamic mesh\n  - Great support for flexible combinational rendering logic\n- Easy-to-decouple server structure to refine the server-side logic\n- Isolated modules that just work\n- Realtime built-in multiplayer support\n- Fast voxel chunk mesh generation on both client and server side (multithreaded)\n- Multi-stage chunk generation with chunk overflow support\n  - No need to worry if a tree overflows to neighboring chunk, that is handled automatically\n- Fully configurable chat system with commands registry\n- AABB Physics engine that works with any static or dynamic blocks\n  - Auto-stepping, raycasting, all included\n- Entity-to-entity collision detection and resolution system\n- Periodic world data persistence\n- Robust event system for custom game events\n- For-dev debug panels that look nice\n\n## Documentation\n\nCheckout the Voxelize documentations here:\n\n- [Backend](https://docs.rs/voxelize/0.8.11/voxelize/index.html)\n- [Frontend](https://docs.voxelize.io/tutorials/intro/what-is-voxelize)\n\n## Development\n\nBefore starting, make sure to install the following:\n\n- [rust](https://www.rust-lang.org/tools/install)\n- [node.js](https://nodejs.org/en/download/)\n- [cargo-watch](https://crates.io/crates/cargo-watch)\n- [protoc](https://grpc.io/docs/protoc-installation/)\n\n```bash\n# clone the repository\ngit clone https://github.com/shaoruu/voxelize.git\ncd voxelize\n\n# download dependencies\npnpm install\n\n# generate protocol buffers\npnpm run proto\n\n# fresh build\npnpm run build\n\n# in a separate terminal, start both frontend/backend demo\npnpm run demo\n```\n\nvisit http://localhost:3000\n\n## Supporting\n\nIf you like our work, please consider supporting us on Patreon, BuyMeACoffee, or PayPal. Thanks a lot!\n\n<p align=\"center\">\n  <a href=\"https://www.patreon.com/voxelize\"><img src=\"https://c5.patreon.com/external/logo/become_a_patron_button.png\" alt=\"Patreon donate button\" /> </a>\n  <a href=\"https://paypal.me/iantheboss\"><img src=\"https://werwolv.net/assets/paypal_banner.png\" alt=\"PayPal donate button\" /> </a>\n  <a href=\"https://www.buymeacoffee.com/shaoruu\"><img src=\"https://i.imgur.com/xPDiGKQ.png\" alt=\"Buy Me A Coffee\" style=\"height: 50px\"/> </a>\n</p>\n\n<p align=\"center\">\n  <img src=\"https://api.star-history.com/svg?repos=voxelize/voxelize&type=Date\" />\n</p>\n\n## Assets Used\n\n- [Connection Serif Font (SIL Open Font)](https://fonts2u.com/connection-serif.font)\n"
  },
  {
    "name": "lunar-lander-ai",
    "url": "https://github.com/davidatoms/lunar-lander-ai",
    "description": ":rocket: Lunar Lander with Genetic Algorithm",
    "type": "fork",
    "updated_at": "2025-05-26T08:49:53Z",
    "readme": "# :rocket: Lunar Lander with Genetic Algorithm\n\nThis project was highly inspired by [Machine Learning Flappy Bird](https://github.com/ssusnic/Machine-Learning-Flappy-Bird).\n\n<a href=\"https://ian13456.github.io/lunar-lander-ai/\">\n<img src=\"https://i.imgur.com/0LRpwjS.png\" style=\"padding-bottom: 20px\"/>\n</a>\n\nLunar Lander is a single-player arcade game in the Lunar Lander subgenre. In the game, the player controls a lunar landing module as viewed from the side and attempts to land safely on the Moon.\n\nFor this project, I thought it would be a nice idea to combine this classic game with modern technology, and train these rockets to land by themselves.\n\n## How?\n\nInspired by [Machine Learning Flappy Bird](https://github.com/ssusnic/Machine-Learning-Flappy-Bird), I decided to combine the concept of Neuron Networks with Genetic Algorithm.\n\n### Neural Network\n\nEach rocket is assigned with a neural network, with an input layer of 9 neurons, a hidden layer of 16 neurons, and an output layer of 4 neurons:\n![](https://i.imgur.com/Gpcfwf2.png)\n\n#### Inputs of the NN\n\n9 input neurons:\n\n- 8 inputs represent the surroundings of the rocket, telling the rocket the distances to the obstacles around it\n- 1 input is the closest \"landable\" platform to the rocket\n\n![](https://i.imgur.com/EThdEsG.png)\n\n#### Outputs of the NN\n\nThe outputs of the NN comes in an array of 4 numbers between 0-1. Four of them each represents `Thrust`, `Rotate Left`, `Rotate Right`, and `Do Nothing`. If the value is over 0.5, the action is performed by the rocket.\n\n### Fitness\n\nEach rocket has a `fitness` function that determines how successful a rocket is upon these criteria:\n\n- Angle difference of landing/crashing platform\n- Fuel used in rocket's lifetime\n- Landing/crashing speed\n- Distance of rocket to the closest landable platform\n\nThis is essentially the score of the rocket, used as a measure of how successful a certain rocket is at landing.\n\n### Genetic Algorithm\n\nI mass spawn 30 rockets each iteration. In a certain time period, the rockets either crash, land or stay in the air.\n\nAfter each iteration, I calculate the fitness of each and every rocket, and rank them from high to low. I keep the top rockets, and [reproduce](https://natureofcode.com/book/chapter-9-the-evolution-of-code/#96-the-genetic-algorithm-part-iii-reproduction) new rockets by crossing over and mutating their weights and biases out of the top units.\n\n## Results\n\nAfter a few iterations, rockets start to land by themselves, and the average fitness slowly reaches a maximum value.\n\n<p align=\"center\">\n<img src=\"https://i.imgur.com/fZeiCZW.gif\" width=\"1000\"/>\n</p>\n\n## Resources\n\n- [Machine Learning Flappy Bird](https://github.com/ssusnic/Machine-Learning-Flappy-Bird)\n- [Coding Train's Evolution of Code](https://natureofcode.com/book/chapter-9-the-evolution-of-code/)\n- [My friend Baltazar's project](https://github.com/balta-z-r/lunar-lander)\n- [Artistic inspiration from online lunar lander](http://moonlander.seb.ly/)\n- [Synaptic NN Library](https://github.com/cazala/synaptic)\n"
  },
  {
    "name": "typehere.app",
    "url": "https://github.com/davidatoms/typehere.app",
    "description": "\u270e A quick, simple, and powerful textarea",
    "type": "fork",
    "updated_at": "2025-05-26T08:49:15Z",
    "readme": "# Type Here: A Powerful Textarea\n\nA textarea (with vim)\n\n<img width=\"2560\" alt=\"image\" src=\"https://github.com/shaoruu/typehere.app/assets/35216312/4fdcbb50-6d84-48f4-88d8-07e5f3547a92\">\n\n## Inspirations\n\nI used to use typehere.co a lot until the site was taken down, so I created [typehere.app](https://typehere.app). For a long while, it was also just a textarea that saved its contents to `localStorage`, but recently I've decided to add more features that I would find useful myself.\n\n## How To Use\n\n- Most things are in the ctrl/cmd-K menu. I will call it cmd-K for this guide.\n- Cmd-K uses fuzzy search. There are two types of things that cmd-K has: notes and commands.\n  - Notes are the notes you've created, commands are things like theme toggle, vim toggle, show/hide scrollbar, import/export, etc.\n- Navigate in the Cmd-K menu by up/down arrows.\n- Create a new note by typing the note title and run the create note command.\n  - Or you could do cmd+shift+enter. \n- Enter a note or run a command by pressing \"Enter\" or clicking on it.\n- Workspaces is the way to separate notes into different \"groups\" under cmd-K.\n  - You can create a workspace by doing cmd-K, type in a workspace name, and \"Create workspace\". This creates a workspace with an empty note.\n  - You can switch between workspaces in cmd-K by doing left/right arrow keys.\n  - You can also switch between workspace by just typing in the workspace name you want to go to and run the command.\n  - You can select a note you want (arrow up/down), and cmd + left/right arrow to move it between workspace.\n  - If there are no notes in a workspace, the workspace is automatically deleted. Under the hood, each note has a workspace string, and all workspace is just a set of all the notes workspaces.\n- For vim/keyboard-only users (like me)\n  - Toggle vim by doing cmd-K, toggle vim.\n  - Arrow up/down keys work the same as cmd+J/K inside the cmd-K menu.\n  - Cmd+B to open a note. This means to switch between the top two notes, hold cmd, and press K-J-B.\n  - Cmd+U/I to switch between workspaces.\n  - Cmd+E to toggle narrow screen view.\n  - Cmd+G to pin a note to all workspaces. this means the note will be displayed no matter which workspace you're in.\n  - Cmd+H to hide a note. In order to access hidden notes, you need to type the first 5 characters of the title right in the cmd-K menu.\n\n## Other Features\n\n- Offline mode \n- Everything client-side, all in `localStorage`\n- Periodically backed up to `indexedDB` (also in your browser)\n- Import/export notes\n- Desktop app (build on own machine)\n"
  },
  {
    "name": "LegalStories",
    "url": "https://github.com/davidatoms/LegalStories",
    "description": "[ACL 2024] Official Repository for \"Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling\"",
    "type": "fork",
    "updated_at": "2025-05-26T05:37:48Z",
    "readme": "# LegalStories\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![arXiv](https://img.shields.io/badge/arXiv-2201.07281-b31b1b.svg)](https://arxiv.org/abs/2402.17019)\n\nOfficial Code for our ACL 2024 Paper \"Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling\"\n\n## Crawl Definitions from Wikipedia\n\n1. Copy-paste the doctrine list from the [wikipedia page](https://en.wikipedia.org/wiki/Category:Legal_doctrines_and_principles). Saved as `cralwer/complex-law-doctrine-list.csv`.\n2. Crawl and preprocess these doctrines with their definitions from Wikipedia.\n    - Downloaded 294 valid doctrine pages from wikipedia, saved as `data/294-doctrines/legal_doctrines_294.csv`.\n    - Sampled 101 doctrines out of 294 whose definition length is between 100 and 200 words, saved as `data/101-doctrines/legal_doctrines_101.csv`.\n    - Sampled 20 doctrines out of 101 for detailed evaluation, saved as `data/20-doctrines/legal_doctrines_20.csv`.\n\n## Generate Stories\n\n1. Fill your OpenAI key into the `generate_story.py` and run the following commands to generate stories for GPT-4, GPT-3.5, and LLaMA-2:\n\n```\npython generate_story.py --model llama2\npython generate_story.py --model gpt-3.5-turbo-0613\npython generate_story.py --model gpt-4-0613\n```\n2. run `organize_data.ipynb` to organize the concepts, definitions, and generated stories altogether in tsv files.\n\n## Generate Questions\n\nFill your OpenAI key into the `generate_question.py` and run the following commands to generate questions for GPT-4, GPT-3.5, and LLaMA-2:\n- check out `organize_data.ipynb` to see how we prepare the datasets under the `data` folder\n\n```\npython generate_question.py --input_file ./outputs/294-doctrines-llama2/294_doctrine_stories.tsv --output_folder ./outputs/294-doctrines-llama2 --question_type concept_question --model llama2\npython generate_question.py --input_file ./outputs/294-doctrines-llama2/294_doctrine_stories.tsv --output_folder ./outputs/294-doctrines-llama2 --question_type ending_question --model llama2\npython generate_question.py --input_file ./outputs/294-doctrines-llama2/294_doctrine_stories.tsv --output_folder ./outputs/294-doctrines-llama2 --question_type limitation_question --model llama2\n\npython generate_question.py --input_file ./outputs/294-doctrines-gpt3.5/294_doctrine_stories.tsv --output_folder ./outputs/294-doctrines-gpt3.5 --question_type concept_question --model gpt-3.5-turbo-0613\npython generate_question.py --input_file ./outputs/294-doctrines-gpt3.5/294_doctrine_stories.tsv --output_folder ./outputs/294-doctrines-gpt3.5 --question_type ending_question --model gpt-3.5-turbo-0613\npython generate_question.py --input_file ./outputs/294-doctrines-gpt3.5/294_doctrine_stories.tsv --output_folder ./outputs/294-doctrines-gpt3.5 --question_type limitation_question --model gpt-3.5-turbo-0613\n\npython generate_question.py --input_file ./outputs/294-doctrines-gpt4/294_doctrine_stories.tsv --output_folder ./outputs/294-doctrines-gpt4 --question_type concept_question --model gpt-4-0613\npython generate_question.py --input_file ./outputs/294-doctrines-gpt4/294_doctrine_stories.tsv --output_folder ./outputs/294-doctrines-gpt4 --question_type ending_question --model gpt-4-0613\npython generate_question.py --input_file ./outputs/294-doctrines-gpt4/294_doctrine_stories.tsv --output_folder ./outputs/294-doctrines-gpt4 --question_type limitation_question --model gpt-4-0613\n```\n\n## Evaluation & Analysis\n\n1. Automatic evaluation with complexity metrics: `analysis/1_complexity_measure.ipynb`\n2. Human evaluation: `analysis/2_analyze_human_ratings_and_errors.ipynb`\n3. Expert annotations on 20 legal doctrines: `analysis/3_expert_annotation.ipynb`\n- the final stories and their corresponding questions: `analysis/expert_annotations/Final_regenerated_questions_20.tsv`\n- the final expert-annotated answers for the questions: `analysis/expert_annotations/Final_answer_annotations.tsv`\n4. Immediate and follow-up RCT result analyses: `analysis/4_analyze_rct_results.ipynb`\n5. Statistical analysis on the RCT results: `analysis/5_statistical_analysis.ipynb`\n\n## References\n\nIf you use this repository in your research, please kindly cite [our paper](https://aclanthology.org/2024.acl-long.388/): \n\n```bibtex\n@inproceedings{jiang-etal-2024-leveraging,\n    title = \"Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling\",\n    author = \"Jiang, Hang  and\n      Zhang, Xiajie  and\n      Mahari, Robert  and\n      Kessler, Daniel  and\n      Ma, Eric  and\n      August, Tal  and\n      Li, Irene  and\n      Pentland, Alex  and\n      Kim, Yoon  and\n      Roy, Deb  and\n      Kabbara, Jad\",\n    editor = \"Ku, Lun-Wei  and\n      Martins, Andre  and\n      Srikumar, Vivek\",\n    booktitle = \"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n    month = aug,\n    year = \"2024\",\n    address = \"Bangkok, Thailand\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.acl-long.388\",\n    pages = \"7194--7219\",\n    abstract = \"Making legal knowledge accessible to non-experts is crucial for enhancing general legal literacy and encouraging civic participation in democracy. However, legal documents are often challenging to understand for people without legal backgrounds. In this paper, we present a novel application of large language models (LLMs) in legal education to help non-experts learn intricate legal concepts through storytelling, an effective pedagogical tool in conveying complex and abstract concepts. We also introduce a new dataset LegalStories, which consists of 294 complex legal doctrines, each accompanied by a story and a set of multiple-choice questions generated by LLMs. To construct the dataset, we experiment with various LLMs to generate legal stories explaining these concepts. Furthermore, we use an expert-in-the-loop approach to iteratively design multiple-choice questions. Then, we evaluate the effectiveness of storytelling with LLMs through randomized controlled trials (RCTs) with legal novices on 10 samples from the dataset. We find that LLM-generated stories enhance comprehension of legal concepts and interest in law among non-native speakers compared to only definitions. Moreover, stories consistently help participants relate legal concepts to their lives. Finally, we find that learning with stories shows a higher retention rate for non-native speakers in the follow-up assessment. Our work has strong implications for using LLMs in promoting teaching and learning in the legal field and beyond.\",\n}\n```\n\n## Acknowledgement\nThis work is done in collaboration with researchers from MIT, Harvard Law School, University of Virginia School of Law, Allen Institute for AI (AI2), and University of Tokyo. We want to thank [MIT Center for Constructive Communication](https://www.ccc.mit.edu/) for funding the project. \n"
  },
  {
    "name": "tails",
    "url": "https://github.com/davidatoms/tails",
    "description": "The Amnesic Incognito Live System",
    "type": "fork",
    "updated_at": "2025-05-25T02:24:41Z",
    "readme": "## About Tails\n\n[**Tails**](https://tails.boum.org/) is a portable operating system that protects your privacy and helps you avoid censorship.\n\n[![Drawing of a Tails stick that is marked to be pluged into a labtop](https://tails.boum.org/index/laptop.svg)](https://tails.boum.org/)\n- Tails uses the Tor network to protect your privacy online and help you avoid censorship. Enjoy the Internet like it should be.\n- Shut down the computer and start on your Tails USB stick instead of starting on Windows, macOS, or Linux. Tails leaves no trace on the computer when shut down.\n- Tails includes a selection of applications to work on sensitive documents and communicate securely. Everything in Tails is ready-to-use and has safe defaults.\n- You can download Tails for free and independent security researchers can verify our work. Tails is based on Debian GNU/Linux.\n\n[Learn learn how Tails works](https://tails.boum.org/about)\n\n\n### How to contribute to Tails\n\nThere are many ways [you can contribute to Tails](https://tails.boum.org/contribute/). No effort is too small and whatever you bring to this community will be appreciated.\n\nFind out how you can make a difference in Tails: https://tails.boum.org/contribute/.\n\n### How to get started with GitLab\n\nhttps://tails.boum.org/contribute/working_together/GitLab/\n\n### How to transition to GitLab\n\nhttps://tails.boum.org/contribute/working_together/GitLab/transition/\n\n###  License and source code distribution\n**Tails** is [Free Software](https://www.gnu.org/philosophy/free-sw.html): you can download, use, and share it with no restrictions.\n\n <a href=\"https://tails.boum.org/doc/about/license/\"><img alt=\"Tails is Free Software\" src=\"https://tails.boum.org/index/gift.svg\" width=\"560\"/>\n\nThe Tails source code is released under the GNU/GPL (version 3 or above) and is Copyright (C) Tails developers tails@boum.org.\nAny exception to this rule is documented either [here](https://tails.boum.org/doc/about/license/) or in the affected source file.\nHowever, Tails includes non-free firmware in order to work on as much hardware as possible.\n\n\n### Contact\n\nemail and mailing lists: https://tails.boum.org/about/contact\n\nXMPP: tails@conference.riseup.net and tails-dev@conference.riseup.net\n\n[![Tails](https://tails.boum.org/contribute/how/promote/material/logo/tails-logo-flat.svg)](https://tails.boum.org)\n"
  },
  {
    "name": "evals",
    "url": "https://github.com/davidatoms/evals",
    "description": "Evals is a framework for evaluating LLMs and LLM systems, and an open-source registry of benchmarks.",
    "type": "fork",
    "updated_at": "2025-05-22T19:27:46Z",
    "readme": "# OpenAI Evals\n\n> You can now configure and run Evals directly in the OpenAI Dashboard. [Get started \u2192](https://platform.openai.com/docs/guides/evals)\n\nEvals provide a framework for evaluating large language models (LLMs) or systems built using LLMs. We offer an existing registry of evals to test different dimensions of OpenAI models and the ability to write your own custom evals for use cases you care about. You can also use your data to build private evals which represent the common LLMs patterns in your workflow without exposing any of that data publicly.\n\nIf you are building with LLMs, creating high quality evals is one of the most impactful things you can do. Without evals, it can be very difficult and time intensive to understand how different model versions might affect your use case. In the words of [OpenAI's President Greg Brockman](https://twitter.com/gdb/status/1733553161884127435):\n\n<img width=\"596\" alt=\"https://x.com/gdb/status/1733553161884127435?s=20\" src=\"https://github.com/openai/evals/assets/35577566/ce7840ff-43a8-4d88-bb2f-6b207410333b\">\n\n## Setup\n\nTo run evals, you will need to set up and specify your [OpenAI API key](https://platform.openai.com/account/api-keys). After you obtain an API key, specify it using the [`OPENAI_API_KEY` environment variable](https://platform.openai.com/docs/quickstart/step-2-setup-your-api-key). Please be aware of the [costs](https://openai.com/pricing) associated with using the API when running evals. You can also run and create evals using [Weights & Biases](https://wandb.ai/wandb_fc/openai-evals/reports/OpenAI-Evals-Demo-Using-W-B-Prompts-to-Run-Evaluations--Vmlldzo0MTI4ODA3).\n\n**Minimum Required Version: Python 3.9**\n\n### Downloading evals\n\nOur evals registry is stored using [Git-LFS](https://git-lfs.com/). Once you have downloaded and installed LFS, you can fetch the evals (from within your local copy of the evals repo) with:\n```sh\ncd evals\ngit lfs fetch --all\ngit lfs pull\n```\n\nThis will populate all the pointer files under `evals/registry/data`.\n\nYou may just want to fetch data for a select eval. You can achieve this via:\n```sh\ngit lfs fetch --include=evals/registry/data/${your eval}\ngit lfs pull\n```\n\n### Making evals\n\nIf you are going to be creating evals, we suggest cloning this repo directly from GitHub and installing the requirements using the following command:\n\n```sh\npip install -e .\n```\n\nUsing `-e`, changes you make to your eval will be reflected immediately without having to reinstall.\n\nOptionally, you can install the formatters for pre-committing with:\n\n```sh\npip install -e .[formatters]\n```\n\nThen run `pre-commit install` to install pre-commit into your git hooks. pre-commit will now run on every commit.\n\nIf you want to manually run all pre-commit hooks on a repository, run `pre-commit run --all-files`. To run individual hooks use `pre-commit run <hook_id>`.\n\n## Running evals\n\nIf you don't want to contribute new evals, but simply want to run them locally, you can install the evals package via pip:\n\n```sh\npip install evals\n```\n\nYou can find the full instructions to run existing evals in [`run-evals.md`](docs/run-evals.md) and our existing eval templates in [`eval-templates.md`](docs/eval-templates.md). For more advanced use cases like prompt chains or tool-using agents, you can use our [Completion Function Protocol](docs/completion-fns.md).\n\nWe provide the option for you to log your eval results to a Snowflake database, if you have one or wish to set one up. For this option, you will further have to specify the `SNOWFLAKE_ACCOUNT`, `SNOWFLAKE_DATABASE`, `SNOWFLAKE_USERNAME`, and `SNOWFLAKE_PASSWORD` environment variables.\n\n## Writing evals\n\nWe suggest getting starting by: \n\n- Walking through the process for building an eval: [`build-eval.md`](docs/build-eval.md)\n- Exploring an example of implementing custom eval logic: [`custom-eval.md`](docs/custom-eval.md)\n- Writing your own completion functions: [`completion-fns.md`](docs/completion-fns.md)\n- Review our starter guide for writing evals: [Getting Started with OpenAI Evals](https://cookbook.openai.com/examples/evaluation/getting_started_with_openai_evals)\n\nPlease note that we are currently not accepting evals with custom code! While we ask you to not submit such evals at the moment, you can still submit model-graded evals with custom model-graded YAML files.\n\nIf you think you have an interesting eval, please open a pull request with your contribution. OpenAI staff actively review these evals when considering improvements to upcoming models.\n\n## FAQ\n\nDo you have any examples of how to build an eval from start to finish?\n\n- Yes! These are in the `examples` folder. We recommend that you also read through [`build-eval.md`](docs/build-eval.md) in order to gain a deeper understanding of what is happening in these examples.\n\nDo you have any examples of evals implemented in multiple different ways?\n\n- Yes! In particular, see `evals/registry/evals/coqa.yaml`. We have implemented small subsets of the [CoQA](https://stanfordnlp.github.io/coqa/) dataset for various eval templates to help illustrate the differences.\n\nWhen I run an eval, it sometimes hangs at the very end (after the final report). What's going on?\n\n- This is a known issue, but you should be able to interrupt it safely and the eval should finish immediately after.\n\nThere's a lot of code, and I just want to spin up a quick eval. Help? OR,\n\nI am a world-class prompt engineer. I choose not to code. How can I contribute my wisdom?\n\n- If you follow an existing [eval template](docs/eval-templates.md) to build a basic or model-graded eval, you don't need to write any evaluation code at all! Just provide your data in JSON format and specify your eval parameters in YAML. [build-eval.md](docs/build-eval.md) walks you through these steps, and you can supplement these instructions with the Jupyter notebooks in the `examples` folder to help you get started quickly. Keep in mind, though, that a good eval will inevitably require careful thought and rigorous experimentation!\n\n## Disclaimer\n\nBy contributing to evals, you are agreeing to make your evaluation logic and data under the same MIT license as this repository. You must have adequate rights to upload any data used in an eval. OpenAI reserves the right to use this data in future service improvements to our product. Contributions to OpenAI evals will be subject to our usual Usage Policies: https://platform.openai.com/docs/usage-policies.\n"
  },
  {
    "name": "open-canvas",
    "url": "https://github.com/davidatoms/open-canvas",
    "description": "\ud83d\udcc3 A better UX for chat, writing content, and coding with LLMs.",
    "type": "fork",
    "updated_at": "2025-05-22T19:26:31Z",
    "readme": "# Open Canvas\n\n[TRY IT OUT HERE](https://opencanvas.langchain.com/)\n\n![Screenshot of app](./static/screenshot.png)\n\nOpen Canvas is an open source web application for collaborating with agents to better write documents. It is inspired by [OpenAI's \"Canvas\"](https://openai.com/index/introducing-canvas/), but with a few key differences.\n\n1. **Open Source**: All the code, from the frontend, to the content generation agent, to the reflection agent is open source and MIT licensed.\n2. **Built in memory**: Open Canvas ships out of the box with a [reflection agent](https://langchain-ai.github.io/langgraphjs/tutorials/reflection/reflection/) which stores style rules and user insights in a [shared memory store](https://langchain-ai.github.io/langgraphjs/concepts/memory/). This allows Open Canvas to remember facts about you across sessions.\n3. **Start from existing documents**: Open Canvas allows users to start with a blank text, or code editor in the language of their choice, allowing you to start the session with your existing content, instead of being forced to start with a chat interaction. We believe this is an ideal UX because many times you will already have some content to start with, and want to iterate on-top of it.\n\n## Features\n\n- **Memory**: Open Canvas has a built in memory system which will automatically generate reflections and memories on you, and your chat history. These are then included in subsequent chat interactions to give a more personalized experience.\n- **Custom quick actions**: Custom quick actions allow you to define your own prompts which are tied to your user, and persist across sessions. These then can be easily invoked through a single click, and apply to the artifact you're currently viewing.\n- **Pre-built quick actions**: There are also a series of pre-built quick actions for common writing and coding tasks that are always available.\n- **Artifact versioning**: All artifacts have a \"version\" tied to them, allowing you to travel back in time and see previous versions of your artifact.\n- **Code, Markdown, or both**: The artifact view allows for viewing and editing both code, and markdown. You can even have chats which generate code, and markdown artifacts, and switch between them.\n- **Live markdown rendering & editing**: Open Canvas's markdown editor allows you to view the rendered markdown while you're editing, without having to toggle back and fourth.\n\n## Setup locally\n\nThis guide will cover how to setup and run Open Canvas locally. If you prefer a YouTube video guide, check out [this video](https://youtu.be/sBzcQYPMekc).\n\n### Prerequisites\n\nOpen Canvas requires the following API keys and external services:\n\n#### Package Manager\n\n- [Yarn](https://yarnpkg.com/)\n\n#### APIs\n\n- [OpenAI API key](https://platform.openai.com/signup/)\n- [Anthropic API key](https://console.anthropic.com/)\n- (optional) [Google GenAI API key](https://aistudio.google.com/apikey)\n- (optional) [Fireworks AI API key](https://fireworks.ai/login)\n- (optional) [Groq AI API key](https://groq.com) - audio/video transcription\n- (optional) [FireCrawl API key](https://firecrawl.dev) - web scraping\n- (optional) [ExaSearch API key](https://exa.ai) - web search\n\n\n#### Authentication\n\n- [Supabase](https://supabase.com/) account for authentication\n\n#### LangGraph Server\n\n- [LangGraph CLI](https://langchain-ai.github.io/langgraph/cloud/reference/cli/) for running the graph locally\n\n#### LangSmith\n\n- [LangSmith](https://smith.langchain.com/) for tracing & observability\n\n### Installation\n\nFirst, clone the repository:\n\n```bash\ngit clone https://github.com/langchain-ai/open-canvas.git\ncd open-canvas\n```\n\nNext, install the dependencies:\n\n```bash\nyarn install\n```\n\nAfter installing dependencies, copy the contents of both `.env.example` files in the root of the project, and in `apps/web` into `.env` and set the required values:\n\n```bash\n# The root `.env` file will be read by the LangGraph server for the agents.\ncp .env.example .env\n```\n\n```bash\n# The `apps/web/.env` file will be read by the frontend.\ncd apps/web/\ncp .env.example .env\n```\n\nThen, setup authentication with Supabase.\n\n### Setup Authentication\n\nAfter creating a Supabase account, visit your [dashboard](https://supabase.com/dashboard/projects) and create a new project.\n\nNext, navigate to the `Project Settings` page inside your project, and then to the `API` tag. Copy the `Project URL`, and `anon public` project API key. Paste them into the `NEXT_PUBLIC_SUPABASE_URL` and `NEXT_PUBLIC_SUPABASE_ANON_KEY` environment variables in the `apps/web/.env` file.\n\nAfter this, navigate to the `Authentication` page, and the `Providers` tab. Make sure `Email` is enabled (also ensure you've enabled `Confirm Email`). You may also enable `GitHub`, and/or `Google` if you'd like to use those for authentication. (see these pages for documentation on how to setup each provider: [GitHub](https://supabase.com/docs/guides/auth/social-login/auth-github), [Google](https://supabase.com/docs/guides/auth/social-login/auth-google))\n\n#### Test authentication\n\nTo verify authentication works, run `yarn dev` and visit [localhost:3000](http://localhost:3000). This should redirect you to the [login page](http://localhost:3000/auth/login). From here, you can either login with Google or GitHub, or if you did not configure these providers, navigate to the [signup page](http://localhost:3000/auth/signup) and create a new account with an email and password. This should then redirect you to a conformation page, and after confirming your email you should be redirected to the [home page](http://localhost:3000).\n\n### Setup LangGraph Server\n\nThe first step to running Open Canvas locally is to build the application. This is because Open Canvas uses a monorepo setup, and requires workspace dependencies to be build so other packages/apps can access them.\n\nRun the following command from the root of the repository:\n\n```bash\nyarn build\n```\n\nNow we'll cover how to setup and run the LangGraph server locally.\n\nNavigate to `apps/agents` and run `yarn dev` (this runs `npx @langchain/langgraph-cli dev --port 54367`).\n\n```\nReady!\n- \ud83d\ude80 API: http://localhost:54367\n- \ud83c\udfa8 Studio UI: https://smith.langchain.com/studio?baseUrl=http://localhost:54367\n```\n\nAfter your LangGraph server is running, execute the following command inside `apps/web` to start the Open Canvas frontend:\n\n```bash\nyarn dev\n```\n\nOn initial load, compilation may take a little bit of time.\n\nThen, open [localhost:3000](http://localhost:3000) with your browser and start interacting!\n\n## LLM Models\n\nOpen Canvas is designed to be compatible with any LLM model. The current deployment has the following models configured:\n\n- **Anthropic Claude 3 Haiku \ud83d\udc64**: Haiku is Anthropic's fastest model, great for quick tasks like making edits to your document. Sign up for an Anthropic account [here](https://console.anthropic.com/).\n- **Fireworks Llama 3 70B \ud83e\udd99**: Llama 3 is a SOTA open source model from Meta, powered by [Fireworks AI](https://fireworks.ai/). You can sign up for an account [here](https://fireworks.ai/login).\n- **OpenAI GPT 4o Mini \ud83d\udca8**: GPT 4o Mini is OpenAI's newest, smallest model. You can sign up for an API key [here](https://platform.openai.com/signup/).\n\nIf you'd like to add a new model, follow these simple steps:\n\n1. Add to or update the model provider variables in `packages/shared/src/models.ts`.\n2. Install the necessary package for the provider (e.g. `@langchain/anthropic`) inside `apps/agents`.\n3. Update the `getModelConfig` function in `apps/agents/src/agent/utils.ts` to include an `if` statement for your new model name and provider.\n4. Manually test by checking you can:\n   > - 4a. Generate a new artifact\n   > - 4b. Generate a followup message (happens automatically after generating an artifact)\n   > - 4c. Update an artifact via a message in chat\n   > - 4d. Update an artifact via a quick action\n   > - 4e. Repeat for text/code (ensure both work)\n\n### Local Ollama models\n\nOpen Canvas supports calling local LLMs running on Ollama. This is not enabled in the hosted version of Open Canvas, but you can use this in your own local/deployed Open Canvas instance.\n\nTo use a local Ollama model, first ensure you have [Ollama](https://ollama.com) installed, and a model that supports tool calling pulled (the default model is `llama3.3`).\n\nNext, start the Ollama server by running `ollama run llama3.3`.\n\nThen, set the `NEXT_PUBLIC_OLLAMA_ENABLED` environment variable to `true`, and the `OLLAMA_API_URL` environment variable to the URL of your Ollama server (defaults to `http://host.docker.internal:11434`. If you do not set a custom port when starting your Ollama server, you should not need to set this environment variable).\n\n> [!NOTE]\n> Open source LLMs are typically not as good at instruction following as proprietary models like GPT-4o or Claude Sonnet. Because of this, you may experience errors or unexpected behavior when using local LLMs.\n\n## Troubleshooting\n\nBelow are some common issues you may run into if running Open Canvas yourself:\n\n- **I have the LangGraph server running successfully, and my client can make requests, but no text is being generated:** This can happen if you start & connect to multiple different LangGraph servers locally in the same browser. Try clearing the `oc_thread_id_v2` cookie and refreshing the page. This is because each unique LangGraph server has its own database where threads are stored, so a thread ID from one server will not be found in the database of another server.\n\n- **I'm getting 500 network errors when I try to make requests on the client:** Ensure you have the LangGraph server running, and you're making requests to the correct port. You can specify the port to use by passing the `--port <PORT>` flag to the `npx @langchain/langgraph-cli dev` command, and you can set the URL to make requests to by either setting the `LANGGRAPH_API_URL` environment variable, or by changing the fallback value of the `LANGGRAPH_API_URL` variable in `constants.ts`.\n\n- **I'm getting \"thread ID not found\" error toasts when I try to make requests on the client:** Ensure you have the LangGraph server running, and you're making requests to the correct port. You can specify the port to use by passing the `--port <PORT>` flag to the `npx @langchain/langgraph-cli dev` command, and you can set the URL to make requests to by either setting the `LANGGRAPH_API_URL` environment variable, or by changing the fallback value of the `LANGGRAPH_API_URL` variable in `constants.ts`.\n\n- **`Model name is missing in config.` error is being thrown when I make requests:** This error occurs when the `customModelName` is not specified in the config. You can resolve this by setting the `customModelName` field inside `config.configurable` to the name of the model you want to use when invoking the graph. See [this doc](https://langchain-ai.github.io/langgraphjs/how-tos/configuration/) on how to use configurable fields in LangGraph.\n\n## Roadmap\n\n### Features\n\nBelow is a list of features we'd like to add to Open Canvas in the near future:\n\n- **Render React in the editor**: Ideally, if you have Open Canvas generate React (or HTML) code, we should be able to render it live in the editor. **Edit**: This is in the planning stage now!\n- **Multiple assistants**: Users should be able to create multiple assistants, each having their own memory store.\n- **Give assistants custom 'tools'**: Once we've implemented `RemoteGraph` in LangGraph.js, users should be able to give assistants access to call their own graphs as tools. This means you could customize your assistant to have access to current events, your own personal knowledge graph, etc.\n\nDo you have a feature request? Please [open an issue](https://github.com/langchain-ai/open-canvas/issues/new)!\n\n### Contributing\n\nWe'd like to continue developing and improving Open Canvas, and want your help!\n\nTo start, there are a handful of GitHub issues with feature requests outlining improvements and additions to make the app's UX even better.\nThere are three main labels:\n\n- `frontend`: This label is added to issues which are UI focused, and do not require much if any work on the agent(s).\n- `ai`: This label is added to issues which are focused on improving the LLM agent(s).\n- `fullstack`: This label is added to issues which require touching both the frontend and agent code.\n\nIf you have questions about contributing, please reach out to me via email: `brace(at)langchain(dot)dev`. For general bugs/issues with the code, please [open an issue on GitHub](https://github.com/langchain-ai/open-canvas/issues/new).\n"
  },
  {
    "name": "nemo",
    "url": "https://github.com/davidatoms/nemo",
    "description": "A scalable generative AI framework built for researchers and developers working on Large Language Models, Multimodal, and Speech AI (Automatic Speech Recognition and Text-to-Speech)",
    "type": "fork",
    "updated_at": "2025-05-22T02:05:48Z",
    "readme": "[![Project Status: Active -- The project has reached a stable, usable state and is being actively developed.](http://www.repostatus.org/badges/latest/active.svg)](http://www.repostatus.org/#active)\n[![Documentation](https://readthedocs.com/projects/nvidia-nemo/badge/?version=main)](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/)\n[![CodeQL](https://github.com/nvidia/nemo/actions/workflows/codeql.yml/badge.svg?branch=main&event=push)](https://github.com/nvidia/nemo/actions/workflows/codeql.yml)\n[![NeMo core license and license for collections in this repo](https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg)](https://github.com/NVIDIA/NeMo/blob/master/LICENSE)\n[![Release version](https://badge.fury.io/py/nemo-toolkit.svg)](https://badge.fury.io/py/nemo-toolkit)\n[![Python version](https://img.shields.io/pypi/pyversions/nemo-toolkit.svg)](https://badge.fury.io/py/nemo-toolkit)\n[![PyPi total downloads](https://static.pepy.tech/personalized-badge/nemo-toolkit?period=total&units=international_system&left_color=grey&right_color=brightgreen&left_text=downloads)](https://pepy.tech/project/nemo-toolkit)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\n# **NVIDIA NeMo Framework**\n\n## Latest News\n\n<!-- markdownlint-disable -->\n<details open>\n  <summary><b>Pretrain and finetune :hugs:Hugging Face models via AutoModel</b></summary>\n      Nemo Framework's latest feature AutoModel enables broad support for :hugs:Hugging Face models, with 25.04 focusing on\n\n  \n- <a href=https://huggingface.co/transformers/v3.5.1/model_doc/auto.html#automodelforcausallm>AutoModelForCausalLM<a> in the <a href=\"https://huggingface.co/models?pipeline_tag=text-generation&sort=trending\">Text Generation<a> category\n- <a href=https://huggingface.co/docs/transformers/main/model_doc/auto#transformers.AutoModelForImageTextToText>AutoModelForImageTextToText<a> in the <a href=\"https://huggingface.co/models?pipeline_tag=image-text-to-text&sort=trending\">Image-Text-to-Text<a> category\n\nMore Details in Blog: <a href=https://developer.nvidia.com/blog/run-hugging-face-models-instantly-with-day-0-support-from-nvidia-nemo-framework>Run Hugging Face Models Instantly with Day-0 Support from NVIDIA NeMo Framework<a>. Future releases will enable support for more model families such as Video Generation models.(2025-05-19)\n</details>\n\n<details open>\n  <summary><b>Training on Blackwell using Nemo</b></summary>\n      NeMo Framework has added Blackwell support, with <a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance_summary.html>performance benchmarks on GB200 & B200<a>. More optimizations to come in the upcoming releases.(2025-05-19)\n</details>\n\n<details open>\n  <summary><b>Training Performance on GPU Tuning Guide</b></summary>\n      NeMo Framework has published <a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance-guide.html>a comprehensive guide for performance tuning to achieve optimal throughput<a>! (2025-05-19)\n</details>\n\n<details open>\n  <summary><b>New Models Support</b></summary>\n      NeMo Framework has added support for latest community models - <a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/vlms/llama4.html>Llama 4<a>, <a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/vision/diffusionmodels/flux.html>Flux<a>, <a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/llama_nemotron.html>Llama Nemotron<a>, <a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/hyena.html#>Hyena & Evo2<a>, <a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/vlms/qwen2vl.html>Qwen2-VL<a>, <a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/qwen2.html>Qwen2.5<a>, Gemma3, Qwen3-30B&32B.(2025-05-19)\n</details>\n\n\n<details open>\n  <summary><b>NeMo Framework 2.0</b></summary>\n      We've released NeMo 2.0, an update on the NeMo Framework which prioritizes modularity and ease-of-use. Please refer to the <a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/index.html>NeMo Framework User Guide</a> to get started.\n</details>\n<details open>\n  <summary><b>New Cosmos World Foundation Models Support</b></summary>\n    <details> \n      <summary> <a href=\"https://developer.nvidia.com/blog/advancing-physical-ai-with-nvidia-cosmos-world-foundation-model-platform\">Advancing Physical AI with NVIDIA Cosmos World Foundation Model Platform </a> (2025-01-09) \n      </summary> \n        The end-to-end NVIDIA Cosmos platform accelerates world model development for physical AI systems. Built on CUDA, Cosmos combines state-of-the-art world foundation models, video tokenizers, and AI-accelerated data processing pipelines. Developers can accelerate world model development by fine-tuning Cosmos world foundation models or building new ones from the ground up. These models create realistic synthetic videos of environments and interactions, providing a scalable foundation for training complex systems, from simulating humanoid robots performing advanced actions to developing end-to-end autonomous driving models. \n        <br><br>\n    </details>\n    <details>\n      <summary>\n        <a href=\"https://developer.nvidia.com/blog/accelerate-custom-video-foundation-model-pipelines-with-new-nvidia-nemo-framework-capabilities/\">\n          Accelerate Custom Video Foundation Model Pipelines with New NVIDIA NeMo Framework Capabilities\n        </a> (2025-01-07)\n      </summary>\n        The NeMo Framework now supports training and customizing the <a href=\"https://github.com/NVIDIA/Cosmos\">NVIDIA Cosmos</a> collection of world foundation models. Cosmos leverages advanced text-to-world generation techniques to create fluid, coherent video content from natural language prompts.\n        <br><br>\n        You can also now accelerate your video processing step using the <a href=\"https://developer.nvidia.com/nemo-curator-video-processing-early-access\">NeMo Curator</a> library, which provides optimized video processing and captioning features that can deliver up to 89x faster video processing when compared to an unoptimized CPU pipeline.\n      <br><br>\n    </details>\n</details>\n<details open>\n  <summary><b>Large Language Models and Multimodal Models</b></summary>\n    <details>\n      <summary>\n        <a href=\"https://developer.nvidia.com/blog/state-of-the-art-multimodal-generative-ai-model-development-with-nvidia-nemo/\">\n          State-of-the-Art Multimodal Generative AI Model Development with NVIDIA NeMo\n        </a> (2024-11-06)\n      </summary>\n        NVIDIA recently announced significant enhancements to the NeMo platform, focusing on multimodal generative AI models. The update includes NeMo Curator and the Cosmos tokenizer, which streamline the data curation process and enhance the quality of visual data. These tools are designed to handle large-scale data efficiently, making it easier to develop high-quality AI models for various applications, including robotics and autonomous driving. The Cosmos tokenizers, in particular, efficiently map visual data into compact, semantic tokens, which is crucial for training large-scale generative models. The tokenizer is available now on the <a href=http://github.com/NVIDIA/cosmos-tokenizer/NVIDIA/cosmos-tokenizer>NVIDIA/cosmos-tokenizer</a> GitHub repo and on <a href=https://huggingface.co/nvidia/Cosmos-Tokenizer-CV8x8x8>Hugging Face</a>.\n      <br><br>\n    </details>\n    <details>\n      <summary>\n        <a href=\"https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/llama/index.html#new-llama-3-1-support for more information/\">\n        New Llama 3.1 Support\n        </a> (2024-07-23)\n      </summary>\n        The NeMo Framework now supports training and customizing the Llama 3.1 collection of LLMs from Meta.\n      <br><br>\n    </details>\n    <details>\n      <summary>\n        <a href=\"https://aws.amazon.com/blogs/machine-learning/accelerate-your-generative-ai-distributed-training-workloads-with-the-nvidia-nemo-framework-on-amazon-eks/\">\n          Accelerate your Generative AI Distributed Training Workloads with the NVIDIA NeMo Framework on Amazon EKS\n        </a> (2024-07-16)\n      </summary>\n     NVIDIA NeMo Framework now runs distributed training workloads on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. For step-by-step instructions on creating an EKS cluster and running distributed training workloads with NeMo, see the GitHub repository <a href=\"https://github.com/aws-samples/awsome-distributed-training/tree/main/3.test_cases/2.nemo-launcher/EKS/\"> here.</a>\n      <br><br>\n    </details>\n    <details>\n      <summary>\n        <a href=\"https://developer.nvidia.com/blog/nvidia-nemo-accelerates-llm-innovation-with-hybrid-state-space-model-support/\">\n          NVIDIA NeMo Accelerates LLM Innovation with Hybrid State Space Model Support\n        </a> (2024/06/17)\n      </summary>\n     NVIDIA NeMo and Megatron Core now support pre-training and fine-tuning of state space models (SSMs). NeMo also supports training models based on the Griffin architecture as described by Google DeepMind. \n      <br><br>\n    </details>\n      <details>\n      <summary>\n        <a href=\"https://huggingface.co/models?sort=trending&search=nvidia%2Fnemotron-4-340B\">\n          NVIDIA releases 340B base, instruct, and reward models pretrained on a total of 9T tokens.\n        </a> (2024-06-18)\n      </summary>\n      See documentation and tutorials for SFT, PEFT, and PTQ with \n      <a href=\"https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/nemotron/index.html\">\n        Nemotron 340B \n      </a>\n      in the NeMo Framework User Guide.\n      <br><br>\n    </details>\n    <details>\n      <summary>\n        <a href=\"https://developer.nvidia.com/blog/nvidia-sets-new-generative-ai-performance-and-scale-records-in-mlperf-training-v4-0/\">\n          NVIDIA sets new generative AI performance and scale records in MLPerf Training v4.0\n        </a> (2024/06/12)\n      </summary>\n      Using NVIDIA NeMo Framework and NVIDIA Hopper GPUs NVIDIA was able to scale to 11,616 H100 GPUs and achieve near-linear performance scaling on LLM pretraining. \n      NVIDIA also achieved the highest LLM fine-tuning performance and raised the bar for text-to-image training.\n      <br><br>\n    </details>\n    <details>\n        <summary>\n          <a href=\"https://cloud.google.com/blog/products/compute/gke-and-nvidia-nemo-framework-to-train-generative-ai-models\">\n            Accelerate your generative AI journey with NVIDIA NeMo Framework on GKE\n          </a> (2024/03/16)\n        </summary>\n        An end-to-end walkthrough to train generative AI models on the Google Kubernetes Engine (GKE) using the NVIDIA NeMo Framework is available at https://github.com/GoogleCloudPlatform/nvidia-nemo-on-gke. \n        The walkthrough includes detailed instructions on how to set up a Google Cloud Project and pre-train a GPT model using the NeMo Framework.\n        <br><br>\n      </details>\n</details>\n<details open>\n  <summary><b>Speech Recognition</b></summary>\n  <details>\n      <summary>\n        <a href=\"https://developer.nvidia.com/blog/accelerating-leaderboard-topping-asr-models-10x-with-nvidia-nemo/\">\n          Accelerating Leaderboard-Topping ASR Models 10x with NVIDIA NeMo\n        </a> (2024/09/24)\n      </summary>\n      NVIDIA NeMo team released a number of inference optimizations for CTC, RNN-T, and TDT models that resulted in up to 10x inference speed-up. \n      These models now exceed an inverse real-time factor (RTFx) of 2,000, with some reaching RTFx of even 6,000.\n      <br><br>\n    </details>\n    <details>\n      <summary>\n        <a href=\"https://developer.nvidia.com/blog/new-standard-for-speech-recognition-and-translation-from-the-nvidia-nemo-canary-model/\">\n          New Standard for Speech Recognition and Translation from the NVIDIA NeMo Canary Model\n        </a> (2024/04/18)\n      </summary>\n      The NeMo team just released Canary, a multilingual model that transcribes speech in English, Spanish, German, and French with punctuation and capitalization. \n      Canary also provides bi-directional translation, between English and the three other supported languages.\n      <br><br>\n    </details>\n    <details>\n      <summary>\n        <a href=\"https://developer.nvidia.com/blog/pushing-the-boundaries-of-speech-recognition-with-nemo-parakeet-asr-models/\">\n          Pushing the Boundaries of Speech Recognition with NVIDIA NeMo Parakeet ASR Models\n        </a> (2024/04/18)\n      </summary>\n      NVIDIA NeMo, an end-to-end platform for the development of multimodal generative AI models at scale anywhere\u2014on any cloud and on-premises\u2014released the Parakeet family of automatic speech recognition (ASR) models. \n      These state-of-the-art ASR models, developed in collaboration with Suno.ai, transcribe spoken English with exceptional accuracy.\n      <br><br>\n    </details>\n  <details>\n    <summary>\n      <a href=\"https://developer.nvidia.com/blog/turbocharge-asr-accuracy-and-speed-with-nvidia-nemo-parakeet-tdt/\">\n        Turbocharge ASR Accuracy and Speed with NVIDIA NeMo Parakeet-TDT\n      </a> (2024/04/18)\n    </summary>\n    NVIDIA NeMo, an end-to-end platform for developing multimodal generative AI models at scale anywhere\u2014on any cloud and on-premises\u2014recently released Parakeet-TDT. \n    This new addition to the \u202fNeMo ASR Parakeet model family boasts better accuracy and 64% greater speed over the previously best model, Parakeet-RNNT-1.1B.\n    <br><br>\n  </details>\n</details>\n<!-- markdownlint-enable -->\n\n## Introduction\n\nNVIDIA NeMo Framework is a scalable and cloud-native generative AI\nframework built for researchers and PyTorch developers working on Large\nLanguage Models (LLMs), Multimodal Models (MMs), Automatic Speech\nRecognition (ASR), Text to Speech (TTS), and Computer Vision (CV)\ndomains. It is designed to help you efficiently create, customize, and\ndeploy new generative AI models by leveraging existing code and\npre-trained model checkpoints.\n\nFor technical documentation, please see the [NeMo Framework User\nGuide](https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/index.html).\n\n## What's New in NeMo 2.0\n\nNVIDIA NeMo 2.0 introduces several significant improvements over its predecessor, NeMo 1.0, enhancing flexibility, performance, and scalability.\n\n- **Python-Based Configuration** - NeMo 2.0 transitions from YAML files to a Python-based configuration, providing more flexibility and control. This shift makes it easier to extend and customize configurations programmatically.\n\n- **Modular Abstractions** - By adopting PyTorch Lightning\u2019s modular abstractions, NeMo 2.0 simplifies adaptation and experimentation. This modular approach allows developers to more easily modify and experiment with different components of their models.\n\n- **Scalability** - NeMo 2.0 seamlessly scaling large-scale experiments across thousands of GPUs using [NeMo-Run](https://github.com/NVIDIA/NeMo-Run), a powerful tool designed to streamline the configuration, execution, and management of machine learning experiments across computing environments.\n\nOverall, these enhancements make NeMo 2.0 a powerful, scalable, and user-friendly framework for AI model development.\n\n> [!IMPORTANT]  \n> NeMo 2.0 is currently supported by the LLM (large language model) and VLM (vision language model) collections.\n\n### Get Started with NeMo 2.0\n\n- Refer to the [Quickstart](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/quickstart.html) for examples of using NeMo-Run to launch NeMo 2.0 experiments locally and on a slurm cluster.\n- For more information about NeMo 2.0, see the [NeMo Framework User Guide](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/index.html).\n- [NeMo 2.0 Recipes](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/llm/recipes) contains additional examples of launching large-scale runs using NeMo 2.0 and NeMo-Run.\n- For an in-depth exploration of the main features of NeMo 2.0, see the [Feature Guide](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/features/index.html#feature-guide).\n- To transition from NeMo 1.0 to 2.0, see the [Migration Guide](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/migration/index.html#migration-guide) for step-by-step instructions.\n\n### Get Started with Cosmos\n\nNeMo Curator and NeMo Framework support video curation and post-training of the Cosmos World Foundation Models, which are open and available on [NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/cosmos/collections/cosmos) and [Hugging Face](https://huggingface.co/collections/nvidia/cosmos-6751e884dc10e013a0a0d8e6). For more information on video datasets, refer to [NeMo Curator](https://developer.nvidia.com/nemo-curator). To post-train World Foundation Models using the NeMo Framework for your custom physical AI tasks, see the [Cosmos Diffusion models](https://github.com/NVIDIA/Cosmos/blob/main/cosmos1/models/diffusion/nemo/post_training/README.md) and the [Cosmos Autoregressive models](https://github.com/NVIDIA/Cosmos/blob/main/cosmos1/models/autoregressive/nemo/post_training/README.md).\n\n## LLMs and MMs Training, Alignment, and Customization\n\nAll NeMo models are trained with\n[Lightning](https://github.com/Lightning-AI/lightning). Training is\nautomatically scalable to 1000s of GPUs. You can check the performance benchmarks using the\nlatest NeMo Framework container [here](https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance_summary.html).\n\nWhen applicable, NeMo models leverage cutting-edge distributed training\ntechniques, incorporating [parallelism\nstrategies](https://docs.nvidia.com/nemo-framework/user-guide/latest/modeloverview.html)\nto enable efficient training of very large models. These techniques\ninclude Tensor Parallelism (TP), Pipeline Parallelism (PP), Fully\nSharded Data Parallelism (FSDP), Mixture-of-Experts (MoE), and Mixed\nPrecision Training with BFloat16 and FP8, as well as others.\n\nNeMo Transformer-based LLMs and MMs utilize [NVIDIA Transformer\nEngine](https://github.com/NVIDIA/TransformerEngine) for FP8 training on\nNVIDIA Hopper GPUs, while leveraging [NVIDIA Megatron\nCore](https://github.com/NVIDIA/Megatron-LM/tree/main/megatron/core) for\nscaling Transformer model training.\n\nNeMo LLMs can be aligned with state-of-the-art methods such as SteerLM,\nDirect Preference Optimization (DPO), and Reinforcement Learning from\nHuman Feedback (RLHF). See [NVIDIA NeMo\nAligner](https://github.com/NVIDIA/NeMo-Aligner) for more information.\n\nIn addition to supervised fine-tuning (SFT), NeMo also supports the\nlatest parameter efficient fine-tuning (PEFT) techniques such as LoRA,\nP-Tuning, Adapters, and IA3. Refer to the [NeMo Framework User\nGuide](https://docs.nvidia.com/nemo-framework/user-guide/latest/sft_peft/index.html)\nfor the full list of supported models and techniques.\n\n## LLMs and MMs Deployment and Optimization\n\nNeMo LLMs and MMs can be deployed and optimized with [NVIDIA NeMo\nMicroservices](https://developer.nvidia.com/nemo-microservices-early-access).\n\n## Speech AI\n\nNeMo ASR and TTS models can be optimized for inference and deployed for\nproduction use cases with [NVIDIA Riva](https://developer.nvidia.com/riva).\n\n## NeMo Framework Launcher\n\n> [!IMPORTANT]  \n> NeMo Framework Launcher is compatible with NeMo version 1.0 only. [NeMo-Run](https://github.com/NVIDIA/NeMo-Run) is recommended for launching experiments using NeMo 2.0.\n\n[NeMo Framework\nLauncher](https://github.com/NVIDIA/NeMo-Megatron-Launcher) is a\ncloud-native tool that streamlines the NeMo Framework experience. It is\nused for launching end-to-end NeMo Framework training jobs on CSPs and\nSlurm clusters.\n\nThe NeMo Framework Launcher includes extensive recipes, scripts,\nutilities, and documentation for training NeMo LLMs. It also includes\nthe NeMo Framework [Autoconfigurator](https://github.com/NVIDIA/NeMo-Megatron-Launcher#53-using-autoconfigurator-to-find-the-optimal-configuration),\nwhich is designed to find the optimal model parallel configuration for\ntraining on a specific cluster.\n\nTo get started quickly with the NeMo Framework Launcher, please see the\n[NeMo Framework\nPlaybooks](https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/index.html).\nThe NeMo Framework Launcher does not currently support ASR and TTS\ntraining, but it will soon.\n\n## Get Started with NeMo Framework\n\nGetting started with NeMo Framework is easy. State-of-the-art pretrained\nNeMo models are freely available on [Hugging Face\nHub](https://huggingface.co/models?library=nemo&sort=downloads&search=nvidia)\nand [NVIDIA\nNGC](https://catalog.ngc.nvidia.com/models?query=nemo&orderBy=weightPopularDESC).\nThese models can be used to generate text or images, transcribe audio,\nand synthesize speech in just a few lines of code.\n\nWe have extensive\n[tutorials](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/starthere/tutorials.html)\nthat can be run on [Google Colab](https://colab.research.google.com) or\nwith our [NGC NeMo Framework\nContainer](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo).\nWe also have\n[playbooks](https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/index.html)\nfor users who want to train NeMo models with the NeMo Framework\nLauncher.\n\nFor advanced users who want to train NeMo models from scratch or\nfine-tune existing NeMo models, we have a full suite of [example\nscripts](https://github.com/NVIDIA/NeMo/tree/main/examples) that support\nmulti-GPU/multi-node training.\n\n## Key Features\n\n- [Large Language Models](nemo/collections/nlp/README.md)\n- [Multimodal](nemo/collections/multimodal/README.md)\n- [Automatic Speech Recognition](nemo/collections/asr/README.md)\n- [Text to Speech](nemo/collections/tts/README.md)\n- [Computer Vision](nemo/collections/vision/README.md)\n\n## Requirements\n\n- Python 3.10 or above\n- Pytorch 2.5 or above\n- NVIDIA GPU (if you intend to do model training)\n\n## Developer Documentation\n\n| Version | Status                                                                                                                                                              | Description                                                                                                                    |\n| ------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------ |\n| Latest  | [![Documentation Status](https://readthedocs.com/projects/nvidia-nemo/badge/?version=main)](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/)     | [Documentation of the latest (i.e. main) branch.](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/)          |\n| Stable  | [![Documentation Status](https://readthedocs.com/projects/nvidia-nemo/badge/?version=stable)](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/) | [Documentation of the stable (i.e. most recent release)](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/) |\n\n## Install NeMo Framework\n\nThe NeMo Framework can be installed in a variety of ways, depending on\nyour needs. Depending on the domain, you may find one of the following\ninstallation methods more suitable.\n\n- [Conda / Pip](#conda--pip): Install NeMo-Framework with native Pip into a virtual environment.\n  - Used to explore NeMo on any supported platform.\n  - This is the recommended method for ASR and TTS domains.\n  - Limited feature-completeness for other domains.\n- [NGC PyTorch container](#ngc-pytorch-container): Install NeMo-Framework from source with feature-completeness into a highly optimized container.\n  - For users that want to install from source in a highly optimized container.\n- [NGC NeMo container](#ngc-nemo-container): Ready-to-go solution of NeMo-Framework\n  - For users that seek highest performance.\n  - Contains all dependencies installed and tested for performance and convergence.\n\n### Support matrix\n\nNeMo-Framework provides tiers of support based on OS / Platform and mode of installation. Please refer the following overview of support levels:\n\n- Fully supported: Max performance and feature-completeness.\n- Limited supported: Used to explore NeMo.\n- No support yet: In development.\n- Deprecated: Support has reached end of life.\n\nPlease refer to the following table for current support levels:\n\n| OS / Platform              | Install from PyPi | Source into NGC container |\n|----------------------------|-------------------|---------------------------|\n| `linux` - `amd64/x84_64`   | Limited support   | Full support              |\n| `linux` - `arm64`          | Limited support   | Limited support           |\n| `darwin` - `amd64/x64_64`  | Deprecated        | Deprecated                |\n| `darwin` - `arm64`         | Limited support   | Limited support           |\n| `windows` - `amd64/x64_64` | No support yet    | No support yet            |\n| `windows` - `arm64`        | No support yet    | No support yet            |\n\n### Conda / Pip\n\nInstall NeMo in a fresh Conda environment:\n\n```bash\nconda create --name nemo python==3.10.12\nconda activate nemo\n```\n\n#### Pick the right version\n\nNeMo-Framework publishes pre-built wheels with each release.\nTo install nemo_toolkit from such a wheel, use the following installation method:\n\n```bash\npip install \"nemo_toolkit[all]\"\n```\n\nIf a more specific version is desired, we recommend a Pip-VCS install. From [NVIDIA/NeMo](github.com/NVIDIA/NeMo), fetch the commit, branch, or tag that you would like to install.  \nTo install nemo_toolkit from this Git reference `$REF`, use the following installation method:\n\n```bash\ngit clone https://github.com/NVIDIA/NeMo\ncd NeMo\ngit checkout @${REF:-'main'}\npip install '.[all]'\n```\n\n#### Install a specific Domain\n\nTo install a specific domain of NeMo, you must first install the\nnemo_toolkit using the instructions listed above. Then, you run the\nfollowing domain-specific commands:\n\n```bash\npip install nemo_toolkit['all'] # or pip install \"nemo_toolkit['all']@git+https://github.com/NVIDIA/NeMo@${REF:-'main'}\"\npip install nemo_toolkit['asr'] # or pip install \"nemo_toolkit['asr']@git+https://github.com/NVIDIA/NeMo@$REF:-'main'}\"\npip install nemo_toolkit['nlp'] # or pip install \"nemo_toolkit['nlp']@git+https://github.com/NVIDIA/NeMo@${REF:-'main'}\"\npip install nemo_toolkit['tts'] # or pip install \"nemo_toolkit['tts']@git+https://github.com/NVIDIA/NeMo@${REF:-'main'}\"\npip install nemo_toolkit['vision'] # or pip install \"nemo_toolkit['vision']@git+https://github.com/NVIDIA/NeMo@${REF:-'main'}\"\npip install nemo_toolkit['multimodal'] # or pip install \"nemo_toolkit['multimodal']@git+https://github.com/NVIDIA/NeMo@${REF:-'main'}\"\n```\n\n### NGC PyTorch container\n\n**NOTE: The following steps are supported beginning with 24.04 (NeMo-Toolkit 2.3.0)**\n\nWe recommended that you start with a base NVIDIA PyTorch container:\nnvcr.io/nvidia/pytorch:25.01-py3.\n\nIf starting with a base NVIDIA PyTorch container, you must first launch\nthe container:\n\n```bash\ndocker run \\\n  --gpus all \\\n  -it \\\n  --rm \\\n  --shm-size=16g \\\n  --ulimit memlock=-1 \\\n  --ulimit stack=67108864 \\\n  nvcr.io/nvidia/pytorch:${NV_PYTORCH_TAG:-'nvcr.io/nvidia/pytorch:25.01-py3'}\n```\n\nFrom [NVIDIA/NeMo](github.com/NVIDIA/NeMo), fetch the commit/branch/tag that you want to install.  \nTo install nemo_toolkit including all of its dependencies from this Git reference `$REF`, use the following installation method:\n\n```bash\ncd /opt\ngit clone https://github.com/NVIDIA/NeMo\ncd NeMo\ngit checkout ${REF:-'main'}\nbash docker/common/install_dep.sh --library all\npip install \".[all]\"\n```\n\n## NGC NeMo container\n\nNeMo containers are launched concurrently with NeMo version updates.\nNeMo Framework now supports LLMs, MMs, ASR, and TTS in a single\nconsolidated Docker container. You can find additional information about\nreleased containers on the [NeMo releases\npage](https://github.com/NVIDIA/NeMo/releases).\n\nTo use a pre-built container, run the following code:\n\n```bash\ndocker run \\\n  --gpus all \\\n  -it \\\n  --rm \\\n  --shm-size=16g \\\n  --ulimit memlock=-1 \\\n  --ulimit stack=67108864 \\\n  nvcr.io/nvidia/pytorch:${NV_PYTORCH_TAG:-'nvcr.io/nvidia/nemo:25.02'}\n```\n\n## Future Work\n\nThe NeMo Framework Launcher does not currently support ASR and TTS\ntraining, but it will soon.\n\n## Discussions Board\n\nFAQ can be found on the NeMo [Discussions\nboard](https://github.com/NVIDIA/NeMo/discussions). You are welcome to\nask questions or start discussions on the board.\n\n## Contribute to NeMo\n\nWe welcome community contributions! Please refer to\n[CONTRIBUTING.md](https://github.com/NVIDIA/NeMo/blob/stable/CONTRIBUTING.md)\nfor the process.\n\n## Publications\n\nWe provide an ever-growing list of\n[publications](https://nvidia.github.io/NeMo/publications/) that utilize\nthe NeMo Framework.\n\nTo contribute an article to the collection, please submit a pull request\nto the `gh-pages-src` branch of this repository. For detailed\ninformation, please consult the README located at the [gh-pages-src\nbranch](https://github.com/NVIDIA/NeMo/tree/gh-pages-src#readme).\n\n## Blogs\n\n<!-- markdownlint-disable -->\n<details open>\n  <summary><b>Large Language Models and Multimodal Models</b></summary>\n    <details>\n      <summary>\n        <a href=\"https://blogs.nvidia.com/blog/bria-builds-responsible-generative-ai-using-nemo-picasso/\">\n          Bria Builds Responsible Generative AI for Enterprises Using NVIDIA NeMo, Picasso\n        </a> (2024/03/06)\n      </summary>\n      Bria, a Tel Aviv startup at the forefront of visual generative AI for enterprises now leverages the NVIDIA NeMo Framework. \n      The Bria.ai platform uses reference implementations from the NeMo Multimodal collection, trained on NVIDIA Tensor Core GPUs, to enable high-throughput and low-latency image generation. \n      Bria has also adopted NVIDIA Picasso, a foundry for visual generative AI models, to run inference.\n      <br><br>\n    </details>\n    <details>\n      <summary>\n        <a href=\"https://developer.nvidia.com/blog/new-nvidia-nemo-framework-features-and-nvidia-h200-supercharge-llm-training-performance-and-versatility/\">\n          New NVIDIA NeMo Framework Features and NVIDIA H200\n        </a> (2023/12/06)\n      </summary>\n      NVIDIA NeMo Framework now includes several optimizations and enhancements, \n      including: \n      1) Fully Sharded Data Parallelism (FSDP) to improve the efficiency of training large-scale AI models, \n      2) Mix of Experts (MoE)-based LLM architectures with expert parallelism for efficient LLM training at scale, \n      3) Reinforcement Learning from Human Feedback (RLHF) with TensorRT-LLM for inference stage acceleration, and \n      4) up to 4.2x speedups for Llama 2 pre-training on NVIDIA H200 Tensor Core GPUs.\n      <br><br>\n      <a href=\"https://developer.nvidia.com/blog/new-nvidia-nemo-framework-features-and-nvidia-h200-supercharge-llm-training-performance-and-versatility\">\n      <img src=\"https://github.com/sbhavani/TransformerEngine/blob/main/docs/examples/H200-NeMo-performance.png\" alt=\"H200-NeMo-performance\" style=\"width: 600px;\"></a>\n      <br><br>\n    </details>\n    <details>\n      <summary>\n        <a href=\"https://blogs.nvidia.com/blog/nemo-amazon-titan/\">\n          NVIDIA now powers training for Amazon Titan Foundation models\n        </a> (2023/11/28)\n      </summary>\n      NVIDIA NeMo Framework now empowers the Amazon Titan foundation models (FM) with efficient training of large language models (LLMs). \n      The Titan FMs form the basis of Amazon\u2019s generative AI service, Amazon Bedrock. \n      The NeMo Framework provides a versatile framework for building, customizing, and running LLMs.\n      <br><br>\n    </details>\n</details>\n<!-- markdownlint-enable -->\n\n## Licenses\n\n- [NeMo GitHub Apache 2.0\n  license](https://github.com/NVIDIA/NeMo?tab=Apache-2.0-1-ov-file#readme)\n- NeMo is licensed under the [NVIDIA AI PRODUCT\n  AGREEMENT](https://www.nvidia.com/en-us/data-center/products/nvidia-ai-enterprise/eula/).\n  By pulling and using the container, you accept the terms and\n  conditions of this license.\n"
  },
  {
    "name": "davidatoms",
    "url": "https://github.com/davidatoms/davidatoms",
    "description": "Github Readme Profile",
    "type": "original",
    "updated_at": "2025-05-20T12:02:38Z",
    "readme": "# David Adams Automatic GitHub Readme\n\n<p align=\"left\"><b>Last Updated:</b> <!-- last_updated starts -->May 20, 2025 at 12:02 (140/365 (0.384) of the year)<!-- last_updated ends -->\n</p>\n\n<p align=\"left\">\n  <img src=\"https://img.shields.io/badge/Python-3776AB?style=flat&logo=python&logoColor=white\" />\n  <img src=\"https://img.shields.io/badge/Go-00ADD8?style=flat&logo=go&logoColor=white\" />\n  <img src=\"https://img.shields.io/badge/Rust-000000?style=flat&logo=rust&logoColor=white\" />\n  <img src=\"https://img.shields.io/badge/React-20232A?style=flat&logo=react&logoColor=61DAFB\" />\n  <img src=\"https://img.shields.io/badge/Bash-4EAA25?style=flat&logo=gnu-bash&logoColor=white\" />\n</p>\n\nI am passionate about a better future through innovation and investments. \n\n## Recent Repositories\n<!-- recent_repos starts -->\n[**NLWeb**](https://github.com/davidatoms/NLWeb) - NLWeb is an open-source project enabling websites to create natural language interfaces leveraging schema.org data formats, allowing conversational interactions for both humans and AI agents.\n\n[**llmscript**](https://github.com/davidatoms/llmscript) - llmscript allows you to write shell scripts in natural language. It leverages large language models to generate and test bash programs from your instructions.\n\n[**llm**](https://github.com/davidatoms/llm) - llm is a CLI utility and Python library for interacting with Large Language Models, both via remote APIs and models that can be installed and run locally.\n\n[**following-instructions-human-feedback**](https://github.com/davidatoms/following-instructions-human-feedback) - This repository provides resources related to InstructGPT, a language model fine-tuned with human feedback to better align with user intent across various prompts and tasks.\n\n[**awesome-langgraphjs**](https://github.com/davidatoms/awesome-langgraphjs) - This is a curated list of open source projects, applications, and YouTube videos related to LangGraph.js, a framework for building language AI applications.\n\n[**ai-chatbot**](https://github.com/davidatoms/ai-chatbot) - The ai-chatbot repository provides an open-source Next.js template for quickly building AI-powered chatbot applications using the AI SDK and various language models.\n\n[**open-agent-platform**](https://github.com/davidatoms/open-agent-platform) - Open Agent Platform is a citizen developer platform allowing non-technical users to build, connect, and use agents with various tools, RAG servers, and other agents through an Agent Supervisor.\n\n[**simple-evals**](https://github.com/davidatoms/simple-evals) - This repository provides a lightweight library for evaluating language models on various benchmarks, emphasizing zero-shot, chain-of-thought prompting to reflect realistic usage.\n\n[**build_tools**](https://github.com/davidatoms/build_tools) - build_tools provides automation scripts to build ONLYOFFICE Document Server, Document Builder, and Desktop Editors from source on Linux systems.\n\n[**davidatoms**](https://github.com/davidatoms/davidatoms) - This repository showcases David Adams' coding projects, professional profile, and documentation generated using Anthropic's Claude AI and GitHub Actions to update the README dynamically.\n<!-- recent_repos ends -->\n\n<br>\n\n![Star this repository](https://img.shields.io/badge/Star%20this%20repository-FFDD00?style=flat&logo=github&logoColor=white)\n![Profile Views](https://komarev.com/ghpvc/?username=davidatoms&style=flat&color=blue&label=Views)\n"
  }
]