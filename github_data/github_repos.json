[
  {
    "name": "mlx-lm",
    "url": "https://github.com/davidatoms/mlx-lm",
    "description": "Run LLMs with MLX",
    "type": "fork",
    "updated_at": "2025-04-29T03:03:31Z",
    "readme": "## MLX LM \n\nMLX LM is a Python package for generating text and fine-tuning large language\nmodels on Apple silicon with MLX.\n\nSome key features include:\n\n* Integration with the Hugging Face Hub to easily use thousands of LLMs with a\n  single command. \n* Support for quantizing and uploading models to the Hugging Face Hub.\n* [Low-rank and full model\n  fine-tuning](https://github.com/ml-explore/mlx-lm/blob/main/mlx_lm/LORA.md)\n  with support for quantized models.\n* Distributed inference and fine-tuning with `mx.distributed`\n\nThe easiest way to get started is to install the `mlx-lm` package:\n\n**With `pip`**:\n\n```sh\npip install mlx-lm\n```\n\n**With `conda`**:\n\n```sh\nconda install -c conda-forge mlx-lm\n```\n\n### Quick Start\n\nTo generate text with an LLM use:\n\n```bash\nmlx_lm.generate --prompt \"How tall is Mt Everest?\"\n```\n\nTo chat with an LLM use:\n\n```bash\nmlx_lm.chat\n```\n\nThis will give you a chat REPL that you can use to interact with the LLM. The\nchat context is preserved during the lifetime of the REPL.\n\nCommands in `mlx-lm` typically take command line options which let you specify\nthe model, sampling parameters, and more. Use `-h` to see a list of available\noptions for a command, e.g.:\n\n```bash\nmlx_lm.generate -h\n```\n\n### Python API\n\nYou can use `mlx-lm` as a module:\n\n```python\nfrom mlx_lm import load, generate\n\nmodel, tokenizer = load(\"mlx-community/Mistral-7B-Instruct-v0.3-4bit\")\n\nprompt = \"Write a story about Einstein\"\n\nmessages = [{\"role\": \"user\", \"content\": prompt}]\nprompt = tokenizer.apply_chat_template(\n    messages, add_generation_prompt=True\n)\n\ntext = generate(model, tokenizer, prompt=prompt, verbose=True)\n```\n\nTo see a description of all the arguments you can do:\n\n```\n>>> help(generate)\n```\n\nCheck out the [generation\nexample](https://github.com/ml-explore/mlx-lm/tree/main/mlx_lm/examples/generate_response.py)\nto see how to use the API in more detail.\n\nThe `mlx-lm` package also comes with functionality to quantize and optionally\nupload models to the Hugging Face Hub.\n\nYou can convert models using the Python API:\n\n```python\nfrom mlx_lm import convert\n\nrepo = \"mistralai/Mistral-7B-Instruct-v0.3\"\nupload_repo = \"mlx-community/My-Mistral-7B-Instruct-v0.3-4bit\"\n\nconvert(repo, quantize=True, upload_repo=upload_repo)\n```\n\nThis will generate a 4-bit quantized Mistral 7B and upload it to the repo\n`mlx-community/My-Mistral-7B-Instruct-v0.3-4bit`. It will also save the\nconverted model in the path `mlx_model` by default.\n\nTo see a description of all the arguments you can do:\n\n```\n>>> help(convert)\n```\n\n#### Streaming\n\nFor streaming generation, use the `stream_generate` function. This yields\na generation response object.\n\nFor example,\n\n```python\nfrom mlx_lm import load, stream_generate\n\nrepo = \"mlx-community/Mistral-7B-Instruct-v0.3-4bit\"\nmodel, tokenizer = load(repo)\n\nprompt = \"Write a story about Einstein\"\n\nmessages = [{\"role\": \"user\", \"content\": prompt}]\nprompt = tokenizer.apply_chat_template(\n    messages, add_generation_prompt=True\n)\n\nfor response in stream_generate(model, tokenizer, prompt, max_tokens=512):\n    print(response.text, end=\"\", flush=True)\nprint()\n```\n\n#### Sampling\n\nThe `generate` and `stream_generate` functions accept `sampler` and\n`logits_processors` keyword arguments. A sampler is any callable which accepts\na possibly batched logits array and returns an array of sampled tokens.  The\n`logits_processors` must be a list of callables which take the token history\nand current logits as input and return the processed logits. The logits\nprocessors are applied in order.\n\nSome standard sampling functions and logits processors are provided in\n`mlx_lm.sample_utils`.\n\n### Command Line\n\nYou can also use `mlx-lm` from the command line with:\n\n```\nmlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.3 --prompt \"hello\"\n```\n\nThis will download a Mistral 7B model from the Hugging Face Hub and generate\ntext using the given prompt.\n\nFor a full list of options run:\n\n```\nmlx_lm.generate --help\n```\n\nTo quantize a model from the command line run:\n\n```\nmlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.3 -q\n```\n\nFor more options run:\n\n```\nmlx_lm.convert --help\n```\n\nYou can upload new models to Hugging Face by specifying `--upload-repo` to\n`convert`. For example, to upload a quantized Mistral-7B model to the\n[MLX Hugging Face community](https://huggingface.co/mlx-community) you can do:\n\n```\nmlx_lm.convert \\\n    --hf-path mistralai/Mistral-7B-Instruct-v0.3 \\\n    -q \\\n    --upload-repo mlx-community/my-4bit-mistral\n```\n\nModels can also be converted and quantized directly in the\n[mlx-my-repo](https://huggingface.co/spaces/mlx-community/mlx-my-repo) Hugging\nFace Space.\n\n### Long Prompts and Generations \n\n`mlx-lm` has some tools to scale efficiently to long prompts and generations:\n\n- A rotating fixed-size key-value cache.\n- Prompt caching\n\nTo use the rotating key-value cache pass the argument `--max-kv-size n` where\n`n` can be any integer. Smaller values like `512` will use very little RAM but\nresult in worse quality. Larger values like `4096` or higher will use more RAM\nbut have better quality.\n\nCaching prompts can substantially speedup reusing the same long context with\ndifferent queries. To cache a prompt use `mlx_lm.cache_prompt`. For example:\n\n```bash\ncat prompt.txt | mlx_lm.cache_prompt \\\n  --model mistralai/Mistral-7B-Instruct-v0.3 \\\n  --prompt - \\\n  --prompt-cache-file mistral_prompt.safetensors\n``` \n\nThen use the cached prompt with `mlx_lm.generate`:\n\n```\nmlx_lm.generate \\\n    --prompt-cache-file mistral_prompt.safetensors \\\n    --prompt \"\\nSummarize the above text.\"\n```\n\nThe cached prompt is treated as a prefix to the supplied prompt. Also notice\nwhen using a cached prompt, the model to use is read from the cache and need\nnot be supplied explicitly.\n\nPrompt caching can also be used in the Python API in order to avoid\nrecomputing the prompt. This is useful in multi-turn dialogues or across\nrequests that use the same context. See the\n[example](https://github.com/ml-explore/mlx-lm/blob/main/mlx_lm/examples/chat.py)\nfor more usage details.\n\n### Supported Models\n\n`mlx-lm` supports thousands of Hugging Face format LLMs. If the model you want to\nrun is not supported, file an\n[issue](https://github.com/ml-explore/mlx-lm/issues/new) or better yet,\nsubmit a pull request.\n\nHere are a few examples of Hugging Face models that work with this example:\n\n- [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)\n- [meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf)\n- [deepseek-ai/deepseek-coder-6.7b-instruct](https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct)\n- [01-ai/Yi-6B-Chat](https://huggingface.co/01-ai/Yi-6B-Chat)\n- [microsoft/phi-2](https://huggingface.co/microsoft/phi-2)\n- [mistralai/Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)\n- [Qwen/Qwen-7B](https://huggingface.co/Qwen/Qwen-7B)\n- [pfnet/plamo-13b](https://huggingface.co/pfnet/plamo-13b)\n- [pfnet/plamo-13b-instruct](https://huggingface.co/pfnet/plamo-13b-instruct)\n- [stabilityai/stablelm-2-zephyr-1_6b](https://huggingface.co/stabilityai/stablelm-2-zephyr-1_6b)\n- [internlm/internlm2-7b](https://huggingface.co/internlm/internlm2-7b)\n- [tiiuae/falcon-mamba-7b-instruct](https://huggingface.co/tiiuae/falcon-mamba-7b-instruct)\n\nMost\n[Mistral](https://huggingface.co/models?library=transformers,safetensors&other=mistral&sort=trending),\n[Llama](https://huggingface.co/models?library=transformers,safetensors&other=llama&sort=trending),\n[Phi-2](https://huggingface.co/models?library=transformers,safetensors&other=phi&sort=trending),\nand\n[Mixtral](https://huggingface.co/models?library=transformers,safetensors&other=mixtral&sort=trending)\nstyle models should work out of the box.\n\nFor some models (such as `Qwen` and `plamo`) the tokenizer requires you to\nenable the `trust_remote_code` option. You can do this by passing\n`--trust-remote-code` in the command line. If you don't specify the flag\nexplicitly, you will be prompted to trust remote code in the terminal when\nrunning the model. \n\nFor `Qwen` models you must also specify the `eos_token`. You can do this by\npassing `--eos-token \"<|endoftext|>\"` in the command\nline. \n\nThese options can also be set in the Python API. For example:\n\n```python\nmodel, tokenizer = load(\n    \"qwen/Qwen-7B\",\n    tokenizer_config={\"eos_token\": \"<|endoftext|>\", \"trust_remote_code\": True},\n)\n```\n\n### Large Models\n\n> [!NOTE]\n    This requires macOS 15.0 or higher to work.\n\nModels which are large relative to the total RAM available on the machine can\nbe slow. `mlx-lm` will attempt to make them faster by wiring the memory\noccupied by the model and cache. This requires macOS 15 or higher to\nwork.\n\nIf you see the following warning message:\n\n> [WARNING] Generating with a model that requires ...\n\nthen the model will likely be slow on the given machine. If the model fits in\nRAM then it can often be sped up by increasing the system wired memory limit.\nTo increase the limit, set the following `sysctl`:\n\n```bash\nsudo sysctl iogpu.wired_limit_mb=N\n```\n\nThe value `N` should be larger than the size of the model in megabytes but\nsmaller than the memory size of the machine.\n"
  },
  {
    "name": "paper2code",
    "url": "https://github.com/davidatoms/paper2code",
    "description": "Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning",
    "type": "fork",
    "updated_at": "2025-04-27T17:13:23Z",
    "readme": "# \ud83d\udcc4 Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning\n\n![PaperCoder Overview](./assets/papercoder_overview.png)\n\n\ud83d\udcc4 [Read the paper on arXiv](https://arxiv.org/abs/2504.17192)\n\n**PaperCoder** is a multi-agent LLM system that transforms paper into code repository.\nIt follows a three-stage pipeline: planning, analysis, and code generation, each handled by specialized agents.  \nOur method outperforms strong baselines both on Paper2Code and PaperBench and produces faithful, high-quality implementations.\n\n---\n\n## \u26a1 QuickStart\n- Note: The following command runs example paper ([Attention Is All You Need](https://arxiv.org/abs/1706.03762)).  \n- \ud83d\udcb5 Estimated cost for using o3-mini: $0.50\u2013$0.70\n\n```bash\npip install openai\n\nexport OPENAI_API_KEY=\"<OPENAI_API_KEY>\"\n\ncd scripts\nbash run.sh\n```\n\n### Output Folder Structure (Only Important Files)\n```bash\noutputs\n\u251c\u2500\u2500 Transformer\n\u2502   \u251c\u2500\u2500 analyzing_artifacts\n\u2502   \u251c\u2500\u2500 coding_artifacts\n\u2502   \u2514\u2500\u2500 planning_artifacts\n\u2514\u2500\u2500 Transformer_repo  # Final output repository\n```\n---\n\n## \ud83d\udcda Detailed Setup Instructions\n\n### \ud83d\udee0\ufe0f Environment Setup\n\n- Note: If you wish to use the `o3-mini` version, please make sure to install the latest version of the OpenAI package.\n\n```bash\npip install openai\n```\n\n### \ud83d\udcc4 Convert PDF to JSON\n\n1. Clone the `s2orc-doc2json` repository to convert your PDF file into a structured JSON format.  \n   (For detailed configuration, please refer to the [official repository](https://github.com/allenai/s2orc-doc2json).)\n\n```bash\ngit clone https://github.com/allenai/s2orc-doc2json.git\n```\n\n2. Running the PDF processing service.\n\n```bash\ncd ./s2orc-doc2json/grobid-0.7.3\n./gradlew run\n```\n\n3. Convert your PDF into JSON format.\n\n```bash\nmkdir -p ./s2orc-doc2json/output_dir/paper_coder\npython ./s2orc-doc2json/doc2json/grobid2json/process_pdf.py \\\n    -i ${PDF_PATH} \\\n    -t ./s2orc-doc2json/temp_dir/ \\ \n    -o ./s2orc-doc2json/output_dir/paper_coder\n```\n\n### \ud83d\ude80 Runing PaperCoder\n- Note: The following command runs example paper ([Attention Is All You Need](https://arxiv.org/abs/1706.03762)).  \n  If you want to run PaperCoder on your own paper, please modify the environment variables accordingly.\n\n```bash\nexport OPENAI_API_KEY=\"<OPENAI_API_KEY>\"\n\ncd scripts\nbash run.sh\n```\n\n\n---\n\n## \ud83d\udcca Model-based Evaluation of Repositories Generated by PaperCoder\n\n- We evaluate repository quality using a model-based approach, supporting both reference-based and reference-free settings.  \n  The model critiques key implementation components, assigns severity levels, and generates a 1\u20135 correctness score averaged over 8 samples using **o3-mini-high**.\n\n- For more details, please refer to Section 4.3.1 (*Paper2Code Benchmark*) of the paper.\n- **Note:** The following examples evaluate the sample repository (**Transformer_repo**).  \n  Please modify the relevant paths and arguments if you wish to evaluate a different repository.\n\n### \ud83d\udee0\ufe0f Environment Setup\n```bash\npip install tiktoken\nexport OPENAI_API_KEY=\"<OPENAI_API_KEY>\"\n```\n\n\n### \ud83d\udcdd Reference-free Evaluation\n- `target_repo_dir` is the generated repository.\n\n```bash\ncd codes/\npython eval.py \\\n    --paper_name Transformer \\\n    --pdf_json_path ../examples/Transformer_cleaned.json \\\n    --data_dir ../data \\\n    --output_dir ../outputs/Transformer \\\n    --target_repo_dir ../outputs/Transformer_repo \\\n    --eval_result_dir ../results \\\n    --eval_type ref_free \\\n    --generated_n 8 \\\n    --papercoder\n```\n\n### \ud83d\udcdd Reference-based Evaluation\n- `target_repo_dir` is the generated repository.\n- `gold_repo_dir` should point to the official repository (e.g., author-released code).\n\n```bash\ncd codes/\npython eval.py \\\n    --paper_name Transformer \\\n    --pdf_json_path ../examples/Transformer_cleaned.json \\\n    --data_dir ../data \\\n    --output_dir ../outputs/Transformer \\\n    --target_repo_dir ../outputs/Transformer_repo \\\n    --gold_repo_dir ../examples/Transformer_gold_repo \\\n    --eval_result_dir ../results \\\n    --eval_type ref_based \\\n    --generated_n 8 \\\n    --papercoder\n```\n\n\n### \ud83d\udcc4 Example Output\n```bash\n========================================\n\ud83c\udf1f Evaluation Summary \ud83c\udf1f\n\ud83d\udcc4 Paper name: Transformer\n\ud83e\uddea Evaluation type: ref_based\n\ud83d\udcc1 Target repo directory: ../outputs/Transformer_repo\n\ud83d\udcca Evaluation result:\n        \ud83d\udcc8 Score: 4.5000\n        \u2705 Valid: 8/8\n========================================\n\ud83c\udf1f Usage Summary \ud83c\udf1f\n[Evaluation] Transformer - ref_based\n\ud83d\udee0\ufe0f Model: o3-mini\n\ud83d\udce5 Input tokens: 44318 (Cost: $0.04874980)\n\ud83d\udce6 Cached input tokens: 0 (Cost: $0.00000000)\n\ud83d\udce4 Output tokens: 26310 (Cost: $0.11576400)\n\ud83d\udcb5 Current total cost: $0.16451380\n\ud83e\ude99 Accumulated total cost so far: $0.16451380\n============================================\n```\n\n"
  },
  {
    "name": "system-prompts-and-models-of-ai-tools",
    "url": "https://github.com/davidatoms/system-prompts-and-models-of-ai-tools",
    "description": "FULL v0, Cursor, Manus, Same.dev, Lovable, Devin, Replit Agent, Windsurf Agent & VSCode Agent (And other Open Sourced) System Prompts, Tools & AI Models.",
    "type": "fork",
    "updated_at": "2025-04-26T19:22:04Z",
    "readme": "# **FULL v0, Cursor, Manus, Same.dev, Lovable, Devin, Replit Agent, Windsurf Agent & VSCode Agent (And other Open Sourced) System Prompts, Tools & AI Models**  \n\n(All the published system prompts are extracted by myself, except the already open sourced ones and Manus)\n\n\ud83d\ude80 **I managed to obtain FULL official v0, Manus, Cursor, Same.dev, Lovable, Devin, Replit Agent, Windsurf Agent & VSCode Agent system prompts and internal tools.**\n\n\ud83d\udcdc Over **6,500+ lines** of insights into their structure and functionality.  \n\n## \ud83d\udcc2 **Available Files**\n- **v0 Folder**  \n- **Manus Folder**\n- **Lovable Folder**\n- **Devin Folder**\n- **Same.dev Folder**\n- **Replit Folder**\n- **Windsurf Agent folder**\n- **VSCode (Copilot) Agent folder**\n- **Cursor Folder**\n- **Open Source prompts folder**\n  - Codex CLI\n  - Cline\n  - Bolt\n  - RooCode\n\n---\n\n## \ud83d\udee1\ufe0f **Security Notice for AI Startups***\n\n\u26a0\ufe0f **If you're an AI startup, make sure your data is secure.** Exposed prompts or AI models can easily become a target for hackers.\n\n\ud83d\udd10 **Interested in securing your AI systems?**  \nCheck out **[ZeroLeaks](https://0leaks.vercel.app)**, a service designed to help startups **identify and secure** leaks in system instructions, internal tools, and model configurations. **Get a free AI security audit** to ensure your AI is protected from vulnerabilities.\n\n\n**The company is mine, this is NOT a 3rd party AD.*\n---\n\n## \ud83d\udee0 **Roadmap & Feedback**\n\n\ud83d\udea8 **Note:** We no longer use GitHub issues for roadmap and feedback.  \nPlease visit [System Prompts Roadmap & Feedback](https://systemprompts.featurebase.app/) to share your suggestions and track upcoming features.\n\n\ud83c\udd95 **LATEST UPDATE:** 25/04/2025 \n\n## \ud83d\udcca **Star History**\n\n<a href=\"https://www.star-history.com/#x1xhlol/system-prompts-and-models-of-ai-tools&Date\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=x1xhlol/system-prompts-and-models-of-ai-tools&type=Date&theme=dark\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=x1xhlol/system-prompts-and-models-of-ai-tools&type=Date\" />\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=x1xhlol/system-prompts-and-models-of-ai-tools&type=Date\" />\n </picture>\n</a>\n\n## \u2764\ufe0f Support the Project\n\nIf you find this collection valuable and appreciate the effort involved in obtaining and sharing these insights, please consider supporting the project. Your contribution helps keep this resource updated and allows for further exploration.\n\nYou can show your support via:\n\n*   **PayPal:** `lucknitelol@proton.me`\n*   **Cryptocurrency:**\n    *   **BTC:** `bc1q7zldmzjwspnaa48udvelwe6k3fef7xrrhg5625`\n    *   **LTC:** `LRWgqwEYDwqau1WeiTs6Mjg85NJ7m3fsdQ`\n\nThank you for your support! \ud83d\ude4f\n\n\n## \ud83d\udd17 **Connect With Me**  \n\u2716 **X:** [NotLucknite](https://x.com/NotLucknite)  \n\ud83d\udcac **Discord:** `x1xh`  \n\n\u2b50 **Drop a star if you find this useful!**\n"
  },
  {
    "name": "pandas-ai",
    "url": "https://github.com/davidatoms/pandas-ai",
    "description": "Chat with your database or your datalake (SQL, CSV, parquet). PandasAI makes data analysis conversational using LLMs and RAG.",
    "type": "fork",
    "updated_at": "2025-04-24T00:39:38Z",
    "readme": "# ![PandaAI](assets/logo.png)\n\n[![Release](https://img.shields.io/pypi/v/pandasai?label=Release&style=flat-square)](https://pypi.org/project/pandasai/)\n[![CI](https://github.com/sinaptik-ai/pandas-ai/actions/workflows/ci-core.yml/badge.svg)](https://github.com/sinaptik-ai/pandas-ai/actions/workflows/ci-core.yml/badge.svg)\n[![CD](https://github.com/sinaptik-ai/pandas-ai/actions/workflows/cd.yml/badge.svg)](https://github.com/sinaptik-ai/pandas-ai/actions/workflows/cd.yml/badge.svg)\n[![Coverage](https://codecov.io/gh/sinaptik-ai/pandas-ai/branch/main/graph/badge.svg)](https://codecov.io/gh/sinaptik-ai/pandas-ai)\n[![Discord](https://dcbadge.vercel.app/api/server/kF7FqH2FwS?style=flat&compact=true)](https://discord.gg/KYKj9F2FRH)\n[![Downloads](https://static.pepy.tech/badge/pandasai)](https://pepy.tech/project/pandasai) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1ZnO-njhL7TBOYPZaqvMvGtsjckZKrv2E?usp=sharing)\n\nPandaAI is a Python platform that makes it easy to ask questions to your data in natural language. It helps non-technical users to interact with their data in a more natural way, and it helps technical users to save time, and effort when working with data.\n\n# \ud83d\udd27 Getting started\n\nYou can find the full documentation for PandaAI [here](https://pandas-ai.readthedocs.io/en/latest/).\n\nYou can either decide to use PandaAI in your Jupyter notebooks, Streamlit apps, or use the client and server architecture from the repo.\n\n## \u2601\ufe0f Using the platform\n\nThe library can be used alongside our powerful data platform, making end-to-end conversational data analytics possible with as little as a few lines of code.\n\nLoad your data, save them as a dataframe, and push them to the platform\n\n```python\nimport pandasai as pai\n\npai.api_key.set(\"your-pai-api-key\")\n\nfile = pai.read_csv(\"./filepath.csv\")\n\ndataset = pai.create(path=\"your-organization/dataset-name\",\n    df=file,\n    name=\"dataset-name\",\n    description=\"dataset-description\")\n\ndataset.push()\n```\n\nYour team can now access and query this data using natural language through the platform.\n\n![PandaAI](assets/demo.gif)\n\n## \ud83d\udcda Using the library\n\n### Python Requirements\n\nPython version `3.8+ <3.12`\n\n### \ud83d\udce6 Installation\n\nYou can install the PandaAI library using pip or poetry.\n\nWith pip:\n\n```bash\npip install \"pandasai>=3.0.0b2\"\n```\n\nWith poetry:\n\n```bash\npoetry add \"pandasai>=3.0.0b2\"\n```\n\n### \ud83d\udcbb Usage\n\n#### Ask questions\n\n```python\nimport pandasai as pai\n\n# Sample DataFrame\ndf = pai.DataFrame({\n    \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"],\n    \"revenue\": [5000, 3200, 2900, 4100, 2300, 2100, 2500, 2600, 4500, 7000]\n})\n\n# By default, unless you choose a different LLM, it will use BambooLLM.\n# You can get your free API key signing up at https://app.pandabi.ai (you can also configure it in your .env file)\npai.api_key.set(\"your-pai-api-key\")\n\ndf.chat('Which are the top 5 countries by sales?')\n```\n\n```\nChina, United States, Japan, Germany, Australia\n```\n\n---\n\nOr you can ask more complex questions:\n\n```python\ndf.chat(\n    \"What is the total sales for the top 3 countries by sales?\"\n)\n```\n\n```\nThe total sales for the top 3 countries by sales is 16500.\n```\n\n#### Visualize charts\n\nYou can also ask PandaAI to generate charts for you:\n\n```python\ndf.chat(\n    \"Plot the histogram of countries showing for each one the gd. Use different colors for each bar\",\n)\n```\n\n![Chart](assets/histogram-chart.png?raw=true)\n\n#### Multiple DataFrames\n\nYou can also pass in multiple dataframes to PandaAI and ask questions relating them.\n\n```python\nimport pandasai as pai\n\nemployees_data = {\n    'EmployeeID': [1, 2, 3, 4, 5],\n    'Name': ['John', 'Emma', 'Liam', 'Olivia', 'William'],\n    'Department': ['HR', 'Sales', 'IT', 'Marketing', 'Finance']\n}\n\nsalaries_data = {\n    'EmployeeID': [1, 2, 3, 4, 5],\n    'Salary': [5000, 6000, 4500, 7000, 5500]\n}\n\nemployees_df = pai.DataFrame(employees_data)\nsalaries_df = pai.DataFrame(salaries_data)\n\n# By default, unless you choose a different LLM, it will use BambooLLM.\n# You can get your free API key signing up at https://app.pandabi.ai (you can also configure it in your .env file)\npai.api_key.set(\"your-pai-api-key\")\n\npai.chat(\"Who gets paid the most?\", employees_df, salaries_df)\n```\n\n```\nOlivia gets paid the most.\n```\n\n#### Docker Sandbox\n\nYou can run PandaAI in a Docker sandbox, providing a secure, isolated environment to execute code safely and mitigate the risk of malicious attacks.\n\n##### Python Requirements\n\n```bash\npip install \"pandasai-docker\"\n```\n\n##### Usage\n\n```python\nimport pandasai as pai\nfrom pandasai_docker import DockerSandbox\n\n# Initialize the sandbox\nsandbox = DockerSandbox()\nsandbox.start()\n\nemployees_data = {\n    'EmployeeID': [1, 2, 3, 4, 5],\n    'Name': ['John', 'Emma', 'Liam', 'Olivia', 'William'],\n    'Department': ['HR', 'Sales', 'IT', 'Marketing', 'Finance']\n}\n\nsalaries_data = {\n    'EmployeeID': [1, 2, 3, 4, 5],\n    'Salary': [5000, 6000, 4500, 7000, 5500]\n}\n\nemployees_df = pai.DataFrame(employees_data)\nsalaries_df = pai.DataFrame(salaries_data)\n\n# By default, unless you choose a different LLM, it will use BambooLLM.\n# You can get your free API key signing up at https://app.pandabi.ai (you can also configure it in your .env file)\npai.api_key.set(\"your-pai-api-key\")\n\npai.chat(\"Who gets paid the most?\", employees_df, salaries_df, sandbox=sandbox)\n\n# Don't forget to stop the sandbox when done\nsandbox.stop()\n```\n\n```\nOlivia gets paid the most.\n```\n\nYou can find more examples in the [examples](examples) directory.\n\n## \ud83d\udcdc License\n\nPandaAI is available under the MIT expat license, except for the `pandasai/ee` directory of this repository, which has its [license here](https://github.com/sinaptik-ai/pandas-ai/blob/main/ee/LICENSE).\n\nIf you are interested in managed PandaAI Cloud or self-hosted Enterprise Offering, [contact us](https://getpanda.ai/pricing).\n\n## Resources\n\n> **Beta Notice**  \n> Release v3 is currently in beta. The following documentation and examples reflect the features and functionality in progress and may change before the final release.\n\n- [Docs](https://pandas-ai.readthedocs.io/en/latest/) for comprehensive documentation\n- [Examples](examples) for example notebooks\n- [Discord](https://discord.gg/KYKj9F2FRH) for discussion with the community and PandaAI team\n\n## \ud83e\udd1d Contributing\n\nContributions are welcome! Please check the outstanding issues and feel free to open a pull request.\nFor more information, please check out the [contributing guidelines](CONTRIBUTING.md).\n\n### Thank you!\n\n[![Contributors](https://contrib.rocks/image?repo=sinaptik-ai/pandas-ai)](https://github.com/sinaptik-ai/pandas-ai/graphs/contributors)\n"
  },
  {
    "name": "kalshi-starter-code-python",
    "url": "https://github.com/davidatoms/kalshi-starter-code-python",
    "description": null,
    "type": "fork",
    "updated_at": "2025-04-23T16:09:35Z",
    "readme": "# kalshi-starter-code-python\nExample python code for accessing api-authenticated endpoints on [Kalshi](https://kalshi.com). This is not an SDK. \n\n## Installation \nInstall requirements.txt in a virtual environment of your choice and execute main.py from within the repo.\n\n```\npip install -r requirements.txt\npython main.py\n```\n"
  },
  {
    "name": "davidatoms",
    "url": "https://github.com/davidatoms/davidatoms",
    "description": "Github Readme Profile",
    "type": "original",
    "updated_at": "2025-04-22T12:02:12Z",
    "readme": "# David Adams Automatic GitHub Readme\n\n<p align=\"left\"><b>Last Updated:</b> <!-- last_updated starts -->April 22, 2025 at 12:02 (112/365 (0.307) of the year)<!-- last_updated ends -->\n</p>\n\n<p align=\"left\">\n  <img src=\"https://img.shields.io/badge/Python-3776AB?style=flat&logo=python&logoColor=white\" />\n  <img src=\"https://img.shields.io/badge/Go-00ADD8?style=flat&logo=go&logoColor=white\" />\n  <img src=\"https://img.shields.io/badge/Rust-000000?style=flat&logo=rust&logoColor=white\" />\n  <img src=\"https://img.shields.io/badge/React-20232A?style=flat&logo=react&logoColor=61DAFB\" />\n  <img src=\"https://img.shields.io/badge/Bash-4EAA25?style=flat&logo=gnu-bash&logoColor=white\" />\n</p>\n\nI am passionate about a better future through innovation and investments. \n\n## Recent Repositories\n<!-- recent_repos starts -->\n[**ndx-template**](https://github.com/davidatoms/ndx-template) - This repository provides a cookiecutter template for creating Neurodata Extensions (NDX) for the Neurodata Without Borders (NWB) data standard, allowing users to easily generate a project structure for developing NWB extensions in Python.\n\n[**davidatoms**](https://github.com/davidatoms/davidatoms) - This repository showcases David Adams' coding projects, professional profile, and documentation generated using Anthropic's Claude AI and GitHub Actions to update the README dynamically.\n\n[**openbb**](https://github.com/davidatoms/openbb) - The OpenBB Platform is a free and open-source financial platform offering access to equity, options, crypto, forex, macro economy, fixed income, and more, along with a range of extensions for enhanced user experience.\n\n[**transformerlab-app**](https://github.com/davidatoms/transformerlab-app) - Transformer Lab is an open-source toolkit that allows users to train, tune, and chat with large language models on their own machine, providing a user-friendly interface and a wide range of features.\n\n[**OpenGlass**](https://github.com/davidatoms/OpenGlass) - OpenGlass turns regular glasses into smart glasses using an affordable microcontroller and open-source software, enabling features like object recognition, translation, and life recording.\n\n[**omi**](https://github.com/davidatoms/omi) - Omi is an open-source AI wearable that captures conversations, provides summaries, action items, and performs actions for you, offering automatic, high-quality transcriptions through a mobile app integration.\n\n[**vscode**](https://github.com/davidatoms/vscode) - Visual Studio Code is an open-source code editor developed by Microsoft, featuring comprehensive code editing, navigation, debugging, and an extensible architecture.\n\n[**DocumentServer**](https://github.com/davidatoms/DocumentServer) - ONLYOFFICE Docs is a free collaborative online office suite comprising viewers and editors for texts, spreadsheets and presentations, forms and PDF, fully compatible with Office Open XML formats: .docx, .xlsx, .pptx and enabling collaborative editing in real time.\n\n[**healthcare-shared-components**](https://github.com/davidatoms/healthcare-shared-components) - This repository hosts shared components used by Microsoft Health Care services, such as FHIR Server for Azure, IoMT FHIR Connector, and Medical Imaging Server for DICOM.\n\n[**mlfrm**](https://github.com/davidatoms/mlfrm) - This repository offers Python code and Jupyter Notebooks companion to the O'Reilly book \"Machine Learning for Financial Risk Management with Python.\"\n<!-- recent_repos ends -->\n\n<br>\n\n![Star this repository](https://img.shields.io/badge/Star%20this%20repository-FFDD00?style=flat&logo=github&logoColor=white)\n![Profile Views](https://komarev.com/ghpvc/?username=davidatoms&style=flat&color=blue&label=Views)\n"
  },
  {
    "name": "ndx-template",
    "url": "https://github.com/davidatoms/ndx-template",
    "description": "A template for creating Neurodata Extensions for the NWB neurophysiology data standard",
    "type": "fork",
    "updated_at": "2025-04-19T05:53:55Z",
    "readme": "## About\n\n[![Run tests](https://github.com/nwb-extensions/ndx-template/actions/workflows/run_tests.yml/badge.svg)](https://github.com/nwb-extensions/ndx-template/actions/workflows/run_tests.yml)\n\nThis repo provides a template for creating Neurodata Extensions (NDX) for the\n[Neurodata Without Borders](https://nwb.org/)\n data standard.\n\nThis template currently supports creating Neurodata Extensions only using Python 3.9+.\nMATLAB support is in development.\n\n## Getting started\n\n1. Install [Python](https://www.python.org/downloads/) for your operating system if it is not already installed.\n\n2. Install [cookiecutter](https://pypi.org/project/cookiecutter/), [pynwb](https://pypi.org/project/pynwb/), and [hdmf-docutils](https://pypi.org/project/hdmf-docutils/).\n`cookiecutter` is a Python-based command-line utility that creates projects from templates.\n   ```bash\n   python -m pip install -U cookiecutter pynwb hdmf-docutils\n   ```\n3. Run cookiecutter in the directory where you want to create a new directory with the extension:\n   ```bash\n   cookiecutter gh:nwb-extensions/ndx-template\n   ```\n\n   To overwrite the contents of an existing directory, use the `--overwrite-if-exists` flag:\n   ```bash\n   cookiecutter --overwrite-if-exists gh:nwb-extensions/ndx-template\n   ```\n   This can be useful if you want to populate an existing empty git repository with a new extension.\n   \n4. Answer the prompts, which will be used to fill in the blanks throughout the\ntemplate automatically. Guidelines:\n    - `Select a name for your extension. It must start with 'ndx-'` - The name of the namespace for your extension. This could be a\n    description of the extension (e.g., \"ndx-cortical-surface\") or the name of your\n    lab or group (e.g., \"ndx-allen-institute\"). The name should generally follow the following naming conventions:\n      - Use only lower-case ASCII letters (no special characters)\n      - Use \"-\" to separate different parts of the name (no spaces allowed)\n      - Be short and descriptive\n    - `Select an initial version string` - Version of your extension. Versioning should start at 0.1.0 and follow [semantic versioning](https://semver.org/) guidelines\n    - `Select a license` - Name of license used for your extension source code. A permissive license, such as BSD, should be used if possible.\n5. A new folder with the same name as your entered `namespace` will be\ncreated. See `NEXTSTEPS.md` in that folder for the next steps in creating\nyour awesome new Neurodata Extension.\n\nIn case cookiecutter runs into problems and you want to avoid reentering\nall the information, you can edit the file `~/.cookiecutter_replay/ndx-template.json`,\nand use that via `cookiecutter --replay gh:nwb-extensions/ndx-template`.\n\nSee the [PyNWB tutorial](https://pynwb.readthedocs.io/en/stable/tutorials/general/extensions.html) for guidance on how to write your extension.\n\nWhen you are done creating your extension, we encourage you to follow the steps\nto publish your Neurodata Extension in the [NDX Catalog](https://github.com/nwb-extensions/) for the benefit of the\ngreater neuroscience community! :)\n\n## Running tests with breakpoint debugging\n\nBy default, to aid with debugging, the project is configured NOT to run code coverage as part of the tests. Code coverage testing is useful to help with creation of tests and report test coverage. However, with this option enabled, breakpoints for debugging with pdb are being ignored. To enable this option for code coverage reporting, uncomment out the following line in your `pyproject.toml`:\n\nhttps://github.com/nwb-extensions/ndx-template/blob/11ae225b3fd3934fa3c56e6e7b563081793b3b43/%7B%7B%20cookiecutter.namespace%20%7D%7D/pyproject.toml#L82-L83\n\n## Integrating with NWB Widgets\n\nWhen answering the cookiecutter prompts, you will be asked whether you would like to create templates for integration with [NWB Widgets](https://github.com/NeurodataWithoutBorders/nwbwidgets), a library of plotting widgets for interactive visualization of NWB neurodata types within a Jupyter notebook. If you answer \"yes\", then an example widget and example notebook will be created for you. If you answer \"no\", but would like to add a widget later on, follow the instructions below:\n\n1. Create a directory named `widgets` in `src/pynwb/{your_python_package_name}/`.\n2. Copy [`__init__.py`](https://github.com/nwb-extensions/ndx-template/blob/main/%7B%7B%20cookiecutter.namespace%20%7D%7D/src/pynwb/%7B%7B%20cookiecutter.py_pkg_name%20%7D%7D/widgets/__init__.py) to that directory and adapt the contents to your extension.\n3. Copy [`tetrode_series_widget.py`](https://github.com/nwb-extensions/ndx-template/blob/main/%7B%7B%20cookiecutter.namespace%20%7D%7D/src/pynwb/%7B%7B%20cookiecutter.py_pkg_name%20%7D%7D/widgets/tetrode_series_widget.py) to that directory and adapt the contents to your extension.\n4. Create a directory named `notebooks` in the root of the repository.\n5. Copy [example.ipynb](https://github.com/nwb-extensions/ndx-template/blob/main/%7B%7B%20cookiecutter.namespace%20%7D%7D/notebooks/example.ipynb) to that directory and adapt the contents to your extension.\n6. Add `nwbwidgets` to the `[project] dependencies` or `[project.optional-dependencies]` section of `pyproject.toml`.\n\n## Maintainers\n- [@rly](https://github.com/rly)\n- [@oruebel](https://github.com/oruebel)\n- [@jcfr](https://github.com/jcfr)\n- [@bendichter](https://github.com/bendichter)\n- [@ajtritt](https://github.com/ajtritt)\n\n## Copyright\n\nNeurodata Extensions Catalog (NDX Catalog) Copyright (c) 2021-2025,\nThe Regents of the University of California, through Lawrence\nBerkeley National Laboratory (subject to receipt of any required\napprovals from the U.S. Dept. of Energy).  All rights reserved.\n\nIf you have questions about your rights to use or distribute this software,\nplease contact Berkeley Lab's Intellectual Property Office at\nIPO@lbl.gov.\n\nNOTICE.  This Software was developed under funding from the U.S. Department\nof Energy and the U.S. Government consequently retains certain rights.  As\nsuch, the U.S. Government has been granted for itself and others acting on\nits behalf a paid-up, nonexclusive, irrevocable, worldwide license in the\nSoftware to reproduce, distribute copies to the public, prepare derivative\nworks, and perform publicly and display publicly, and to permit others to do so.\n"
  },
  {
    "name": "openbb",
    "url": "https://github.com/davidatoms/openbb",
    "description": "Investment Research for Everyone, Everywhere.",
    "type": "fork",
    "updated_at": "2025-04-15T05:58:09Z",
    "readme": "<br />\n<img src=\"https://github.com/OpenBB-finance/OpenBB/blob/develop/images/platform-light.svg?raw=true#gh-light-mode-only\" alt=\"OpenBB Platform logo\" width=\"600\">\n<img src=\"https://github.com/OpenBB-finance/OpenBB/blob/develop/images/platform-dark.svg?raw=true#gh-dark-mode-only\" alt=\"OpenBB Platform logo\" width=\"600\">\n<br />\n<br />\n\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/openbb_finance.svg?style=social&label=Follow%20%40openbb_finance)](https://x.com/openbb_finance)\n[![Discord Shield](https://img.shields.io/discord/831165782750789672)](https://discord.com/invite/xPHTuHCmuV)\n[![Open in Dev Containers](https://img.shields.io/static/v1?label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/OpenBB-finance/OpenBB)\n<a href=\"https://codespaces.new/OpenBB-finance/OpenBB\">\n  <img src=\"https://github.com/codespaces/badge.svg\" height=\"20\" />\n</a>\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/OpenBB-finance/OpenBB/blob/develop/examples/googleColab.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n[![PyPI](https://img.shields.io/pypi/v/openbb?color=blue&label=PyPI%20Package)](https://pypi.org/project/openbb/)\n\nThe first financial Platform that is free and fully open source.\n\nThe OpenBB Platform offers access to equity, options, crypto, forex, macro economy, fixed income, and more while also offering a broad range of extensions to enhance the user experience according to their needs.\n\nSign up to the [OpenBB Hub](https://my.openbb.co/login) to get the most out of the OpenBB ecosystem.\n\n---\n\nIf you are looking for our **FREE** AI-powered Research and Analytics Workspace, you can find it here: [pro.openbb.co](https://pro.openbb.co).\n\n<a href=\"https://pro.openbb.co\">\n  <div align=\"center\">\n  <img src=\"https://openbb.co/api/image?src=https%3A%2F%2Fopenbb-cms.directus.app%2Fassets%2Ff431ed60-5e46-439a-a9f7-4b06e72d0720.png&width=2400&height=1552&fit=cover&position=center&background[]=0&background[]=0&background[]=0&background[]=0&quality=100&compressionLevel=9&loop=0&delay=100&crop=null\" alt=\"Logo\" width=\"600\">\n  </div>\n</a>\n\nWe also have an open source AI financial analyst agent that can access all of the data within OpenBB, and that repo can be found [here](https://github.com/OpenBB-finance/openbb-agents).\n\n---\n\n<!-- TABLE OF CONTENTS -->\n<details closed=\"closed\">\n  <summary><h2 style=\"display: inline-block\">Table of Contents</h2></summary>\n  <ol>\n    <li><a href=\"#1-installation\">Installation</a></li>\n    <li><a href=\"#2-contributing\">Contributing</a></li>\n    <li><a href=\"#3-license\">License</a></li>\n    <li><a href=\"#4-disclaimer\">Disclaimer</a></li>\n    <li><a href=\"#5-contacts\">Contacts</a></li>\n    <li><a href=\"#6-star-history\">Star History</a></li>\n    <li><a href=\"#7-contributors\">Contributors</a></li>\n  </ol>\n</details>\n\n## 1. Installation\n\nThe OpenBB Platform can be installed as a [PyPI package](https://pypi.org/project/openbb/) by running `pip install openbb`\n\nor by cloning the repository directly with `git clone https://github.com/OpenBB-finance/OpenBB.git`.\n\nPlease find more about the installation process in the [OpenBB Documentation](https://docs.openbb.co/platform/installation).\n\n### OpenBB Platform CLI installation\n\nThe OpenBB Platform CLI is a command-line interface that allows you to access the OpenBB Platform directly from your command line.\n\nIt can be installed by running `pip install openbb-cli`\n\nor by cloning the repository directly with  `git clone https://github.com/OpenBB-finance/OpenBB.git`.\n\nPlease find more about the installation process in the [OpenBB Documentation](https://docs.openbb.co/cli/installation).\n\n## 2. Contributing\n\nThere are three main ways of contributing to this project. (Hopefully you have starred the project by now \u2b50\ufe0f)\n\n### Become a Contributor\n\n* More information on our [Contributing Documentation](https://docs.openbb.co/platform/developer_guide/contributing).\n\n### Create a GitHub ticket\n\nBefore creating a ticket make sure the one you are creating doesn't exist already [here](https://github.com/OpenBB-finance/OpenBB/issues)\n\n* [Report bug](https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&labels=bug&template=bug_report.md&title=%5BBug%5D)\n* [Suggest improvement](https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&labels=enhancement&template=enhancement.md&title=%5BIMPROVE%5D)\n* [Request a feature](https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&labels=new+feature&template=feature_request.md&title=%5BFR%5D)\n\n### Provide feedback\n\nWe are most active on [our Discord](https://openbb.co/discord), but feel free to reach out to us in any of [our social media](https://openbb.co/links) for feedback.\n\n## 3. License\n\nDistributed under the AGPLv3 License. See\n[LICENSE](https://github.com/OpenBB-finance/OpenBB/blob/main/LICENSE) for more information.\n\n## 4. Disclaimer\n\nTrading in financial instruments involves high risks including the risk of losing some, or all, of your investment\namount, and may not be suitable for all investors.\n\nBefore deciding to trade in a financial instrument you should be fully informed of the risks and costs associated with trading the financial markets, carefully consider your investment objectives, level of experience, and risk appetite, and seek professional advice where needed.\n\nThe data contained in the OpenBB Platform is not necessarily accurate.\n\nOpenBB and any provider of the data contained in this website will not accept liability for any loss or damage as a result of your trading, or your reliance on the information displayed.\n\nAll names, logos, and brands of third parties that may be referenced in our sites, products or documentation are trademarks of their respective owners. Unless otherwise specified, OpenBB and its products and services are not endorsed by, sponsored by, or affiliated with these third parties.\n\nOur use of these names, logos, and brands is for identification purposes only, and does not imply any such endorsement, sponsorship, or affiliation.\n\n## 5. Contacts\n\nIf you have any questions about the platform or anything OpenBB, feel free to email us at `support@openbb.co`\n\nIf you want to say hi, or are interested in partnering with us, feel free to reach us at `hello@openbb.co`\n\nAny of our social media platforms: [openbb.co/links](https://openbb.co/links)\n\n## 6. Star History\n\nThis is a proxy of our growth and that we are just getting started.\n\nBut for more metrics important to us check [openbb.co/open](https://openbb.co/open).\n\n[![Star History Chart](https://api.star-history.com/svg?repos=openbb-finance/OpenBB&type=Date&theme=dark)](https://api.star-history.com/svg?repos=openbb-finance/OpenBB&type=Date&theme=dark)\n\n## 7. Contributors\n\nOpenBB wouldn't be OpenBB without you. If we are going to disrupt financial industry, every contribution counts. Thank you for being part of this journey.\n\n<a href=\"https://github.com/OpenBB-finance/OpenBB/graphs/contributors\">\n   <img src=\"https://contributors-img.web.app/image?repo=OpenBB-finance/OpenBB\" width=\"800\"/>\n</a>\n\n<!-- MARKDOWN LINKS & IMAGES -->\n<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->\n\n[contributors-shield]: https://img.shields.io/github/contributors/OpenBB-finance/OpenBB.svg?style=for-the-badge\n[contributors-url]: https://github.com/OpenBB-finance/OpenBB/graphs/contributors\n[forks-shield]: https://img.shields.io/github/forks/OpenBB-finance/OpenBB.svg?style=for-the-badge\n[forks-url]: https://github.com/OpenBB-finance/OpenBB/network/members\n[stars-shield]: https://img.shields.io/github/stars/OpenBB-finance/OpenBB.svg?style=for-the-badge\n[stars-url]: https://github.com/OpenBB-finance/OpenBB/stargazers\n[issues-shield]: https://img.shields.io/github/issues/OpenBB-finance/OpenBB.svg?style=for-the-badge&color=blue\n[issues-url]: https://github.com/OpenBB-finance/OpenBB/issues\n[bugs-open-shield]: https://img.shields.io/github/issues/OpenBB-finance/OpenBB/bug.svg?style=for-the-badge&color=yellow\n[bugs-open-url]: https://github.com/OpenBB-finance/OpenBB/issues?q=is%3Aissue+label%3Abug+is%3Aopen\n[bugs-closed-shield]: https://img.shields.io/github/issues-closed/OpenBB-finance/OpenBB/bug.svg?style=for-the-badge&color=success\n[bugs-closed-url]: https://github.com/OpenBB-finance/OpenBB/issues?q=is%3Aissue+label%3Abug+is%3Aclosed\n[license-shield]: https://img.shields.io/github/license/OpenBB-finance/OpenBB.svg?style=for-the-badge\n[license-url]: https://github.com/OpenBB-finance/OpenBB/blob/main/LICENSE.txt\n[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&logo=linkedin&colorB=555\n[linkedin-url]: https://linkedin.com/in/DidierRLopes\n"
  },
  {
    "name": "transformerlab-app",
    "url": "https://github.com/davidatoms/transformerlab-app",
    "description": "Open Source Application for Advanced LLM Engineering: interact, train, fine-tune, and evaluate large language models on your own computer.",
    "type": "fork",
    "updated_at": "2025-04-14T01:42:05Z",
    "readme": "<div align=\"center\">\n  <a href=\"https://transformerlab.ai\"><picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/transformerlab/transformerlab-app/refs/heads/main/assets/Transformer-Lab_Logo_Reverse.svg\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/transformerlab/transformerlab-app/refs/heads/main/assets/Transformer-Lab_Logo.svg\">\n    <img alt=\"transformer lab logo\" src=\"https://raw.githubusercontent.com/transformerlab/transformerlab-app/refs/heads/main/assets/Transformer-Lab_Logo.svg\" style=\"max-width: 650px\">\n  </picture></a>\n\n  <p align=\"center\">\n    100% Open Source Toolkit for Large Language Models: Train, Tune, Chat on your own Machine\n    <br />\n    <a href=\"https://transformerlab.ai/docs/download/\"><strong>Download</strong></a>\n    \u00b7\n    <a href=\"https://transformerlab.ai/docs/intro\"><strong>Explore the docs \u00bb</strong></a>\n    <br />\n    <br />\n    <a href=\"https://youtu.be/tY5TAvKviLo\">View Demo</a>\n    \u00b7\n    <a href=\"https://github.com/transformerlab/transformerlab-app/issues\">Report Bugs</a>\n    \u00b7\n    <a href=\"https://github.com/transformerlab/transformerlab-app/issues/new\">Suggest Features</a>\n    \u00b7\n    <a href=\"https://discord.gg/transformerlab\">Join Discord</a>\n    \u00b7\n    <a href=\"https://twitter.com/transformerlab\">Follow on Twitter</a>\n  </p>\n  <p align=\"center\">\n   Note: Transformer Lab is actively being developed. Please join our Discord or follow us on Twitter for updates. Questions, feedback and contributions are highly valued!</p>\n</div>\n\n<!-- ABOUT THE PROJECT -->\n\n## Download Now\n\n[![Download Icon]][Download URL]\n\n## About The Project\n\n![Product Screen Shot](assets/transformerlab-demo-jan2025.gif)\n\nTransformer Lab is an app that allows anyone to experiment with Large Language Models.\n\n## Backed by Mozilla\n\nTransformer Lab is proud to be supported by Mozilla through the <a href=\"https://future.mozilla.org/builders/\">Mozilla Builders Program</a>\n\n<a href=\"https://future.mozilla.org/builders/\">\n    <img src=\"https://transformerlab.ai/img/mozilla-builders-2024.png\" alt=\"Mozilla Builders Logo\" width=300>\n</a>\n\n## Features\n\nTransformer Lab allows you to:\n\n- \ud83d\udc95 **One-click Download Hundreds of Popular Models**:\n  - DeepSeek, Llama3, Qwen, Phi4, Gemma, Mistral, Mixtral, Command-R, and dozens more\n- \u2b07 **Download any LLM from Huggingface**\n- \ud83c\udfb6 **Finetune / Train Across Different Hardware**\n  - Finetune using MLX on Apple Silicon\n  - Finetune using Huggingface on GPU\n- \u2696\ufe0f **RLHF and Preference Optimization**\n  - DPO\n  - ORPO\n  - SIMPO\n  - Reward Modeling\n- \ud83d\udcbb **Work with LLMs Across Operating Systems**:\n  - Windows App\n  - MacOS App\n  - Linux\n- \ud83d\udcac **Chat with Models**\n  - Chat\n  - Completions\n  - Preset (Templated) Prompts\n  - Chat History\n  - Tweak generation parameters\n  - Batched Inference\n  - Tool Use / Function Calling (in alpha)\n- \ud83d\ude82 **Use Different Inference Engines**\n  - MLX on Apple Silicon\n  - Huggingface Transformers\n  - vLLM\n  - Llama CPP\n- \ud83e\uddd1\u200d\ud83c\udf93 **Evaluate models**\n- \ud83d\udcd6 **RAG (Retreival Augmented Generation)**\n  - Drag and Drop File UI\n  - Works on Apple MLX, Transformers, and other engines\n- \ud83d\udcd3 **Build Datasets for Training**\n  - Pull from hundreds of common datasets available on HuggingFace\n  - Provide your own dataset using drag and drop\n- \ud83d\udd22 **Calculate Embeddings**\n- \ud83d\udc81 **Full REST API**\n- \ud83c\udf29 **Run in the Cloud**\n  - You can run the user interface on your desktop/laptop while the engine runs on a remote or cloud machine\n  - Or you can run everything locally on a single machine\n- \ud83d\udd00 **Convert Models Across Platforms**\n  - Convert from/to Huggingface, MLX, GGUF\n- \ud83d\udd0c **Plugin Support**\n  - Easily pull from a library of existing plugins\n  - Write your own plugins to extend functionality\n- \ud83e\uddd1\u200d\ud83d\udcbb **Embedded Monaco Code Editor**\n  - Edit plugins and view what's happening behind the scenes\n- \ud83d\udcdd **Prompt Editing**\n  - Easily edit System Messages or Prompt Templates\n- \ud83d\udcdc **Inference Logs**\n  - While doing inference or RAG, view a log of the raw queries sent to the LLM\n\nAnd you can do the above, all through a simple cross-platform GUI.\n\n<!-- GETTING STARTED -->\n\n## Getting Started\n\n<a href=\"https://transformerlab.ai/docs/download\">Click here</a> to download Transformer Lab.\n\n<a href=\"https://transformerlab.ai/docs/intro\">Read this page</a> to learn how to install and use.\n\n### Built With\n\n- [![Electron][Electron]][Electron-url]\n- [![React][React.js]][React-url]\n- [![HuggingFace][HuggingFace]][HuggingFace-url]\n\n## Developers\n\n### Building from Scratch\n\nTo build the app yourself, pull this repo, and follow the steps below:\n\n(Please note that the current build doesn't work on Node v23 but it works on v22)\n\n```bash\nnpm install\n```\n\n```bash\nnpm start\n```\n\n## Packaging for Production\n\nTo package apps for the local platform:\n\n```bash\nnpm run package\n```\n\n<!-- LICENSE -->\n\n## License\n\nDistributed under the AGPL V3 License. See `LICENSE.txt` for more information.\n\n## Reference\n\nIf you found Transformer Lab useful in your research or applications, please cite using the following BibTeX:\n\n```\n@software{transformerlab,\n  author = {Asaria, Ali},\n  title = {Transformer Lab: Experiment with Large Language Models},\n  month = December,\n  year = 2023,\n  url = {https://github.com/transformerlab/transformerlab-app}\n}\n```\n\n<!-- CONTACT -->\n\n## Contact\n\n- [@aliasaria](https://twitter.com/aliasaria) - Ali Asasria\n- [@dadmobile](https://github.com/dadmobile) - Tony Salomone\n\n<!-- MARKDOWN LINKS & IMAGES -->\n\n[product-screenshot]: https://transformerlab.ai/assets/images/screenshot01-53ecb8c52338db3c9246cf2ebbbdc40d.png\n[React.js]: https://img.shields.io/badge/React-20232A?style=for-the-badge&logo=react&logoColor=61DAFB\n[React-url]: https://reactjs.org/\n[Electron]: https://img.shields.io/badge/Electron-20232A?style=for-the-badge&logo=electron&logoColor=61DAFB\n[Electron-url]: https://www.electronjs.org/\n[HuggingFace]: https://img.shields.io/badge/\ud83e\udd17_HuggingFace-20232A?style=for-the-badge\n[HuggingFace-url]: https://huggingface.co/\n[Download Icon]: https://img.shields.io/badge/Download-EF2D5E?style=for-the-badge&logoColor=white&logo=DocuSign\n[Download URL]: https://transformerlab.ai/docs/download\n"
  },
  {
    "name": "OpenGlass",
    "url": "https://github.com/davidatoms/OpenGlass",
    "description": "Turn any glasses into AI-powered smart glasses",
    "type": "fork",
    "updated_at": "2025-04-13T03:23:10Z",
    "readme": "## \u2139\ufe0f \ud83d\udd34 We moved this project to [Omi repository](https://github.com/BasedHardware/Omi). Current repo isn't supported anymore =>\n\n# OpenGlass - Open Source Smart Glasses\n\nTurn any glasses into hackable smart glasses with less than $25 of off-the-shelf components. Record your life, remember people you meet, identify objects, translate text, and more.\n\n![OpenGlass](https://github.com/BasedHardware/OpenGlass/assets/43514161/2fdc9d9d-2206-455c-ba60-10dbd6fb3dfb)\n\n\n## Video Demo\n\n[![OpenGlass Demo](https://img.youtube.com/vi/DsM_-c2e1ew/0.jpg)](https://youtu.be/DsM_-c2e1ew)\n\n## Want a Pre-built Version?\n\nWe will ship a limited number of pre-built kits. Fill out the [interest form](https://basedhardware.com/openglass) to get notified.\n\n## Community\n\nJoin the [Based Hardware Discord](https://discord.com/invite/ZutWMTJnwA) for setup questions, contribution guide, and more.\n\n## Getting Started\n\nFollow these steps to set up OpenGlass:\n\n### Hardware\n\n1. Gather the required components:\n   - [Seeed Studio XIAO ESP32 S3 Sense](https://www.amazon.com/dp/B0C69FFVHH/ref=dp_iou_view_item?ie=UTF8&psc=1)\n   - [EEMB LP502030 3.7v 250mAH battery](https://www.amazon.com/EEMB-Battery-Rechargeable-Lithium-Connector/dp/B08VRZTHDL)\n   - [3D printed glasses mount case](https://storage.googleapis.com/scott-misc/openglass_case.stl)\n\n2. 3D print the glasses mount case using the provided STL file.\n\n3. Open the [firmware folder](https://github.com/BasedHardware/openglass/tree/main/firmware) and open the `.ino` file in the Arduino IDE.\n   - If you don't have the Arduino IDE installed, download and install it from the [official website](https://www.arduino.cc/en/software).\n   - Alternatively, follow the steps in the [firmware readme](firmware/readme.md) to build using `arduino-cli`\n\n4. Follow the software preparation steps to set up the Arduino IDE for the XIAO ESP32S3 board:\n   - Add ESP32 board package to your Arduino IDE:\n     - Navigate to File > Preferences, and fill \"Additional Boards Manager URLs\" with the URL: `https://raw.githubusercontent.com/espressif/arduino-esp32/gh-pages/package_esp32_index.json`\n     - Navigate to Tools > Board > Boards Manager..., type the keyword `esp32` in the search box, select the latest version of `esp32`, and install it.\n   - Select your board and port:\n     - On top of the Arduino IDE, select the port (likely to be COM3 or higher).\n     - Search for `xiao` in the development board on the left and select `XIAO_ESP32S3`.\n\n5. Before you flash go to the \"Tools\" drop down in the Arduino IDE and make sure you set \"PSRAM:\" to be \"PSRAM: \"OPI PSRAM\"\n\n![Like this](image.png)\n\n6. Upload the firmware to the XIAO ESP32S3 board.\n\n### Software\n\n1. Clone the OpenGlass repository and install the dependencies:\n   ```\n   git clone https://github.com/BasedHardware/openglass.git\n   cd openglass\n   npm install\n   ```\n   You can also use **yarn** to install, by doing\n   ```\n   yarn install\n   ```\n\n3. Add API keys for Groq and OpenAI in the `keys.ts` file located at [https://github.com/BasedHardware/OpenGlass/blob/main/sources/keys.ts](https://github.com/BasedHardware/OpenGlass/blob/main/sources/keys.ts).\n\n4. For Ollama, self-host the REST API from the repository at [https://github.com/ollama/ollama](https://github.com/ollama/ollama) and add the URL to the `keys.ts` file. The URL should be http://localhost:11434/api/chat\n5. go to terminal and type \"ollama pull moondream:1.8b-v2-fp16\"\n\n\n6. Start the application:\n   ```\n   npm start\n   ```\n\n   If using **yarn** start the application with\n   ```\n   yarn start\n   ```\n\n   Note: This is an Expo project. For now, open the localhost link (this will appear after completing step 5) to access the web version.\n\n## License\n\nThis project is licensed under the MIT License.\n\n## [\u2139\ufe0f \ud83d\udd34 We moved this project to Omi repository. Current repo isn't supported anymore =>](https://github.com/BasedHardware/Omi)\n"
  }
]