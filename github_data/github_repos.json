[
  {
    "name": "davidatoms",
    "url": "https://github.com/davidatoms/davidatoms",
    "description": null,
    "type": "original",
    "updated_at": "2025-03-02T04:57:07Z",
    "readme": "# David Adams GitHub Readme\n\n<p align=\"left\"><b>Last Updated:</b> <!-- LAST_UPDATED:START --> June 21, 2024 at 15:45 (173/365 (0.474) of the year) <!-- LAST_UPDATED:END -->\n</p>\n\n<p align=\"left\">\n  <img src=\"https://img.shields.io/badge/Python-3776AB?style=flat&logo=python&logoColor=white\" />\n  <img src=\"https://img.shields.io/badge/Go-00ADD8?style=flat&logo=go&logoColor=white\" />\n  <img src=\"https://img.shields.io/badge/Rust-000000?style=flat&logo=rust&logoColor=white\" />\n  <img src=\"https://img.shields.io/badge/Racket-9F1D20?style=flat&logo=racket&logoColor=white\" />\n  <img src=\"https://img.shields.io/badge/React-20232A?style=flat&logo=react&logoColor=61DAFB\" />\n  <img src=\"https://img.shields.io/badge/Bash-4EAA25?style=flat&logo=gnu-bash&logoColor=white\" />\n</p>\n\nI am passionate about a better tomorrow through innovation and investments. Find my [resume here](https://docs.google.com/document/d/1deAcWqb8XMO_oV23URmtHWdM7cHVlyMQpMUB47OZgyM/edit?tab=t.0).\n\n## Recent Repositories\n<!-- PROJECTS-LIST:START --> \n<!-- This section will be automatically populated with your 10 most recent repositories -->\n<!-- PROJECTS-LIST:END -->\n\n_Project descriptions generated by Anthropic's Claude, backed by GitHub Actions_\n\n<!-- RECENT_FORKED_REPOS:START --> \n<!-- This section will be empty in the new setup -->\n<!-- RECENT_FORKED_REPOS:END -->\n\n<br>\n\n![Star this repository](https://img.shields.io/badge/Star%20this%20repository-FFDD00?style=flat&logo=github&logoColor=white)\n![Profile Views](https://komarev.com/ghpvc/?username=davidatoms&style=flat&color=blue&label=Views)"
  },
  {
    "name": "jupyter_tool",
    "url": "https://github.com/davidatoms/jupyter_tool",
    "description": null,
    "type": "fork",
    "updated_at": "2025-03-02T03:30:43Z",
    "readme": "# Jupyter Notebook Tool\n\nA Python package providing atomic tools for langchain-based AI agents to manipulate Jupyter notebooks. Built on\nnbclient/nbformat, it enables programmatic notebook creation, loading, and manipulation.\n\n## Installation\n\nYou can install from this repo in development mode with `pip install -e ./src/`\n\n## Core Functionality\n\nThe system maintains notebook sessions using tokens. Each operation requires a valid token obtained from either:\n\n- `create_notebook()`: Creates new empty notebook\n- `load_notebook(url)`: Loads notebook from URL or file path\n\n## Available Operations\n\nWith a valid token, the following operations are supported:\n\n- `list_cells(token)`: Get ordered list of cell IDs\n- `create_cell(token, source, cell_type, position)`: Add new cell\n- `update_cell(token, id, source)`: Modify cell content\n- `execute_cell(token, id)`: Run code cell and get output\n- `delete_cell(token, id)`: Remove cell from notebook\n- `get_notebook(token)`: Retrieve current notebook state\n\n## Error Handling\n\nThe system provides specific error types for common failure cases:\n\n- InvalidTokenError\n- CellNotFoundError\n- CellTypeError\n- NotebookLoadError\n- KernelError\n\n## Worked Example\n\nSee docs/example.ipynb for an example of how to use the package.\n"
  },
  {
    "name": "fabric",
    "url": "https://github.com/davidatoms/fabric",
    "description": "fabric is an open-source framework for augmenting humans using AI. It provides a modular framework for solving specific problems using a crowdsourced set of AI prompts that can be used anywhere.",
    "type": "fork",
    "updated_at": "2025-03-02T03:14:23Z",
    "readme": "<div align=\"center\">\n\n<img src=\"./images/fabric-logo-gif.gif\" alt=\"fabriclogo\" width=\"400\" height=\"400\"/>\n\n# `fabric`\n\n![Static Badge](https://img.shields.io/badge/mission-human_flourishing_via_AI_augmentation-purple)\n<br />\n![GitHub top language](https://img.shields.io/github/languages/top/danielmiessler/fabric)\n![GitHub last commit](https://img.shields.io/github/last-commit/danielmiessler/fabric)\n[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)\n\n<p class=\"align center\">\n<h4><code>fabric</code> is an open-source framework for augmenting humans using AI.</h4>\n</p>\n\n[Updates](#updates) \u2022\n[What and Why](#whatandwhy) \u2022\n[Philosophy](#philosophy) \u2022\n[Installation](#Installation) \u2022\n[Usage](#Usage) \u2022\n[Examples](#examples) \u2022\n[Just Use the Patterns](#just-use-the-patterns) \u2022\n[Custom Patterns](#custom-patterns) \u2022\n[Helper Apps](#helper-apps) \u2022\n[Meta](#meta)\n\n![Screenshot of fabric](images/fabric-summarize.png)\n\n</div>\n\n## Navigation\n\n- [`fabric`](#fabric)\n  - [Navigation](#navigation)\n  - [Updates](#updates)\n  - [Intro videos](#intro-videos)\n  - [What and why](#what-and-why)\n  - [Philosophy](#philosophy)\n    - [Breaking problems into components](#breaking-problems-into-components)\n    - [Too many prompts](#too-many-prompts)\n  - [Installation](#installation)\n    - [Get Latest Release Binaries](#get-latest-release-binaries)\n    - [From Source](#from-source)\n    - [Environment Variables](#environment-variables)\n    - [Setup](#setup)\n    - [Add aliases for all patterns](#add-aliases-for-all-patterns)\n      - [Save your files in markdown using aliases](#save-your-files-in-markdown-using-aliases)\n    - [Migration](#migration)\n    - [Upgrading](#upgrading)\n  - [Usage](#usage)\n  - [Our approach to prompting](#our-approach-to-prompting)\n  - [Examples](#examples)\n  - [Just use the Patterns](#just-use-the-patterns)\n  - [Custom Patterns](#custom-patterns)\n  - [Helper Apps](#helper-apps)\n    - [`to_pdf`](#to_pdf)\n    - [`to_pdf` Installation](#to_pdf-installation)\n  - [pbpaste](#pbpaste)\n  - [Web Interface](#Web_Interface)\n  - [Meta](#meta)\n    - [Primary contributors](#primary-contributors)\n\n<br />\n\n## Updates\n\n> [!NOTE]\n> February 24, 2025\n>\n> - Fabric now supports Sonnet 3.7! Update and use `-S` to select it as your default if you want, or just use the shortcut `-m claude-3-7-sonnet-latest`. Enjoy!\n\n## What and why\n\nSince the start of 2023 and GenAI we've seen a massive number of AI applications for accomplishing tasks. It's powerful, but _it's not easy to integrate this functionality into our lives._\n\n<div align=\"center\">\n<h4>In other words, AI doesn't have a capabilities problem\u2014it has an <em>integration</em> problem.</h4>\n</div>\n\nFabric was created to address this by enabling everyone to granularly apply AI to everyday challenges.\n\n## Intro videos\n\nKeep in mind that many of these were recorded when Fabric was Python-based, so remember to use the current [install instructions](#Installation) below.\n\n- [Network Chuck](https://www.youtube.com/watch?v=UbDyjIIGaxQ)\n- [David Bombal](https://www.youtube.com/watch?v=vF-MQmVxnCs)\n- [My Own Intro to the Tool](https://www.youtube.com/watch?v=wPEyyigh10g)\n- [More Fabric YouTube Videos](https://www.youtube.com/results?search_query=fabric+ai)\n\n## Philosophy\n\n> AI isn't a thing; it's a _magnifier_ of a thing. And that thing is **human creativity**.\n\nWe believe the purpose of technology is to help humans flourish, so when we talk about AI we start with the **human** problems we want to solve.\n\n### Breaking problems into components\n\nOur approach is to break problems into individual pieces (see below) and then apply AI to them one at a time. See below for some examples.\n\n<img width=\"2078\" alt=\"augmented_challenges\" src=\"https://github.com/danielmiessler/fabric/assets/50654/31997394-85a9-40c2-879b-b347e4701f06\">\n\n### Too many prompts\n\nPrompts are good for this, but the biggest challenge I faced in 2023\u2014\u2014which still exists today\u2014is **the sheer number of AI prompts out there**. We all have prompts that are useful, but it's hard to discover new ones, know if they are good or not, _and manage different versions of the ones we like_.\n\nOne of <code>fabric</code>'s primary features is helping people collect and integrate prompts, which we call _Patterns_, into various parts of their lives.\n\nFabric has Patterns for all sorts of life and work activities, including:\n\n- Extracting the most interesting parts of YouTube videos and podcasts\n- Writing an essay in your own voice with just an idea as an input\n- Summarizing opaque academic papers\n- Creating perfectly matched AI art prompts for a piece of writing\n- Rating the quality of content to see if you want to read/watch the whole thing\n- Getting summaries of long, boring content\n- Explaining code to you\n- Turning bad documentation into usable documentation\n- Creating social media posts from any content input\n- And a million more\u2026\n\n## Installation\n\nTo install Fabric, you can use the latest release binaries or install it from the source.\n\n### Get Latest Release Binaries\n\n#### Windows:\n`https://github.com/danielmiessler/fabric/releases/latest/download/fabric-windows-amd64.exe`\n\n#### MacOS (arm64):\n`curl -L https://github.com/danielmiessler/fabric/releases/latest/download/fabric-darwin-arm64 > fabric && chmod +x fabric && ./fabric --version`\n\n#### MacOS (amd64):\n`curl -L https://github.com/danielmiessler/fabric/releases/latest/download/fabric-darwin-amd64 > fabric && chmod +x fabric && ./fabric --version`\n\n#### Linux (amd64):\n`curl -L https://github.com/danielmiessler/fabric/releases/latest/download/fabric-linux-amd64 > fabric && chmod +x fabric && ./fabric --version`\n\n#### Linux (arm64):\n`curl -L https://github.com/danielmiessler/fabric/releases/latest/download/fabric-linux-arm64 > fabric && chmod +x fabric && ./fabric --version`\n\n### From Source\n\nTo install Fabric, [make sure Go is installed](https://go.dev/doc/install), and then run the following command.\n\n```bash\n# Install Fabric directly from the repo\ngo install github.com/danielmiessler/fabric@latest\n```\n\n### Environment Variables\n\nYou may need to set some environment variables in your `~/.bashrc` on linux or `~/.zshrc` file on mac to be able to run the `fabric` command. Here is an example of what you can add:\n\nFor Intel based macs or linux\n\n```bash\n# Golang environment variables\nexport GOROOT=/usr/local/go\nexport GOPATH=$HOME/go\n\n# Update PATH to include GOPATH and GOROOT binaries\nexport PATH=$GOPATH/bin:$GOROOT/bin:$HOME/.local/bin:$PATH\n```\n\nfor Apple Silicon based macs\n\n```bash\n# Golang environment variables\nexport GOROOT=$(brew --prefix go)/libexec\nexport GOPATH=$HOME/go\nexport PATH=$GOPATH/bin:$GOROOT/bin:$HOME/.local/bin:$PATH\n```\n\n### Setup\n\nNow run the following command\n\n```bash\n# Run the setup to set up your directories and keys\nfabric --setup\n```\n\nIf everything works you are good to go.\n\n### Add aliases for all patterns\n\nIn order to add aliases for all your patterns and use them directly as commands ie. `summarize` instead of `fabric --pattern summarize`\nYou can add the following to your `.zshrc` or `.bashrc` file.\n\n```bash\n# Loop through all files in the ~/.config/fabric/patterns directory\nfor pattern_file in $HOME/.config/fabric/patterns/*; do\n    # Get the base name of the file (i.e., remove the directory path)\n    pattern_name=$(basename \"$pattern_file\")\n\n    # Create an alias in the form: alias pattern_name=\"fabric --pattern pattern_name\"\n    alias_command=\"alias $pattern_name='fabric --pattern $pattern_name'\"\n\n    # Evaluate the alias command to add it to the current shell\n    eval \"$alias_command\"\ndone\n\nyt() {\n    local video_link=\"$1\"\n    fabric -y \"$video_link\" --transcript\n}\n```\n\nYou can add the below code for the equivalent aliases inside PowerShell by running `notepad $PROFILE` inside a PowerShell window:\n\n```powershell\n# Path to the patterns directory\n$patternsPath = Join-Path $HOME \".config/fabric/patterns\"\nforeach ($patternDir in Get-ChildItem -Path $patternsPath -Directory) {\n    $patternName = $patternDir.Name\n\n    # Dynamically define a function for each pattern\n    $functionDefinition = @\"\nfunction $patternName {\n    [CmdletBinding()]\n    param(\n        [Parameter(ValueFromPipeline = `$true)]\n        [string] `$InputObject,\n\n        [Parameter(ValueFromRemainingArguments = `$true)]\n        [String[]] `$patternArgs\n    )\n\n    begin {\n        # Initialize an array to collect pipeline input\n        `$collector = @()\n    }\n\n    process {\n        # Collect pipeline input objects\n        if (`$InputObject) {\n            `$collector += `$InputObject\n        }\n    }\n\n    end {\n        # Join all pipeline input into a single string, separated by newlines\n        `$pipelineContent = `$collector -join \"`n\"\n\n        # If there's pipeline input, include it in the call to fabric\n        if (`$pipelineContent) {\n            `$pipelineContent | fabric --pattern $patternName `$patternArgs\n        } else {\n            # No pipeline input; just call fabric with the additional args\n            fabric --pattern $patternName `$patternArgs\n        }\n    }\n}\n\"@\n    # Add the function to the current session\n    Invoke-Expression $functionDefinition\n}\n\n# Define the 'yt' function as well\nfunction yt {\n    [CmdletBinding()]\n    param(\n        [Parameter(Mandatory = $true)]\n        [string]$videoLink\n    )\n    fabric -y $videoLink --transcript\n}\n```\n\nThis also creates a `yt` alias that allows you to use `yt https://www.youtube.com/watch?v=4b0iet22VIk` to get transcripts, comments, and metadata.\n\n#### Save your files in markdown using aliases\n\nIf in addition to the above aliases you would like to have the option to save the output to your favourite markdown note vault like Obsidian then instead of the above add the following to your `.zshrc` or `.bashrc` file:\n\n```bash\n# Define the base directory for Obsidian notes\nobsidian_base=\"/path/to/obsidian\"\n\n# Loop through all files in the ~/.config/fabric/patterns directory\nfor pattern_file in ~/.config/fabric/patterns/*; do\n    # Get the base name of the file (i.e., remove the directory path)\n    pattern_name=$(basename \"$pattern_file\")\n\n    # Unalias any existing alias with the same name\n    unalias \"$pattern_name\" 2>/dev/null\n\n    # Define a function dynamically for each pattern\n    eval \"\n    $pattern_name() {\n        local title=\\$1\n        local date_stamp=\\$(date +'%Y-%m-%d')\n        local output_path=\\\"\\$obsidian_base/\\${date_stamp}-\\${title}.md\\\"\n\n        # Check if a title was provided\n        if [ -n \\\"\\$title\\\" ]; then\n            # If a title is provided, use the output path\n            fabric --pattern \\\"$pattern_name\\\" -o \\\"\\$output_path\\\"\n        else\n            # If no title is provided, use --stream\n            fabric --pattern \\\"$pattern_name\\\" --stream\n        fi\n    }\n    \"\ndone\n\nyt() {\n    local video_link=\"$1\"\n    fabric -y \"$video_link\" --transcript\n}\n```\n\nThis will allow you to use the patterns as aliases like in the above for example `summarize` instead of `fabric --pattern summarize --stream`, however if you pass in an extra argument like this `summarize \"my_article_title\"` your output will be saved in the destination that you set in `obsidian_base=\"/path/to/obsidian\"` in the following format `YYYY-MM-DD-my_article_title.md` where the date gets autogenerated for you.\nYou can tweak the date format by tweaking the `date_stamp` format.\n\n### Migration\n\nIf you have the Legacy (Python) version installed and want to migrate to the Go version, here's how you do it. It's basically two steps: 1) uninstall the Python version, and 2) install the Go version.\n\n```bash\n# Uninstall Legacy Fabric\npipx uninstall fabric\n\n# Clear any old Fabric aliases\n(check your .bashrc, .zshrc, etc.)\n# Install the Go version\ngo install github.com/danielmiessler/fabric@latest\n# Run setup for the new version. Important because things have changed\nfabric --setup\n```\n\nThen [set your environmental variables](#environmental-variables) as shown above.\n\n### Upgrading\n\nThe great thing about Go is that it's super easy to upgrade. Just run the same command you used to install it in the first place and you'll always get the latest version.\n\n```bash\ngo install github.com/danielmiessler/fabric@latest\n```\n\n## Usage\n\nOnce you have it all set up, here's how to use it.\n\n```bash\nfabric -h\n```\n\n```bash\n\nUsage:\n  fabric [OPTIONS]\n\nApplication Options:\n  -p, --pattern=             Choose a pattern from the available patterns\n  -v, --variable=            Values for pattern variables, e.g. -v=#role:expert -v=#points:30\"\n  -C, --context=             Choose a context from the available contexts\n      --session=             Choose a session from the available sessions\n  -a, --attachment=          Attachment path or URL (e.g. for OpenAI image recognition messages)\n  -S, --setup                Run setup for all reconfigurable parts of fabric\n  -t, --temperature=         Set temperature (default: 0.7)\n  -T, --topp=                Set top P (default: 0.9)\n  -s, --stream               Stream\n  -P, --presencepenalty=     Set presence penalty (default: 0.0)\n  -r, --raw                  Use the defaults of the model without sending chat options (like temperature etc.) and use the user role instead of the system role for patterns.\n  -F, --frequencypenalty=    Set frequency penalty (default: 0.0)\n  -l, --listpatterns         List all patterns\n  -L, --listmodels           List all available models\n  -x, --listcontexts         List all contexts\n  -X, --listsessions         List all sessions\n  -U, --updatepatterns       Update patterns\n  -c, --copy                 Copy to clipboard\n  -m, --model=               Choose model\n  -o, --output=              Output to file\n      --output-session       Output the entire session (also a temporary one) to the output file\n  -n, --latest=              Number of latest patterns to list (default: 0)\n  -d, --changeDefaultModel   Change default model\n  -y, --youtube=             YouTube video \"URL\" to grab transcript, comments from it and send to chat\n      --transcript           Grab transcript from YouTube video and send to chat (it used per default).\n      --comments             Grab comments from YouTube video and send to chat\n      --metadata             Grab metadata from YouTube video and send to chat\n  -g, --language=            Specify the Language Code for the chat, e.g. -g=en -g=zh\n  -u, --scrape_url=          Scrape website URL to markdown using Jina AI\n  -q, --scrape_question=     Search question using Jina AI\n  -e, --seed=                Seed to be used for LMM generation\n  -w, --wipecontext=         Wipe context\n  -W, --wipesession=         Wipe session\n      --printcontext=        Print context\n      --printsession=        Print session\n      --readability          Convert HTML input into a clean, readable view\n      --serve                Initiate the API server\n      --dry-run              Show what would be sent to the model without actually sending it\n      --version              Print current version\n\nHelp Options:\n  -h, --help                 Show this help message\n\n```\n\n## Our approach to prompting\n\nFabric _Patterns_ are different than most prompts you'll see.\n\n- **First, we use `Markdown` to help ensure maximum readability and editability**. This not only helps the creator make a good one, but also anyone who wants to deeply understand what it does. _Importantly, this also includes the AI you're sending it to!_\n\nHere's an example of a Fabric Pattern.\n\n```bash\nhttps://github.com/danielmiessler/fabric/blob/main/patterns/extract_wisdom/system.md\n```\n\n<img width=\"1461\" alt=\"pattern-example\" src=\"https://github.com/danielmiessler/fabric/assets/50654/b910c551-9263-405f-9735-71ca69bbab6d\">\n\n- **Next, we are extremely clear in our instructions**, and we use the Markdown structure to emphasize what we want the AI to do, and in what order.\n\n- **And finally, we tend to use the System section of the prompt almost exclusively**. In over a year of being heads-down with this stuff, we've just seen more efficacy from doing that. If that changes, or we're shown data that says otherwise, we will adjust.\n\n## Examples\n\n> The following examples use the macOS `pbpaste` to paste from the clipboard. See the [pbpaste](#pbpaste) section below for Windows and Linux alternatives.\n\nNow let's look at some things you can do with Fabric.\n\n1. Run the `summarize` Pattern based on input from `stdin`. In this case, the body of an article.\n\n```bash\npbpaste | fabric --pattern summarize\n```\n\n2. Run the `analyze_claims` Pattern with the `--stream` option to get immediate and streaming results.\n\n```bash\npbpaste | fabric --stream --pattern analyze_claims\n```\n\n3. Run the `extract_wisdom` Pattern with the `--stream` option to get immediate and streaming results from any Youtube video (much like in the original introduction video).\n\n```bash\nfabric -y \"https://youtube.com/watch?v=uXs-zPc63kM\" --stream --pattern extract_wisdom\n```\n\n\n4. Create patterns- you must create a .md file with the pattern and save it to `~/.config/fabric/patterns/[yourpatternname]`.\n\n\n5. Run a `analyze_claims` pattern on a website. Fabric uses Jina AI to scrape the URL into markdown format before sending it to the model.\n\n```bash\nfabric -u https://github.com/danielmiessler/fabric/ -p analyze_claims\n```\n\n## Just use the Patterns\n\n<img width=\"1173\" alt=\"fabric-patterns-screenshot\" src=\"https://github.com/danielmiessler/fabric/assets/50654/9186a044-652b-4673-89f7-71cf066f32d8\">\n\n<br />\n<br />\n\nIf you're not looking to do anything fancy, and you just want a lot of great prompts, you can navigate to the [`/patterns`](https://github.com/danielmiessler/fabric/tree/main/patterns) directory and start exploring!\n\nWe hope that if you used nothing else from Fabric, the Patterns by themselves will make the project useful.\n\nYou can use any of the Patterns you see there in any AI application that you have, whether that's ChatGPT or some other app or website. Our plan and prediction is that people will soon be sharing many more than those we've published, and they will be way better than ours.\n\nThe wisdom of crowds for the win.\n\n## Custom Patterns\n\nYou may want to use Fabric to create your own custom Patterns\u2014but not share them with others. No problem!\n\nJust make a directory in `~/.config/custompatterns/` (or wherever) and put your `.md` files in there.\n\nWhen you're ready to use them, copy them into:\n\n```\n~/.config/fabric/patterns/\n```\n\nYou can then use them like any other Patterns, but they won't be public unless you explicitly submit them as Pull Requests to the Fabric project. So don't worry\u2014they're private to you.\n\n\n## Helper Apps\n\nFabric also makes use of some core helper apps (tools) to make it easier to integrate with your various workflows. Here are some examples:\n\n### `to_pdf`\n\n`to_pdf` is a helper command that converts LaTeX files to PDF format. You can use it like this:\n\n```bash\nto_pdf input.tex\n```\n\nThis will create a PDF file from the input LaTeX file in the same directory.\n\nYou can also use it with stdin which works perfectly with the `write_latex` pattern:\n\n```bash\necho \"ai security primer\" | fabric --pattern write_latex | to_pdf\n```\n\nThis will create a PDF file named `output.pdf` in the current directory.\n\n### `to_pdf` Installation\n\nTo install `to_pdf`, install it the same way as you install Fabric, just with a different repo name.\n\n```bash\ngo install github.com/danielmiessler/fabric/plugins/tools/to_pdf@latest\n```\n\nMake sure you have a LaTeX distribution (like TeX Live or MiKTeX) installed on your system, as `to_pdf` requires `pdflatex` to be available in your system's PATH.\n\n## pbpaste\n\nThe [examples](#examples) use the macOS program `pbpaste` to paste content from the clipboard to pipe into `fabric` as the input. `pbpaste` is not available on Windows or Linux, but there are alternatives.\n\nOn Windows, you can use the PowerShell command `Get-Clipboard` from a PowerShell command prompt. If you like, you can also alias it to `pbpaste`. If you are using classic PowerShell, edit the file `~\\Documents\\WindowsPowerShell\\.profile.ps1`, or if you are using PowerShell Core, edit `~\\Documents\\PowerShell\\.profile.ps1` and add the alias,\n\n```powershell\nSet-Alias pbpaste Get-Clipboard\n```\n\nOn Linux, you can use `xclip -selection clipboard -o` to paste from the clipboard. You will likely need to install `xclip` with your package manager. For Debian based systems including Ubuntu,\n\n```sh\nsudo apt update\nsudo apt install xclip -y\n```\n\nYou can also create an alias by editing `~/.bashrc` or `~/.zshrc` and adding the alias,\n\n```sh\nalias pbpaste='xclip -selection clipboard -o'\n```\n\n## Web Interface\n\nFabric now includes a built-in web interface that provides a GUI alternative to the command-line interface and an out-of-the-box website for those who want to get started with web development or blogging.  \nYou can use this app as a GUI interface for Fabric, a ready to go blog-site, or a website template for your own projects.\n\nThe `web/src/lib/content` directory includes starter `.obsidian/` and `templates/` directories, allowing you to open up the `web/src/lib/content/` directory as an [Obsidian.md](https://obsidian.md) vault. You can place your posts in the posts directory when you're ready to publish.\n\n### Installing\n\nThe GUI can be installed by navigating to the\u00a0`web`\u00a0directory and using\u00a0`npm install`,\u00a0`pnpm install`, or your favorite package manager. Then simply run\u00a0the development server to start the app.\n\n_You will need to run fabric in a separate terminal with the\u00a0`fabric --serve`\u00a0command._\n\n**From the fabric project `web/` directory:**\n\n```shell\nnpm run dev\n\n## or ##\n\npnpm run dev\n\n## or your equivalent\n```\n\n### Streamlit UI\n\nTo run the Streamlit user interface:\n\n```bash\n# Install required dependencies\npip install streamlit pandas matplotlib seaborn numpy python-dotenv\n\n# Run the Streamlit app\nstreamlit run streamlit.py\n```\n\nThe Streamlit UI provides a user-friendly interface for:\n\n- Running and chaining patterns\n- Managing pattern outputs\n- Creating and editing patterns\n- Analyzing pattern results\n\n## Meta\n\n> [!NOTE]\n> Special thanks to the following people for their inspiration and contributions!\n\n- _Jonathan Dunn_ for being the absolute MVP dev on the project, including spearheading the new Go version, as well as the GUI! All this while also being a full-time medical doctor!\n- _Caleb Sima_ for pushing me over the edge of whether to make this a public project or not.\n- _Eugen Eisler_ and _Frederick Ros_ for their invaluable contributions to the Go version\n- _David Peters_ for his work on the web interface.\n- _Joel Parish_ for super useful input on the project's Github directory structure..\n- _Joseph Thacker_ for the idea of a `-c` context flag that adds pre-created context in the `./config/fabric/` directory to all Pattern queries.\n- _Jason Haddix_ for the idea of a stitch (chained Pattern) to filter content using a local model before sending on to a cloud model, i.e., cleaning customer data using `llama2` before sending on to `gpt-4` for analysis.\n- _Andre Guerra_ for assisting with numerous components to make things simpler and more maintainable.\n\n### Primary contributors\n\n<a href=\"https://github.com/danielmiessler\"><img src=\"https://avatars.githubusercontent.com/u/50654?v=4\" title=\"Daniel Miessler\" width=\"50\" height=\"50\"></a>\n<a href=\"https://github.com/xssdoctor\"><img src=\"https://avatars.githubusercontent.com/u/9218431?v=4\" title=\"Jonathan Dunn\" width=\"50\" height=\"50\"></a>\n<a href=\"https://github.com/sbehrens\"><img src=\"https://avatars.githubusercontent.com/u/688589?v=4\" title=\"Scott Behrens\" width=\"50\" height=\"50\"></a>\n<a href=\"https://github.com/agu3rra\"><img src=\"https://avatars.githubusercontent.com/u/10410523?v=4\" title=\"Andre Guerra\" width=\"50\" height=\"50\"></a>\n\n`fabric` was created by <a href=\"https://danielmiessler.com/subscribe\" target=\"_blank\">Daniel Miessler</a> in January of 2024.\n<br /><br />\n<a href=\"https://twitter.com/intent/user?screen_name=danielmiessler\">![X (formerly Twitter) Follow](https://img.shields.io/twitter/follow/danielmiessler)</a>\n"
  },
  {
    "name": "buster",
    "url": "https://github.com/davidatoms/buster",
    "description": "The open-source, AI-native data stack",
    "type": "fork",
    "updated_at": "2025-03-01T07:50:18Z",
    "readme": "![Buster GitHub Banner](/assets/image.png)\n\n<div align=\"center\"><h1>The Buster Platform</h1></div>\n<div align=\"center\"><h4>A modern analytics platform for AI-powered data applications</h4></div>\n\n<div align=\"center\">\n   <div>\n      <h3>\n         <a href=\"https://www.buster.so/get-started\">\n            <strong>Sign up</strong>\n         </a> \u00b7 \n         <a href=\"#quickstart\">\n            <strong>Quickstart</strong>\n         </a> \u00b7 \n         <a href=\"#deployment\">\n            <strong>Deployment</strong>\n         </a>\n      </h3>\n   </div>\n\n   <div>\n      <a href=\"https://github.com/buster-so/warehouse/blob/main/LICENSE\"><img alt=\"MIT License\" src=\"https://img.shields.io/badge/License-MIT-red.svg?style=flat-square\" ></a>\n      <a href=\"https://www.ycombinator.com/companies/buster\"><img src=\"https://img.shields.io/badge/Y%20Combinator-W24-orange?style=flat-square\" alt=\"Y Combinator W24\"></a>\n   </div>\n</div>\n</br>\n\n## What is Buster?\n\nBuster is a modern analytics platform built from the ground up with AI in mind.\n\nWe've spent the last two years working with companies to help them implement Large Language Models in their data stack.  This has mainly revolved around truly self-serve experiences that are powered by Large Language Models.  We've noticed a few pain points when it comes to the tools that are available today:\n\n1. Slapping an AI copilot on top of existing BI tools can often result in a subpar experience for users. To deploy a powerful analytics experience, we believe that the entire app needs to be built from the ground up with AI in mind. \n2. Most organizations can't deploy ad-hoc, self-serve experiences for their users because their warehousing costs/performance are too prohibitive.  We believe that new storage formats like Apache Iceberg and query engines like Starrocks and DuckDB have the potential to change data warehousing and make it more accessible for the type of workloads that come with AI-powered analytics experiences.\n3. The current CI/CD process for most analytics stacks struggle to keep up with changes and often result in broken dashboards, slow query performance, and other issues.  Introducing hundreds, if not thousands of user queries made with Large Language Models can exacerbate these issues and make it nearly impossible to maintain. We believe there is a huge opportunity to rethink how Large Language Models can be used to improve this process with workflows around self-healing, model suggestions, and more.\n4. Current tools don't have tooling or workflows built around augmenting data teams.  They are designed for the analyst to continue working as they did before, instead of helping them build powerful data experiences for their users.  We believe that instead of spending hours and hours building out unfulfilling dashboards, data teams should be empowered to build out powerful, self-serve experiences for their users.\n\nUltimately, we believe that the future of AI analytics is about helping data teams build powerful, self-serve experiences for their users. We think that requires a new approach to the analytics stack.  One that allows for deep integrations between products and allows data teams to truly own their entire experience.\n\n## Roadmap\n\nCurrently, we are in the process of open-sourcing the platform.  This includes:\n\n- [Warehouse](/warehouse) \u2705\n- [BI platform](https://buster.so) \u23f0\n\nAfter that, we will release an official roadmap.\n\n## How We Plan to Make Money\n\nCurrently, we offer a few commercial products:\n\n- [Cloud-Hosted Versions](https://buster.so)\n  - Warehouse\n    - Cluster\n    - Serverless\n  - BI Platform\n- Managed Self-Hosted Version of the Warehouse product.\n\n## Support and feedback\n\nYou can contact us through either:\n\n- [Github Discussions](https://github.com/orgs/buster-so/discussions)\n- Email us at founders at buster dot com\n\n## License\n\nThis repository is MIT licensed, except for the `ee` folders. See [LICENSE](LICENSE) for more details."
  },
  {
    "name": "gnome-calendar",
    "url": "https://github.com/davidatoms/gnome-calendar",
    "description": "Read-only mirror of https://gitlab.gnome.org/GNOME/gnome-calendar",
    "type": "fork",
    "updated_at": "2025-02-26T22:56:53Z",
    "readme": "# GNOME Calendar\n\nGNOME Calendar is a simple and beautiful calendar application for GNOME. We give\na lot of attention to details, and as such, design is an essential and ongoing\neffort.\n\n[![Flatpak](https://flathub.org/api/badge?svg&locale=en)](https://flathub.org/apps/details/org.gnome.Calendar)\n\n\n## Useful links\n\n- Homepage: <https://apps.gnome.org/Calendar/>\n- Report issues: <https://gitlab.gnome.org/GNOME/gnome-calendar/issues/>\n- Donate: <https://www.gnome.org/donate/>\n- Translate: <https://l10n.gnome.org/module/gnome-calendar/>\n"
  },
  {
    "name": "supervision",
    "url": "https://github.com/davidatoms/supervision",
    "description": "We write your reusable computer vision tools. \ud83d\udc9c",
    "type": "fork",
    "updated_at": "2025-02-26T20:59:20Z",
    "readme": "<div align=\"center\">\n  <p>\n    <a align=\"center\" href=\"\" target=\"https://supervision.roboflow.com\">\n      <img\n        width=\"100%\"\n        src=\"https://media.roboflow.com/open-source/supervision/rf-supervision-banner.png?updatedAt=1678995927529\"\n      >\n    </a>\n  </p>\n\n<br>\n\n[notebooks](https://github.com/roboflow/notebooks) | [inference](https://github.com/roboflow/inference) | [autodistill](https://github.com/autodistill/autodistill) | [maestro](https://github.com/roboflow/multimodal-maestro)\n\n<br>\n\n[![version](https://badge.fury.io/py/supervision.svg)](https://badge.fury.io/py/supervision)\n[![downloads](https://img.shields.io/pypi/dm/supervision)](https://pypistats.org/packages/supervision)\n[![snyk](https://snyk.io/advisor/python/supervision/badge.svg)](https://snyk.io/advisor/python/supervision)\n[![license](https://img.shields.io/pypi/l/supervision)](https://github.com/roboflow/supervision/blob/main/LICENSE.md)\n[![python-version](https://img.shields.io/pypi/pyversions/supervision)](https://badge.fury.io/py/supervision)\n[![colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow/supervision/blob/main/demo.ipynb)\n[![gradio](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/Roboflow/Annotators)\n[![discord](https://img.shields.io/discord/1159501506232451173?logo=discord&label=discord&labelColor=fff&color=5865f2&link=https%3A%2F%2Fdiscord.gg%2FGbfgXGJ8Bk)](https://discord.gg/GbfgXGJ8Bk)\n[![built-with-material-for-mkdocs](https://img.shields.io/badge/Material_for_MkDocs-526CFE?logo=MaterialForMkDocs&logoColor=white)](https://squidfunk.github.io/mkdocs-material/)\n\n  <div align=\"center\">\n    <a href=\"https://trendshift.io/repositories/124\"  target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/124\" alt=\"roboflow%2Fsupervision | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n  </div>\n\n</div>\n\n## \ud83d\udc4b hello\n\n**We write your reusable computer vision tools.** Whether you need to load your dataset from your hard drive, draw detections on an image or video, or count how many detections are in a zone. You can count on us! \ud83e\udd1d\n\n## \ud83d\udcbb install\n\nPip install the supervision package in a\n[**Python>=3.8**](https://www.python.org/) environment.\n\n```bash\npip install supervision\n```\n\nRead more about conda, mamba, and installing from source in our [guide](https://roboflow.github.io/supervision/).\n\n## \ud83d\udd25 quickstart\n\n### models\n\nSupervision was designed to be model agnostic. Just plug in any classification, detection, or segmentation model. For your convenience, we have created [connectors](https://supervision.roboflow.com/latest/detection/core/#detections) for the most popular libraries like Ultralytics, Transformers, or MMDetection.\n\n```python\nimport cv2\nimport supervision as sv\nfrom ultralytics import YOLO\n\nimage = cv2.imread(...)\nmodel = YOLO(\"yolov8s.pt\")\nresult = model(image)[0]\ndetections = sv.Detections.from_ultralytics(result)\n\nlen(detections)\n# 5\n```\n\n<details>\n<summary>\ud83d\udc49 more model connectors</summary>\n\n- inference\n\n  Running with [Inference](https://github.com/roboflow/inference) requires a [Roboflow API KEY](https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key).\n\n  ```python\n  import cv2\n  import supervision as sv\n  from inference import get_model\n\n  image = cv2.imread(...)\n  model = get_model(model_id=\"yolov8s-640\", api_key=<ROBOFLOW API KEY>)\n  result = model.infer(image)[0]\n  detections = sv.Detections.from_inference(result)\n\n  len(detections)\n  # 5\n  ```\n\n</details>\n\n### annotators\n\nSupervision offers a wide range of highly customizable [annotators](https://supervision.roboflow.com/latest/detection/annotators/), allowing you to compose the perfect visualization for your use case.\n\n```python\nimport cv2\nimport supervision as sv\n\nimage = cv2.imread(...)\ndetections = sv.Detections(...)\n\nbox_annotator = sv.BoxAnnotator()\nannotated_frame = box_annotator.annotate(\n  scene=image.copy(),\n  detections=detections)\n```\n\nhttps://github.com/roboflow/supervision/assets/26109316/691e219c-0565-4403-9218-ab5644f39bce\n\n### datasets\n\nSupervision provides a set of [utils](https://supervision.roboflow.com/latest/datasets/core/) that allow you to load, split, merge, and save datasets in one of the supported formats.\n\n```python\nimport supervision as sv\nfrom roboflow import Roboflow\n\nproject = Roboflow().workspace(<WORKSPACE_ID>).project(<PROJECT_ID>)\ndataset = project.version(<PROJECT_VERSION>).download(\"coco\")\n\nds = sv.DetectionDataset.from_coco(\n    images_directory_path=f\"{dataset.location}/train\",\n    annotations_path=f\"{dataset.location}/train/_annotations.coco.json\",\n)\n\npath, image, annotation = ds[0]\n    # loads image on demand\n\nfor path, image, annotation in ds:\n    # loads image on demand\n```\n\n<details close>\n<summary>\ud83d\udc49 more dataset utils</summary>\n\n- load\n\n  ```python\n  dataset = sv.DetectionDataset.from_yolo(\n      images_directory_path=...,\n      annotations_directory_path=...,\n      data_yaml_path=...\n  )\n\n  dataset = sv.DetectionDataset.from_pascal_voc(\n      images_directory_path=...,\n      annotations_directory_path=...\n  )\n\n  dataset = sv.DetectionDataset.from_coco(\n      images_directory_path=...,\n      annotations_path=...\n  )\n  ```\n\n- split\n\n  ```python\n  train_dataset, test_dataset = dataset.split(split_ratio=0.7)\n  test_dataset, valid_dataset = test_dataset.split(split_ratio=0.5)\n\n  len(train_dataset), len(test_dataset), len(valid_dataset)\n  # (700, 150, 150)\n  ```\n\n- merge\n\n  ```python\n  ds_1 = sv.DetectionDataset(...)\n  len(ds_1)\n  # 100\n  ds_1.classes\n  # ['dog', 'person']\n\n  ds_2 = sv.DetectionDataset(...)\n  len(ds_2)\n  # 200\n  ds_2.classes\n  # ['cat']\n\n  ds_merged = sv.DetectionDataset.merge([ds_1, ds_2])\n  len(ds_merged)\n  # 300\n  ds_merged.classes\n  # ['cat', 'dog', 'person']\n  ```\n\n- save\n\n  ```python\n  dataset.as_yolo(\n      images_directory_path=...,\n      annotations_directory_path=...,\n      data_yaml_path=...\n  )\n\n  dataset.as_pascal_voc(\n      images_directory_path=...,\n      annotations_directory_path=...\n  )\n\n  dataset.as_coco(\n      images_directory_path=...,\n      annotations_path=...\n  )\n  ```\n\n- convert\n\n  ```python\n  sv.DetectionDataset.from_yolo(\n      images_directory_path=...,\n      annotations_directory_path=...,\n      data_yaml_path=...\n  ).as_pascal_voc(\n      images_directory_path=...,\n      annotations_directory_path=...\n  )\n  ```\n\n</details>\n\n## \ud83c\udfac tutorials\n\nWant to learn how to use Supervision? Explore our [how-to guides](https://supervision.roboflow.com/develop/how_to/detect_and_annotate/), [end-to-end examples](https://github.com/roboflow/supervision/tree/develop/examples), [cheatsheet](https://roboflow.github.io/cheatsheet-supervision/), and [cookbooks](https://supervision.roboflow.com/develop/cookbooks/)!\n\n<br/>\n\n<p align=\"left\">\n<a href=\"https://youtu.be/hAWpsIuem10\" title=\"Dwell Time Analysis with Computer Vision | Real-Time Stream Processing\"><img src=\"https://github.com/SkalskiP/SkalskiP/assets/26109316/a742823d-c158-407d-b30f-063a5d11b4e1\" alt=\"Dwell Time Analysis with Computer Vision | Real-Time Stream Processing\" width=\"300px\" align=\"left\" /></a>\n<a href=\"https://youtu.be/hAWpsIuem10\" title=\"Dwell Time Analysis with Computer Vision | Real-Time Stream Processing\"><strong>Dwell Time Analysis with Computer Vision | Real-Time Stream Processing</strong></a>\n<div><strong>Created: 5 Apr 2024</strong></div>\n<br/>Learn how to use computer vision to analyze wait times and optimize processes. This tutorial covers object detection, tracking, and calculating time spent in designated zones. Use these techniques to improve customer experience in retail, traffic management, or other scenarios.</p>\n\n<br/>\n\n<p align=\"left\">\n<a href=\"https://youtu.be/uWP6UjDeZvY\" title=\"Speed Estimation & Vehicle Tracking | Computer Vision | Open Source\"><img src=\"https://github.com/SkalskiP/SkalskiP/assets/26109316/61a444c8-b135-48ce-b979-2a5ab47c5a91\" alt=\"Speed Estimation & Vehicle Tracking | Computer Vision | Open Source\" width=\"300px\" align=\"left\" /></a>\n<a href=\"https://youtu.be/uWP6UjDeZvY\" title=\"Speed Estimation & Vehicle Tracking | Computer Vision | Open Source\"><strong>Speed Estimation & Vehicle Tracking | Computer Vision | Open Source</strong></a>\n<div><strong>Created: 11 Jan 2024</strong></div>\n<br/>Learn how to track and estimate the speed of vehicles using YOLO, ByteTrack, and Roboflow Inference. This comprehensive tutorial covers object detection, multi-object tracking, filtering detections, perspective transformation, speed estimation, visualization improvements, and more.</p>\n\n## \ud83d\udc9c built with supervision\n\nDid you build something cool using supervision? [Let us know!](https://github.com/roboflow/supervision/discussions/categories/built-with-supervision)\n\nhttps://user-images.githubusercontent.com/26109316/207858600-ee862b22-0353-440b-ad85-caa0c4777904.mp4\n\nhttps://github.com/roboflow/supervision/assets/26109316/c9436828-9fbf-4c25-ae8c-60e9c81b3900\n\nhttps://github.com/roboflow/supervision/assets/26109316/3ac6982f-4943-4108-9b7f-51787ef1a69f\n\n## \ud83d\udcda documentation\n\nVisit our [documentation](https://roboflow.github.io/supervision) page to learn how supervision can help you build computer vision applications faster and more reliably.\n\n## \ud83c\udfc6 contribution\n\nWe love your input! Please see our [contributing guide](https://github.com/roboflow/supervision/blob/main/CONTRIBUTING.md) to get started. Thank you \ud83d\ude4f to all our contributors!\n\n<p align=\"center\">\n    <a href=\"https://github.com/roboflow/supervision/graphs/contributors\">\n      <img src=\"https://contrib.rocks/image?repo=roboflow/supervision\" />\n    </a>\n</p>\n\n<br>\n\n<div align=\"center\">\n\n<div align=\"center\">\n      <a href=\"https://youtube.com/roboflow\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/youtube.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949634652\"\n            width=\"3%\"\n          />\n      </a>\n      <img src=\"https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png\" width=\"3%\"/>\n      <a href=\"https://roboflow.com\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/roboflow-app.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949746649\"\n            width=\"3%\"\n          />\n      </a>\n      <img src=\"https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png\" width=\"3%\"/>\n      <a href=\"https://www.linkedin.com/company/roboflow-ai/\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/linkedin.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949633691\"\n            width=\"3%\"\n          />\n      </a>\n      <img src=\"https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png\" width=\"3%\"/>\n      <a href=\"https://docs.roboflow.com\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/knowledge.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949634511\"\n            width=\"3%\"\n          />\n      </a>\n      <img src=\"https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png\" width=\"3%\"/>\n      <a href=\"https://discuss.roboflow.com\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/forum.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949633584\"\n            width=\"3%\"\n          />\n      <img src=\"https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png\" width=\"3%\"/>\n      <a href=\"https://blog.roboflow.com\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/blog.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949633605\"\n            width=\"3%\"\n          />\n      </a>\n      </a>\n  </div>\n</div>\n"
  },
  {
    "name": "microeconomic-modeling",
    "url": "https://github.com/davidatoms/microeconomic-modeling",
    "description": null,
    "type": "original",
    "updated_at": "2025-02-25T08:51:40Z",
    "readme": "# microeconomic-modeling\n\n![Companies Cost Revenue Profit](graphs/companies_cost_rev_prof.png)\n\n## description\nUsing Mathematics for Economists, I am modeling different companies cost, revenue, and profit functions.\n\n## purpose\nUse this to build out bottoms-up models. It was something I always wanted to model when I was at the Utah Valley University Wolverine Fund. I also hope to have it connect to a dataroom and then pull the data in, restructure it, and then model it in real time.\n\n## note\nFor earlier stage companies, the real-time monitoring isn't viable, but if it could use this model somehow to divide the revenue of a public company from their filings on SEC, I think it could be a better way to forecast their revenues and estimate their cost expenditures.\n\nFor example, will the expenditures on AI affect the cost and the revenue of the company that purchased them? Then, how will this affect the company's revenue where they purchased the equipment? \n\n## installation\nsteps to intall\n\n## usage\nedit the parameters\n\n## next steps\n- make it more dynamic for different intake, larger product mixes, and so forth. \ninclude more firms.\n- include exogenous parameters to build the market up and understand the supply. \n- include returns to scale\n\n"
  },
  {
    "name": "DeepLearningFlappyBird",
    "url": "https://github.com/davidatoms/DeepLearningFlappyBird",
    "description": "Flappy Bird hack using Deep Reinforcement Learning (Deep Q-learning).",
    "type": "fork",
    "updated_at": "2025-02-24T22:22:52Z",
    "readme": "# Using Deep Q-Network to Learn How To Play Flappy Bird\n\n<img src=\"./images/flappy_bird_demp.gif\" width=\"250\">\n\n7 mins version: [DQN for flappy bird](https://www.youtube.com/watch?v=THhUXIhjkCM)\n\n## Overview\nThis project follows the description of the Deep Q Learning algorithm described in Playing Atari with Deep Reinforcement Learning [2] and shows that this learning algorithm can be further generalized to the notorious Flappy Bird.\n\n## Installation Dependencies:\n* Python 2.7 or 3\n* TensorFlow 0.7\n* pygame\n* OpenCV-Python\n\n## How to Run?\n```\ngit clone https://github.com/yenchenlin1994/DeepLearningFlappyBird.git\ncd DeepLearningFlappyBird\npython deep_q_network.py\n```\n\n## What is Deep Q-Network?\nIt is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards.\n\nFor those who are interested in deep reinforcement learning, I highly recommend to read the following post:\n\n[Demystifying Deep Reinforcement Learning](http://www.nervanasys.com/demystifying-deep-reinforcement-learning/)\n\n## Deep Q-Network Algorithm\n\nThe pseudo-code for the Deep Q Learning algorithm, as given in [1], can be found below:\n\n```\nInitialize replay memory D to size N\nInitialize action-value function Q with random weights\nfor episode = 1, M do\n    Initialize state s_1\n    for t = 1, T do\n        With probability \u03f5 select random action a_t\n        otherwise select a_t=max_a  Q(s_t,a; \u03b8_i)\n        Execute action a_t in emulator and observe r_t and s_(t+1)\n        Store transition (s_t,a_t,r_t,s_(t+1)) in D\n        Sample a minibatch of transitions (s_j,a_j,r_j,s_(j+1)) from D\n        Set y_j:=\n            r_j for terminal s_(j+1)\n            r_j+\u03b3*max_(a^' )  Q(s_(j+1),a'; \u03b8_i) for non-terminal s_(j+1)\n        Perform a gradient step on (y_j-Q(s_j,a_j; \u03b8_i))^2 with respect to \u03b8\n    end for\nend for\n```\n\n## Experiments\n\n#### Environment\nSince deep Q-network is trained on the raw pixel values observed from the game screen at each time step, [3] finds that remove the background appeared in the original game can make it converge faster. This process can be visualized as the following figure:\n\n<img src=\"./images/preprocess.png\" width=\"450\">\n\n#### Network Architecture\nAccording to [1], I first preprocessed the game screens with following steps:\n\n1. Convert image to grayscale\n2. Resize image to 80x80\n3. Stack last 4 frames to produce an 80x80x4 input array for network\n\nThe architecture of the network is shown in the figure below. The first layer convolves the input image with an 8x8x4x32 kernel at a stride size of 4. The output is then put through a 2x2 max pooling layer. The second layer convolves with a 4x4x32x64 kernel at a stride of 2. We then max pool again. The third layer convolves with a 3x3x64x64 kernel at a stride of 1. We then max pool one more time. The last hidden layer consists of 256 fully connected ReLU nodes.\n\n<img src=\"./images/network.png\">\n\nThe final output layer has the same dimensionality as the number of valid actions which can be performed in the game, where the 0th index always corresponds to doing nothing. The values at this output layer represent the Q function given the input state for each valid action. At each time step, the network performs whichever action corresponds to the highest Q value using a \u03f5 greedy policy.\n\n\n#### Training\nAt first, I initialize all weight matrices randomly using a normal distribution with a standard deviation of 0.01, then set the replay memory with a max size of 500,00 experiences.\n\nI start training by choosing actions uniformly at random for the first 10,000 time steps, without updating the network weights. This allows the system to populate the replay memory before training begins.\n\nNote that unlike [1], which initialize \u03f5 = 1, I linearly anneal \u03f5 from 0.1 to 0.0001 over the course of the next 3000,000 frames. The reason why I set it this way is that agent can choose an action every 0.03s (FPS=30) in our game, high \u03f5 will make it **flap** too much and thus keeps itself at the top of the game screen and finally bump the pipe in a clumsy way. This condition will make Q function converge relatively slow since it only start to look other conditions when \u03f5 is low.\nHowever, in other games, initialize \u03f5 to 1 is more reasonable.\n\nDuring training time, at each time step, the network samples minibatches of size 32 from the replay memory to train on, and performs a gradient step on the loss function described above using the Adam optimization algorithm with a learning rate of 0.000001. After annealing finishes, the network continues to train indefinitely, with \u03f5 fixed at 0.001.\n\n## FAQ\n\n#### Checkpoint not found\nChange [first line of `saved_networks/checkpoint`](https://github.com/yenchenlin1994/DeepLearningFlappyBird/blob/master/saved_networks/checkpoint#L1) to \n\n`model_checkpoint_path: \"saved_networks/bird-dqn-2920000\"`\n\n#### How to reproduce?\n1. Comment out [these lines](https://github.com/yenchenlin1994/DeepLearningFlappyBird/blob/master/deep_q_network.py#L108-L112)\n\n2. Modify `deep_q_network.py`'s parameter as follow:\n```python\nOBSERVE = 10000\nEXPLORE = 3000000\nFINAL_EPSILON = 0.0001\nINITIAL_EPSILON = 0.1\n```\n\n## References\n\n[1] Mnih Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. **Human-level Control through Deep Reinforcement Learning**. Nature, 529-33, 2015.\n\n[2] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. **Playing Atari with Deep Reinforcement Learning**. NIPS, Deep Learning workshop\n\n[3] Kevin Chen. **Deep Reinforcement Learning for Flappy Bird** [Report](http://cs229.stanford.edu/proj2015/362_report.pdf) | [Youtube result](https://youtu.be/9WKBzTUsPKc)\n\n## Disclaimer\nThis work is highly based on the following repos:\n\n1. [sourabhv/FlapPyBird] (https://github.com/sourabhv/FlapPyBird)\n2. [asrivat1/DeepLearningVideoGames](https://github.com/asrivat1/DeepLearningVideoGames)\n\n"
  },
  {
    "name": "notebooks",
    "url": "https://github.com/davidatoms/notebooks",
    "description": "This repository offers a comprehensive collection of tutorials on state-of-the-art computer vision models and techniques. Explore everything from foundational architectures like ResNet to cutting-edge models like YOLO11, RT-DETR, SAM 2, Florence-2, PaliGemma 2, and Qwen2.5VL.",
    "type": "fork",
    "updated_at": "2025-02-23T03:11:59Z",
    "readme": "<div align=\"center\">\n  <p>\n    <a align=\"center\" href=\"\" target=\"_blank\">\n      <img\n        width=\"850\"\n        src=\"https://raw.githubusercontent.com/roboflow/notebooks/main/assets/roboflow-notebooks-banner.png\"\n      >\n    </a>\n  </p>\n  <br>\n\n  [notebooks](https://github.com/roboflow/notebooks) | [inference](https://github.com/roboflow/inference) | [autodistill](https://github.com/autodistill/autodistill) | [collect](https://github.com/roboflow/roboflow-collect)\n\n  <br>\n\n  <div align=\"center\">\n      <a href=\"https://youtube.com/roboflow\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/youtube.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949634652\"\n            width=\"3%\"\n          />\n      </a>\n      <img src=\"https://github.com/SkalskiP/SkalskiP/blob/master/icons/transparent.png\" width=\"3%\"/>\n      <a href=\"https://roboflow.com\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/roboflow-app.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949746649\"\n            width=\"3%\"\n          />\n      </a>\n      <img src=\"https://github.com/SkalskiP/SkalskiP/blob/master/icons/transparent.png\" width=\"3%\"/>\n      <a href=\"https://www.linkedin.com/company/roboflow-ai/\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/linkedin.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949633691\"\n            width=\"3%\"\n          />\n      </a>\n      <img src=\"https://github.com/SkalskiP/SkalskiP/blob/master/icons/transparent.png\" width=\"3%\"/>\n      <a href=\"https://docs.roboflow.com\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/knowledge.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949634511\"\n            width=\"3%\"\n          />\n      </a>\n      <img src=\"https://github.com/SkalskiP/SkalskiP/blob/master/icons/transparent.png\" width=\"3%\"/>\n      <a href=\"https://discuss.roboflow.com\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/forum.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949633584\"\n            width=\"3%\"\n          />\n      <img src=\"https://github.com/SkalskiP/SkalskiP/blob/master/icons/transparent.png\" width=\"3%\"/>\n      <a href=\"https://blog.roboflow.com\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/blog.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949633605\"\n            width=\"3%\"\n          />\n      </a>\n      </a>\n  </div>\n\n</div>\n\n## \ud83d\udc4b hello\n\nThis repository offers a growing collection of computer vision tutorials. Learn to use SOTA models like YOLOv11, SAM 2, Florence-2, PaliGemma 2, and Qwen2.5-VL for tasks ranging from object detection, segmentation, and pose estimation to data extraction and OCR. Dive in and explore the exciting world of computer vision!\n\n<!--- AUTOGENERATED-NOTEBOOKS-TABLE -->\n<!---\n   WARNING: DO NOT EDIT THIS TABLE MANUALLY. IT IS AUTOMATICALLY GENERATED.\n   HEAD OVER TO CONTRIBUTING.MD FOR MORE DETAILS ON HOW TO MAKE CHANGES PROPERLY.\n-->\n## \ud83d\ude80 model tutorials (48 notebooks)\n| **notebook** | **open in colab / kaggle / sagemaker studio lab** | **complementary materials** | **repository / paper** |\n|:------------:|:-------------------------------------------------:|:---------------------------:|:----------------------:|\n| [Fine-Tune YOLOv12 on Object Detection Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov12-object-detection-model.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov12-object-detection-model.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov12-object-detection-model.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/train-yolov12-model)  | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/sunsmarterjie/yolov12) [![arXiv](https://img.shields.io/badge/arXiv-2502.12524-b31b1b.svg)](https://arxiv.org/abs/2502.12524)|\n| [Zero-Shot Object Detection with Qwen2.5-VL](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-qwen2-5-vl.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-qwen2-5-vl.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-qwen2-5-vl.ipynb)  |  [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=xEfh0IR8Fvo) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/QwenLM/Qwen2.5-VL) [![arXiv](https://img.shields.io/badge/arXiv-2502.13923-b31b1b.svg)](https://arxiv.org/abs/2502.13923)|\n| [Fine-Tune Qwen2.5-VL for JSON Data Extraction](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-qwen2-5-vl-for-json-data-extraction.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-qwen2-5-vl-for-json-data-extraction.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-qwen2-5-vl-for-json-data-extraction.ipynb)  |  [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=xEfh0IR8Fvo) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/QwenLM/Qwen2.5-VL) [![arXiv](https://img.shields.io/badge/arXiv-2502.13923-b31b1b.svg)](https://arxiv.org/abs/2502.13923)|\n| [Fine-Tune PaliGemma2 on Object Detection Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-paligemma2-on-detection-dataset.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-paligemma2-on-detection-dataset.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-paligemma2-on-detection-dataset.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/fine-tune-paligemma-2/)  | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/README.md) [![arXiv](https://img.shields.io/badge/arXiv-2412.03555-b31b1b.svg)](https://arxiv.org/abs/2412.03555)|\n| [Fine-Tune PaliGemma2 for JSON Data Extraction](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-paligemma2-for-json-data-extraction.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-paligemma2-for-json-data-extraction.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-paligemma2-for-json-data-extraction.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/fine-tune-paligemma-2/)  | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/README.md) [![arXiv](https://img.shields.io/badge/arXiv-2412.03555-b31b1b.svg)](https://arxiv.org/abs/2412.03555)|\n| [Fine-Tune PaliGemma2 for LaTeX OCR](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-paligemma2-on-latex-ocr-dataset.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-paligemma2-on-latex-ocr-dataset.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-paligemma2-on-latex-ocr-dataset.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/fine-tune-paligemma-2/)  | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/README.md) [![arXiv](https://img.shields.io/badge/arXiv-2412.03555-b31b1b.svg)](https://arxiv.org/abs/2412.03555)|\n| [Fine-Tune SAM-2.1](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/fine-tune-sam-2.1.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/fine-tune-sam-2.1.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/fine-tune-sam-2.1.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/fine-tune-sam-2-1/) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=QnCGcFHZy9s) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/facebookresearch/sam2) |\n| [Fine-Tune GPT-4o on Object Detection Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/openai-gpt-4o-fine-tuning.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/openai-gpt-4o-fine-tuning.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/openai-gpt-4o-fine-tuning.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/gpt-4o-object-detection/) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=6Q6TieCBA4E) |  |\n| [Fine-Tune YOLO11 on Object Detection Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolo11-object-detection-on-custom-dataset.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolo11-object-detection-on-custom-dataset.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolo11-object-detection-on-custom-dataset.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/yolov11-how-to-train-custom-data/) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=jE_s4tVgPHA) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/ultralytics/ultralytics) |\n| [Fine-Tune YOLO11 on Instance Segmentation Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolo11-instance-segmentation-on-custom-dataset.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolo11-instance-segmentation-on-custom-dataset.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolo11-instance-segmentation-on-custom-dataset.ipynb)  |  [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=jE_s4tVgPHA) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/ultralytics/ultralytics) |\n| [Segment Images with SAM2](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-images-with-sam-2.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-images-with-sam-2.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-images-with-sam-2.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/what-is-segment-anything-2/) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/live/Dv003fTyO-Y) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/facebookresearch/segment-anything-2) [![arXiv](https://img.shields.io/badge/arXiv-2408.00714-b31b1b.svg)](https://arxiv.org/abs/2408.00714)|\n| [Segment Videos with SAM2](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-videos-with-sam-2.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-videos-with-sam-2.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-videos-with-sam-2.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/what-is-segment-anything-2/) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/live/Dv003fTyO-Y) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/facebookresearch/segment-anything-2) [![arXiv](https://img.shields.io/badge/arXiv-2408.00714-b31b1b.svg)](https://arxiv.org/abs/2408.00714)|\n| [Fine-Tune RT-DETR on Object Detection Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-rt-detr-on-custom-dataset-with-transformers.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-rt-detr-on-custom-dataset-with-transformers.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-rt-detr-on-custom-dataset-with-transformers.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/train-rt-detr-custom-dataset-transformers/)  | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/lyuwenyu/RT-DETR) [![arXiv](https://img.shields.io/badge/arXiv-2304.08069-b31b1b.svg)](https://arxiv.org/abs/2304.08069)|\n| [Fine-Tune Florence-2 on Object Detection Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-florence-2-on-detection-dataset.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-florence-2-on-detection-dataset.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-florence-2-on-detection-dataset.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/fine-tune-florence-2-object-detection/) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=i3KjYgxNH6w) |  [![arXiv](https://img.shields.io/badge/arXiv-2311.06242-b31b1b.svg)](https://arxiv.org/abs/2311.06242)|\n| [Run Different Vision Tasks with Florence-2](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-run-different-vision-tasks-with-florence-2.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-run-different-vision-tasks-with-florence-2.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-run-different-vision-tasks-with-florence-2.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/florence-2/) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=hj_ybcRdk5Y) |  [![arXiv](https://img.shields.io/badge/arXiv-2311.06242-b31b1b.svg)](https://arxiv.org/abs/2311.06242)|\n| [Fine-Tune PaliGemma on Object Detection Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-paligemma-on-detection-dataset.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-paligemma-on-detection-dataset.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-paligemma-on-detection-dataset.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/how-to-fine-tune-paligemma/) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=OMBmVInx68M) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/README.md) [![arXiv](https://img.shields.io/badge/arXiv-2407.07726-b31b1b.svg)](https://arxiv.org/abs/2407.07726)|\n| [Fine-Tune YOLOv10 on Object Detection Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov10-object-detection-on-custom-dataset.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov10-object-detection-on-custom-dataset.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov10-object-detection-on-custom-dataset.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/yolov10-how-to-train/)  | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/THU-MIG/yolov10) [![arXiv](https://img.shields.io/badge/arXiv-2405.14458-b31b1b.svg)](https://arxiv.org/abs/2405.14458)|\n| [Zero-Shot Object Detection with YOLO-World](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-yolo-world.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-yolo-world.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-yolo-world.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/what-is-yolo-world/) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=X7gKBGVz4vs) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/AILab-CVC/YOLO-World) [![arXiv](https://img.shields.io/badge/arXiv-2401.17270-b31b1b.svg)](https://arxiv.org/abs/2401.17270)|\n| [Fine-Tune YOLOv9 on Object Detection Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov9-object-detection-on-custom-dataset.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov9-object-detection-on-custom-dataset.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov9-object-detection-on-custom-dataset.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/train-yolov9-model) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/XHT2c8jT3Bc) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/WongKinYiu/yolov9) [![arXiv](https://img.shields.io/badge/arXiv-2402.13616-b31b1b.svg)](https://arxiv.org/abs/2402.13616)|\n| [Fine-Tune RTMDet on Object Detection Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-rtmdet-object-detection-on-custom-data.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-rtmdet-object-detection-on-custom-data.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-rtmdet-object-detection-on-custom-data.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/how-to-train-rtmdet-on-a-custom-dataset) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/5kgWyo6Sg4E) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/open-mmlab/mmdetection) [![arXiv](https://img.shields.io/badge/arXiv-2212.07784-b31b1b.svg)](https://arxiv.org/abs/2212.07784)|\n| [Segment Images with FastSAM](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-fast-sam.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-fast-sam.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-fast-sam.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/how-to-use-fastsam) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/yHNPyqazYYU) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/CASIA-IVA-Lab/FastSAM) [![arXiv](https://img.shields.io/badge/arXiv-2306.12156-b31b1b.svg)](https://arxiv.org/abs/2306.12156)|\n| [Fine-Tune YOLO-NAS on Object Detection Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolo-nas-on-custom-dataset.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolo-nas-on-custom-dataset.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolo-nas-on-custom-dataset.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/yolo-nas-how-to-train-on-custom-dataset) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/V-H3eoPUnA8) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/Deci-AI/super-gradients/blob/master/YOLONAS.md) |\n| [Segment Images with Segment Anything Model (SAM)](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-sam.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-sam.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-sam.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/how-to-use-segment-anything-model-sam) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/D-D6ZmadzPE) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/facebookresearch/segment-anything) [![arXiv](https://img.shields.io/badge/arXiv-2304.02643-b31b1b.svg)](https://arxiv.org/abs/2304.02643)|\n| [Zero-Shot Object Detection with Grounding DINO](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/grounding-dino-zero-shot-object-detection) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/cMa77r3YrDk) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/IDEA-Research/GroundingDINO) [![arXiv](https://img.shields.io/badge/arXiv-2303.05499-b31b1b.svg)](https://arxiv.org/abs/2303.05499)|\n| [Fine-Tune DETR Transformer on Object Detection Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-huggingface-detr-on-custom-dataset.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-huggingface-detr-on-custom-dataset.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-huggingface-detr-on-custom-dataset.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://youtu.be/AM8D4j9KoaU) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/AM8D4j9KoaU) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/facebookresearch/detr) [![arXiv](https://img.shields.io/badge/arXiv-2005.12872-b31b1b.svg)](https://arxiv.org/abs/2005.12872)|\n| [Classify Images with DINOv2](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/dinov2-classification.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/dinov2-classification.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/dinov2-classification.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/how-to-classify-images-with-dinov2/)  | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/facebookresearch/dinov2/) [![arXiv](https://img.shields.io/badge/arXiv-2304.07193-b31b1b.svg)](https://arxiv.org/abs/2304.07193)|\n| [Fine-Tune YOLOv8 on Object Detection Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-object-detection-on-custom-dataset.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-object-detection-on-custom-dataset.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-object-detection-on-custom-dataset.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/how-to-train-yolov8-on-a-custom-dataset) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/wuZtUMEiKWY) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/ultralytics/ultralytics) |\n| [Fine-Tune YOLOv8 on Pose Estimation Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-keypoint.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-keypoint.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-keypoint.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/train-a-custom-yolov8-pose-estimation-model/)  | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/ultralytics/ultralytics) |\n| [Fine-Tune YOLOv8 on Oriented Bounding Boxes (OBB) Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-obb.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-obb.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-obb.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https:/blog.roboflow.com/train-yolov8-obb-model/)  | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/ultralytics/ultralytics) |\n| [Fine-Tune YOLOv8 on Instance Segmentation Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-instance-segmentation-on-custom-dataset.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-instance-segmentation-on-custom-dataset.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-instance-segmentation-on-custom-dataset.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/how-to-train-yolov8-instance-segmentation/) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/pFiGSrRtaU4) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/ultralytics/ultralytics) |\n| [Fine-Tune YOLOv8 on Classification Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-classification-on-custom-dataset.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-classification-on-custom-dataset.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-classification-on-custom-dataset.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/how-to-train-a-yolov8-classification-model/)  | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/ultralytics/ultralytics) |\n| [Fine-Tune YOLOv7 on Object Detection Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov7-object-detection-on-custom-data.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov7-object-detection-on-custom-data.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov7-object-detection-on-custom-data.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/yolov7-custom-dataset-training-tutorial) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=5nsmXLyDaU4) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/WongKinYiu/yolov7) [![arXiv](https://img.shields.io/badge/arXiv-2207.02696-b31b1b.svg)](https://arxiv.org/abs/2207.02696)|\n| [Fine-Tune YOLOv7 on Instance Segmentation Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov7-instance-segmentation-on-custom-data.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov7-instance-segmentation-on-custom-data.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov7-instance-segmentation-on-custom-data.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/train-yolov7-instance-segmentation-on-custom-data) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=vFGxM2KLs10) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/WongKinYiu/yolov7) [![arXiv](https://img.shields.io/badge/arXiv-2207.02696-b31b1b.svg)](https://arxiv.org/abs/2207.02696)|\n| [Fine-Tune MT-YOLOv6 on Object Detection Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov6-object-detection-on-custom-data.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov6-object-detection-on-custom-data.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov6-object-detection-on-custom-data.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/how-to-train-yolov6-on-a-custom-dataset) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=fFCWrMFH2UY) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/meituan/YOLOv6) [![arXiv](https://img.shields.io/badge/arXiv-2209.02976-b31b1b.svg)](https://arxiv.org/abs/2209.02976)|\n| [Fine-Tune YOLOv5 on Object Detection Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov5-object-detection-on-custom-data.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov5-object-detection-on-custom-data.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov5-object-detection-on-custom-data.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/watch?v=x0ThXHbtqCQ) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/ultralytics/yolov5) |\n| [Fine-Tune YOLOv5 on Classification Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov5-classification-on-custom-data.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov5-classification-on-custom-data.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov5-classification-on-custom-data.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/train-yolov5-classification-custom-data) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=DPjp9Kq4qn8) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/ultralytics/yolov5) |\n| [Fine-Tune YOLOv5 on Instance Segmentation Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov5-instance-segmentation-on-custom-data.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov5-instance-segmentation-on-custom-data.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov5-instance-segmentation-on-custom-data.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/train-yolov5-instance-segmentation-custom-dataset) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=vKzfvtEtiYo) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/ultralytics/yolov5) |\n| [Fine-Tune Faster RCNN on Instance Segmentation Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-detectron2-segmentation-on-custom-data.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-detectron2-segmentation-on-custom-data.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-detectron2-segmentation-on-custom-data.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/how-to-train-detectron2) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/e8LPflX0nwQ) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/facebookresearch/detectron2) [![arXiv](https://img.shields.io/badge/arXiv-1703.06870v3-b31b1b.svg)](https://arxiv.org/abs/1703.06870v3)|\n| [Fine-Tune SegFormer on Instance Segmentation Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-segformer-segmentation-on-custom-data.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-segformer-segmentation-on-custom-data.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-segformer-segmentation-on-custom-data.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/how-to-train-segformer-on-a-custom-dataset-with-pytorch-lightning) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=4HNkBMfw-2o) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/NVlabs/SegFormer) [![arXiv](https://img.shields.io/badge/arXiv-2105.15203v3-b31b1b.svg)](https://arxiv.org/abs/2105.15203v3)|\n| [Fine-Tune ViT on Classification Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-vision-transformer-classification-on-custom-data.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-vision-transformer-classification-on-custom-data.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-vision-transformer-classification-on-custom-data.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/how-to-train-vision-transformer) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=8yRE2Pa-8_I) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/lucidrains/vit-pytorch) [![arXiv](https://img.shields.io/badge/arXiv-2010.11929-b31b1b.svg)](https://arxiv.org/abs/2010.11929)|\n| [Fine-Tune Scaled-YOLOv4 on Object Detection Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-scaled-yolov4-object-detection-on-custom-data.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-scaled-yolov4-object-detection-on-custom-data.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-scaled-yolov4-object-detection-on-custom-data.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/how-to-train-scaled-yolov4) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=rEbpKxZbvIo) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/WongKinYiu/ScaledYOLOv4) [![arXiv](https://img.shields.io/badge/arXiv-2004.10934-b31b1b.svg)](https://arxiv.org/abs/2004.10934)|\n| [Fine-Tune YOLOS on Object Detection Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolos-huggingface-object-detection-on-custom-data.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolos-huggingface-object-detection-on-custom-data.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolos-huggingface-object-detection-on-custom-data.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/train-yolos-transformer-custom-dataset) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=N0V0xxSi6Xc) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/huggingface/transformers) [![arXiv](https://img.shields.io/badge/arXiv-2106.00666-b31b1b.svg)](https://arxiv.org/abs/2106.00666)|\n| [Fine-Tune YOLOR on Object Detection Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolor-object-detection-on-custom-data.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolor-object-detection-on-custom-data.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolor-object-detection-on-custom-data.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/train-yolor-on-a-custom-dataset) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=sZ5DiXDOHEM) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/roboflow-ai/yolor) [![arXiv](https://img.shields.io/badge/arXiv-1506.02640-b31b1b.svg)](https://arxiv.org/abs/1506.02640)|\n| [Fine-Tune YOLOX on Object Detection Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolox-object-detection-on-custom-data.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolox-object-detection-on-custom-data.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolox-object-detection-on-custom-data.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/how-to-train-yolox-on-a-custom-dataset) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=q3RbFbaQQGw) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/Megvii-BaseDetection/YOLOX) [![arXiv](https://img.shields.io/badge/arXiv-2107.08430-b31b1b.svg)](https://arxiv.org/abs/2107.08430)|\n| [Fine-Tune ResNet34 on Classification Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-resnet34-classification.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-resnet34-classification.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-resnet34-classification.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/how-to-train-a-custom-resnet34-model) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=93kXzUOiYY4) |  |\n| [Image Classification with OpenAI Clip](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-use-openai-clip-classification.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-use-openai-clip-classification.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-use-openai-clip-classification.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/how-to-use-openai-clip) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=8o701AEoZ8I) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/openai/CLIP) [![arXiv](https://img.shields.io/badge/arXiv-2103.00020-b31b1b.svg)](https://arxiv.org/abs/2103.00020)|\n| [Fine-Tune YOLOv4-tiny Darknet on Object Detection Dataset](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov4-tiny-object-detection-on-custom-data.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov4-tiny-object-detection-on-custom-data.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov4-tiny-object-detection-on-custom-data.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.ai/train-yolov4-tiny-on-custom-data-lighting-fast-detection) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=NTnZgLsk_DA) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/roboflow-ai/darknet) [![arXiv](https://img.shields.io/badge/arXiv-2011.04244-b31b1b.svg)](https://arxiv.org/abs/2011.04244)|\n| [Train a YOLOv8 Classification Model with No Labeling](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-train-yolov8-classification-no-labeling.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-train-yolov8-classification-no-labeling.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-train-yolov8-classification-no-labeling.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/train-classification-model-no-labeling/)  | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/ultralytics/ultralytics) |\n## \ud83d\udcf8 computer vision skills (21 notebooks)\n| **notebook** | **open in colab / kaggle / sagemaker studio lab** | **complementary materials** | **repository / paper** |\n|:------------:|:-------------------------------------------------:|:---------------------------:|:----------------------:|\n| [Football AI](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/football-ai.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/football-ai.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/football-ai.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/camera-calibration-sports-computer-vision/) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/aBVGKoNZQUw) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/roboflow/sports) |\n| [Auto-Annotate Dataset with GroundedSAM 2](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/grounded-sam-2-auto-label.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/grounded-sam-2-auto-label.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/grounded-sam-2-auto-label.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/what-is-segment-anything-2)  | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/facebookresearch/segment-anything-2) |\n| [Run YOLOv7 Object Detection with OpenVINO + TorchORT](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov7-object-detection-on-custom-data-openvino-torch-ort.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov7-object-detection-on-custom-data-openvino-torch-ort.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov7-object-detection-on-custom-data-openvino-torch-ort.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/accelerate-pytorch-openvino-torch-ort)  | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/roboflow-ai/yolov7) [![arXiv](https://img.shields.io/badge/arXiv-2207.02696-b31b1b.svg)](https://arxiv.org/abs/2207.02696)|\n| [Estimate Vehicle Speed with YOLOv8](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-estimate-vehicle-speed-with-computer-vision.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-estimate-vehicle-speed-with-computer-vision.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-estimate-vehicle-speed-with-computer-vision.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/estimate-speed-computer-vision/) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/uWP6UjDeZvY) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/roboflow/supervision/tree/develop/examples/speed_estimation) |\n| [Detect and Count Objects in Polygon Zone with YOLOv5 / YOLOv8 / Detectron2 + Supervision](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-detect-and-count-objects-in-polygon-zone.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-detect-and-count-objects-in-polygon-zone.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-detect-and-count-objects-in-polygon-zone.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/how-to-count-objects-in-a-zone) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/l_kf9CfZ_8M) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/roboflow/supervision) |\n| [Track and Count Vehicles with YOLOv8 + ByteTRACK + Supervision](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-track-and-count-vehicles-with-yolov8.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-track-and-count-vehicles-with-yolov8.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-track-and-count-vehicles-with-yolov8.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/yolov8-tracking-and-counting/) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/OS5qI9YBkfk) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/roboflow/supervision) [![arXiv](https://img.shields.io/badge/arXiv-2110.06864-b31b1b.svg)](https://arxiv.org/abs/2110.06864)|\n| [Football Players Tracking with YOLOv5 + ByteTRACK](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-track-football-players.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-track-football-players.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-track-football-players.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/track-football-players) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/QCG8QMhga9k) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/ifzhang/ByteTrack) [![arXiv](https://img.shields.io/badge/arXiv-2110.06864-b31b1b.svg)](https://arxiv.org/abs/2110.06864)|\n| [Auto Train YOLOv8 Model with Autodistill](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-auto-train-yolov8-model-with-autodistill.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-auto-train-yolov8-model-with-autodistill.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-auto-train-yolov8-model-with-autodistill.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/autodistill) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/gKTYMfwPo4M) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/autodistill/autodistill) |\n| [Image Embeddings Analysis - Part 1](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/image_embeddings_analysis_part_1.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/image_embeddings_analysis_part_1.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/image_embeddings_analysis_part_1.ipynb)  |  [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/YxJkE6FvGF4) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/openai/CLIP) [![arXiv](https://img.shields.io/badge/arXiv-2103.00020-b31b1b.svg)](https://arxiv.org/abs/2103.00020)|\n| [Automated Dataset Annotation and Evaluation with Grounding DINO and SAM](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino-and-sam.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino-and-sam.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino-and-sam.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/enhance-image-annotation-with-grounding-dino-and-sam/) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/oEQYStnF2l8) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/IDEA-Research/GroundingDINO) [![arXiv](https://img.shields.io/badge/arXiv-2303.05499-b31b1b.svg)](https://arxiv.org/abs/2303.05499)|\n| [Automated Dataset Annotation and Evaluation with Grounding DINO](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino.ipynb)  |  [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/C4NqaRBz_Kw) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/IDEA-Research/GroundingDINO) [![arXiv](https://img.shields.io/badge/arXiv-2303.05499-b31b1b.svg)](https://arxiv.org/abs/2303.05499)|\n| [Roboflow Video Inference with Custom Annotators](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/roboflow_video_inference_with_custom_annotators.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/roboflow_video_inference_with_custom_annotators.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/roboflow_video_inference_with_custom_annotators.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/custom-annotator-video-inference)  | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/roboflow/inference) |\n| [DINO-GPT-4V Object Detection](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/dino-gpt4v-autodistill.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/dino-gpt4v-autodistill.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/dino-gpt4v-autodistill.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/dino-gpt-4v/)  |  |\n| [Train a Segmentation Model with No Labeling](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-segmentation-model-with-no-labeling.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-segmentation-model-with-no-labeling.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-segmentation-model-with-no-labeling.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/train-a-segmentation-model-no-labeling/)  | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/autodistill/autodistill) |\n| [DINOv2 Image Retrieval](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/dinov2-image-retrieval.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/dinov2-image-retrieval.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/dinov2-image-retrieval.ipynb)  |   | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/facebookresearch/dinov2/) [![arXiv](https://img.shields.io/badge/arXiv-2304.07193-b31b1b.svg)](https://arxiv.org/abs/2304.07193)|\n| [Vector Analysis with Scikit-learn and Bokeh](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/vector-analysis-with-sklearn-and-bokeh.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/vector-analysis-with-sklearn-and-bokeh.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/vector-analysis-with-sklearn-and-bokeh.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/vector-analysis)  |  |\n| [RF100 Object Detection Model Benchmarking](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-use-rf100.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-use-rf100.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-use-rf100.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/roboflow-100) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/jIgZMr-PBMo) | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/roboflow-ai/roboflow-100-benchmark) [![arXiv](https://img.shields.io/badge/arXiv-2211.13523-b31b1b.svg)](https://arxiv.org/abs/2211.13523)|\n| [Create Segmentation Masks with Roboflow](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-generate-segmentation-mask-with-roboflow.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-generate-segmentation-mask-with-roboflow.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-generate-segmentation-mask-with-roboflow.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/how-to-create-segmentation-masks-with-roboflow)  |  |\n| [How to Use PolygonZone and Roboflow Supervision](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-use-polygonzone-annotate-and-supervision.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-use-polygonzone-annotate-and-supervision.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/how-to-use-polygonzone-annotate-and-supervision.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/polygonzone/)  |  |\n| [Train a Package Detector With Two Labeled Images](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-package-detector-two-labeled-images.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-package-detector-two-labeled-images.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-package-detector-two-labeled-images.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/package-detector/)  | [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/autodistill/autodistill-seggpt) |\n| [Image-to-Image Search with CLIP and faiss](https://github.com/roboflow-ai/notebooks/blob/main/notebooks/image-to-image-search-clip-faiss.ipynb) | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/image-to-image-search-clip-faiss.ipynb) [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/blob/main/notebooks/image-to-image-search-clip-faiss.ipynb)  | [![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/clip-image-search-faiss/)  |  |\n<!--- AUTOGENERATED-NOTEBOOKS-TABLE -->\n\n## \ud83c\udfac videos\n\nAlmost every week we create tutorials showing you the hottest models in Computer Vision. \ud83d\udd25\n[Subscribe](https://www.youtube.com/@Roboflow), and stay up to date with our latest YouTube videos!\n\n<p align=\"left\">\n<a href=\"https://youtu.be/CilXrt3S-ws\" title=\"How to Choose the Best Computer Vision Model for Your Project\"><img src=\"https://github.com/roboflow/notebooks/assets/26109316/73a01d3b-cf70-40c3-a5e4-e4bc5be38d42\" alt=\"How to Choose the Best Computer Vision Model for Your Project\" width=\"300px\" align=\"left\" /></a>\n<a href=\"https://youtu.be/CilXrt3S-ws\" title=\"How to Choose the Best Computer Vision Model for Your Project\"><strong>How to Choose the Best Computer Vision Model for Your Project</strong></a>\n<div><strong>Created: 26 May 2023</strong> | <strong>Updated: 26 May 2023</strong></div>\n<br/> In this video, we will dive into the complexity of choosing the right computer vision model for your unique project. From the importance of high-quality datasets to hardware considerations, interoperability, benchmarking, and licensing issues, this video covers it all... </p> <br/>\n\n<p align=\"left\">\n<a href=\"https://youtu.be/oEQYStnF2l8\" title=\"Accelerate Image Annotation with SAM and Grounding DINO\"><img src=\"https://github.com/SkalskiP/SkalskiP/assets/26109316/ae1ca38e-40b7-4b35-8582-e8ea5de3806e\" alt=\"Accelerate Image Annotation with SAM and Grounding DINO\" width=\"300px\" align=\"left\" /></a>\n<a href=\"https://youtu.be/oEQYStnF2l8\" title=\"Accelerate Image Annotation with SAM and Grounding DINO\"><strong>Accelerate Image Annotation with SAM and Grounding DINO</strong></a>\n<div><strong>Created: 20 Apr 2023</strong> | <strong>Updated: 20 Apr 2023</strong></div>\n<br/> Discover how to speed up your image annotation process using Grounding DINO and Segment Anything Model (SAM). Learn how to convert object detection datasets into instance segmentation datasets, and see the potential of using these models to automatically annotate your datasets for real-time detectors like YOLOv8... </p> <br/>\n<p align=\"left\">\n<a href=\"https://youtu.be/D-D6ZmadzPE\" title=\"SAM - Segment Anything Model by Meta AI: Complete Guide\"><img src=\"https://github.com/SkalskiP/SkalskiP/assets/26109316/6913ff11-53c6-4341-8d90-eaff3023c3fd\" alt=\"SAM - Segment Anything Model by Meta AI: Complete Guide\" width=\"300px\" align=\"left\" /></a>\n<a href=\"https://youtu.be/D-D6ZmadzPE\" title=\"SAM - Segment Anything Model by Meta AI: Complete Guide\"><strong>SAM - Segment Anything Model by Meta AI: Complete Guide</strong></a>\n<div><strong>Created: 11 Apr 2023</strong> | <strong>Updated: 11 Apr 2023</strong></div>\n\n<br/> Discover the incredible potential of Meta AI's Segment Anything Model (SAM)! We dive into SAM, an efficient and promptable model for image segmentation, which has revolutionized computer vision tasks. With over 1 billion masks on 11M licensed and privacy-respecting images, SAM's zero-shot performance is often superior to prior fully supervised results... </p>\n\n## \ud83d\udcbb run locally\n\nWe try to make it as easy as possible to run Roboflow Notebooks in Colab and Kaggle, but if you still want to run them\nlocally, below you will find instructions on how to do it. Remember don't install your dependencies globally, use\n[venv](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/).\n\n```console\n# clone repository and navigate to root directory\ngit clone git@github.com:roboflow-ai/notebooks.git\ncd notebooks\n\n# setup python environment and activate it\npython3 -m venv venv\nsource venv/bin/activate\n\n# install and run jupyter notebook\npip install notebook\njupyter notebook\n```\n\n## \u2601\ufe0f run in sagemaker studio lab\n\nYou can now open our tutorial notebooks in [Amazon SageMaker Studio Lab](https://aws.amazon.com/sagemaker/studio-lab/) -\na free machine learning development environment that provides the compute, storage, and security\u2014all at no cost\u2014for\nanyone to learn and experiment with ML.\n\n| Stable Diffusion Image Generation | YOLOv5 Custom Dataset Training | YOLOv7 Custom Dataset Training |\n|:---------------------------------:|:------------------------------:|:------------------------------:|\n|  [![SageMaker](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg)](https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/sagemaker-studiolab/stable-diffusion-image-generation.ipynb) | [![SageMaker](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg)](https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/sagemaker-studiolab/yolov5-custom-training.ipynb)       |[![SageMaker](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg)](https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/sagemaker-studiolab/yolov7-custom-training.ipynb)       |\n\n\n## \ud83d\udc1e bugs & \ud83e\uddb8 contribution\n\nComputer Vision moves fast! Sometimes our notebooks lag a tad behind the ever-pushing\nforward libraries. If you notice that any of the notebooks is not working properly, create a\n[bug report](https://github.com/roboflow-ai/notebooks/issues/new?assignees=&labels=bug%2Ctriage&template=bug-report.yml)\nand let us know.\n\nIf you have an idea for a new tutorial we should do, create a\n[feature request](https://github.com/roboflow-ai/notebooks/issues/new?assignees=&labels=enhancement&template=feature-request.yml).\nWe are constantly looking for new ideas. If you feel up to the task and want to create a tutorial yourself, please take\na peek at our [contribution guide](https://github.com/roboflow-ai/notebooks/blob/main/CONTRIBUTING.md). There you can\nfind all the information you need.\n\nWe are here for you, so don't hesitate to [reach out](https://github.com/roboflow-ai/notebooks/discussions).\n"
  },
  {
    "name": "inference",
    "url": "https://github.com/davidatoms/inference",
    "description": "Turn any computer or edge device into a command center for your computer vision projects.",
    "type": "fork",
    "updated_at": "2025-02-23T03:11:22Z",
    "readme": "<div align=\"center\">\n  <p>\n    <a align=\"center\" href=\"\" target=\"https://inference.roboflow.com/\">\n      <img\n        width=\"100%\"\n        src=\"https://github.com/roboflow/inference/blob/main/banner.png?raw=true\"\n      >\n    </a>\n  </p>\n\n  <br>\n\n[notebooks](https://github.com/roboflow/notebooks) | [supervision](https://github.com/roboflow/supervision) | [autodistill](https://github.com/autodistill/autodistill) | [maestro](https://github.com/roboflow/multimodal-maestro)\n\n  <br>\n\n[![version](https://badge.fury.io/py/inference.svg)](https://badge.fury.io/py/inference)\n[![downloads](https://img.shields.io/pypi/dm/inference)](https://pypistats.org/packages/inference)\n[![docker pulls](https://img.shields.io/docker/pulls/roboflow/roboflow-inference-server-cpu)](https://hub.docker.com/u/roboflow)\n[![license](https://img.shields.io/pypi/l/inference)](https://github.com/roboflow/inference/blob/main/LICENSE.core)\n\n<!-- [![huggingface](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/Roboflow/workflows) -->\n\n</div>\n\n## Make Any Camera an AI Camera\n\nInference turns any computer or edge device into a command center for your computer vision projects.\n\n* \ud83d\udee0\ufe0f Self-host [your own fine-tuned models](https://inference.roboflow.com/quickstart/explore_models/)\n* \ud83e\udde0 Access the latest and greatest foundation models (like [Florence-2](https://blog.roboflow.com/florence-2/), [CLIP](https://blog.roboflow.com/openai-clip/), and [SAM2](https://blog.roboflow.com/what-is-segment-anything-2/))\n* \ud83e\udd1d Use [Workflows](https://inference.roboflow.com/workflows/about/) to track, count, time, measure, and visualize\n* \ud83d\udc41\ufe0f Combine ML with traditional CV methods (like OCR, Barcode Reading, QR, and template matching)\n* \ud83d\udcc8 Monitor, record, and analyze predictions\n* \ud83c\udfa5 [Manage](https://inference.roboflow.com/workflows/video_processing/overview/) cameras and video streams\n* \ud83d\udcec Send notifications when events happen\n* \ud83d\udedc Connect with external systems and APIs\n* \ud83d\udd17 [Extend](https://inference.roboflow.com/workflows/create_workflow_block/) with your own code and models\n* \ud83d\ude80 Deploy production systems at scale\n\nSee [Example Workflows](https://inference.roboflow.com/workflows/gallery/) for common use-cases like detecting small objects with SAHI, multi-model consensus, active learning, reading license plates, blurring faces, background removal, and more.\n\n[Time In Zone Workflow Example](https://github.com/user-attachments/assets/743233d9-3460-442d-83f8-20e29e76b346)\n\n## \ud83d\udd25 quickstart\n\n[Install Docker](https://docs.docker.com/engine/install/) (and\n[NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)\nfor GPU acceleration if you have a CUDA-enabled GPU). Then run\n\n```\npip install inference-cli && inference server start --dev\n```\n\nThis will pull the proper image for your machine and start it in development mode.\n\nIn development mode, a Jupyter notebook server with a quickstart guide runs on \n[`localhost:9002`](http://localhost:9002). Dive in there for a whirlwind tour\nof your new Inference Server's functionality!\n\nNow you're ready to connect your camera streams and\n[start building & deploying Workflows in the UI](https://app.roboflow.com/workflows)\nor [interacting with your new server](https://inference.roboflow.com/workflows/create_and_run/)\nvia its API.\n\n## \ud83d\udee0\ufe0f build with Workflows\n\nA key component of Inference is [Workflows](https://roboflow.com/workflows), composable blocks of common functionality that give models a common interface to make chaining and experimentation easy.\n\n![License Plate OCR Workflow Visualization](https://github.com/user-attachments/assets/178046a2-011e-489d-bfc2-41dcfefe44a4)\n\nWith Workflows, you can:\n* Detect, classify, and segment objects in images using state-of-the-art models.\n* Use Large Multimodal Models (LMMs) to make determinations at any stage in a workflow.\n* Seamlessly swap out models for a given task.\n* Chain models together.\n* Track, count, time, measure, and visualize objects.\n* Add business logic and extend functionality to work with your external systems.\n\nWorkflows allow you to extend simple model predictions to build computer vision micro-services that fit into a larger application or fully self-contained visual agents that run on a video stream.\n\n[Learn more](https://roboflow.com/workflows), read [the Workflows docs](https://inference.roboflow.com/workflows/about/), or [start building](https://app.roboflow.com/workflows).\n\n<table border=\"0\" cellspacing=\"0\" cellpadding=\"0\" role=\"presentation\">\n  <tr>\n    <!-- Left cell (thumbnail) -->\n    <td width=\"300\" valign=\"top\">\n      <a href=\"https://youtu.be/aPxlImNxj5A\">\n        <img src=\"https://img.youtube.com/vi/aPxlImNxj5A/0.jpg\" \n             alt=\"Self Checkout with Workflows\" width=\"300\" />\n      </a>\n    </td>\n    <!-- Right cell (title, date, description) -->\n    <td valign=\"middle\">\n      <strong>\n        <a href=\"https://youtu.be/aPxlImNxj5A\">Tutorial: Build an AI-Powered Self-Serve Checkout</a>\n      </strong><br />\n      <strong>Created: 2 Feb 2025</strong><br /><br />\n      Make a computer vision app that identifies different pieces of hardware, calculates\n      the total cost, and records the results to a database.\n    </td>\n  </tr>\n\n  <tr>\n    <td width=\"300\" valign=\"top\">\n      <a href=\"https://youtu.be/r3Ke7ZEh2Qo\">\n        <img src=\"https://img.youtube.com/vi/r3Ke7ZEh2Qo/0.jpg\" \n             alt=\"Workflows Tutorial\" width=\"300\" />\n      </a>\n    </td>\n    <td valign=\"middle\">\n      <strong>\n        <a href=\"https://youtu.be/r3Ke7ZEh2Qo\">\n          Tutorial: Intro to Workflows\n        </a>\n      </strong><br />\n      <strong>Created: 6 Jan 2025</strong><br /><br />\n      Learn how to build and deploy Workflows for common use-cases like detecting\n      vehicles, filtering detections, visualizing results, and calculating dwell \n      time on a live video stream.\n    </td>\n  </tr>\n\n  <tr>\n    <!-- Left cell (thumbnail) -->\n    <td width=\"300\" valign=\"top\">\n      <a href=\"https://youtu.be/tZa-QgFn7jg\">\n        <img src=\"https://img.youtube.com/vi/tZa-QgFn7jg/0.jpg\" \n             alt=\"Smart Parking with AI\" width=\"300\" />\n      </a>\n    </td>\n    <!-- Right cell (title, date, description) -->\n    <td valign=\"middle\">\n      <strong>\n        <a href=\"https://youtu.be/tZa-QgFn7jg\">Tutorial: Build a Smart Parking System</a>\n      </strong><br />\n      <strong>Created: 27 Nov 2024</strong><br /><br />\n      Build a smart parking lot management system using Roboflow Workflows!\n      This tutorial covers license plate detection with YOLOv8, object tracking\n      with ByteTrack, and real-time notifications with a Telegram bot.\n    </td>\n  </tr>\n</table>\n\n## \ud83d\udcdf connecting via api\n  \nOnce you've installed Inference, your machine is a fully-featured CV center.\nYou can use its API to run models and workflows on images and video streams.\nBy default, the server is running locally on\n[`localhost:9001`](http://localhost:9001).\n\nTo interface with your server via Python, use our SDK.\n`pip install inference-sdk` then run\n[an example model comparison Workflow](https://app.roboflow.com/workflows/embed/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ3b3JrZmxvd0lkIjoiSHhIODdZR0FGUWhaVmtOVWNEeVUiLCJ3b3Jrc3BhY2VJZCI6IlhySm9BRVFCQkFPc2ozMmpYZ0lPIiwidXNlcklkIjoiNXcyMFZ6UU9iVFhqSmhUanE2a2FkOXVicm0zMyIsImlhdCI6MTczNTIzNDA4Mn0.AA78pZnlivFs5pBPVX9cMigFAOIIMZk0dA4gxEF5tj4)\nlike this:\n\n```python\nfrom inference_sdk import InferenceHTTPClient\n\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\", # use local inference server\n    # api_key=\"<YOUR API KEY>\" # optional to access your private data and models\n)\n\nresult = client.run_workflow(\n    workspace_name=\"roboflow-docs\",\n    workflow_id=\"model-comparison\",\n    images={\n        \"image\": \"https://media.roboflow.com/workflows/examples/bleachers.jpg\"\n    },\n    parameters={\n        \"model1\": \"yolov8n-640\",\n        \"model2\": \"yolov11n-640\"\n    }\n)\n\nprint(result)\n```\n\nIn other languages, use the server's REST API;\nyou can access the API docs for your server at\n[`/docs` (OpenAPI format)](http://localhost:9001/docs) or\n[`/redoc` (Redoc Format)](http://localhost:9001/redoc).\n\nCheck out [the inference_sdk docs](https://inference.roboflow.com/inference_helpers/inference_sdk/)\nto see what else you can do with your new server.\n\n## \ud83c\udfa5 connect to video streams\n\nThe inference server is a video processing beast. You can set it up to run\nWorkflows on RTSP streams, webcam devices, and more. It will handle hardware\nacceleration, multiprocessing, video decoding and GPU batching to get the\nmost out of your hardware.\n\n[This example workflow](https://app.roboflow.com/workflows/embed/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ3b3JrZmxvd0lkIjoiNHMzSDAzcmtyU0JiSDhFMjEzZUUiLCJ3b3Jrc3BhY2VJZCI6IlhySm9BRVFCQkFPc2ozMmpYZ0lPIiwidXNlcklkIjoiNXcyMFZ6UU9iVFhqSmhUanE2a2FkOXVicm0zMyIsImlhdCI6MTczNTIzOTk3NX0.TYdmD5AS8tbpz8AxEr5xW-05LlegK61kq-5_OReIrwc?showGraph=true&hideToolbar=false)\nwill watch a stream for frames that\n[CLIP thinks](https://blog.roboflow.com/openai-clip/) match an\ninputted text prompt.\n```python\nfrom inference_sdk import InferenceHTTPClient\nimport atexit\nimport time\n\nmax_fps = 4\n\nclient = InferenceHTTPClient(\n    api_url=\"http://localhost:9001\", # use local inference server\n    # api_key=\"<YOUR API KEY>\" # optional to access your private data and models\n)\n\n# Start a stream on an rtsp stream\nresult = client.start_inference_pipeline_with_workflow(\n    video_reference=[\"rtsp://user:password@192.168.0.100:554/\"],\n    workspace_name=\"roboflow-docs\",\n    workflow_id=\"clip-frames\",\n    max_fps=max_fps,\n    workflows_parameters={\n        \"prompt\": \"blurry\", # change to look for something else\n        \"threshold\": 0.16\n    }\n)\n\npipeline_id = result[\"context\"][\"pipeline_id\"]\n\n# Terminate the pipeline when the script exits\natexit.register(lambda: client.terminate_inference_pipeline(pipeline_id))\n\nwhile True:\n  result = client.consume_inference_pipeline_result(pipeline_id=pipeline_id)\n\n  if not result[\"outputs\"] or not result[\"outputs\"][0]:\n    # still initializing\n    continue\n\n  output = result[\"outputs\"][0]\n  is_match = output.get(\"is_match\")\n  similarity = round(output.get(\"similarity\")*100, 1)\n  print(f\"Matches prompt? {is_match} (similarity: {similarity}%)\")\n\n  time.sleep(1/max_fps)\n```\n\nPipeline outputs can be consumed via API for downstream processing or the\nWorkflow can be configured to call external services with Notification blocks\n(like [Email](https://inference.roboflow.com/workflows/blocks/email_notification/)\nor [Twilio](https://inference.roboflow.com/workflows/blocks/twilio_sms_notification/))\nor the [Webhook block](https://inference.roboflow.com/workflows/blocks/webhook_sink/).\nFor more info on video pipeline management, see the\n[Video Processing overview](https://inference.roboflow.com/workflows/video_processing/overview/).\n\nIf you have a Roboflow account & have linked an API key, you can also remotely\n[monitor and manage your running streams](https://app.roboflow.com/devices)\nvia the Roboflow UI.\n\n## \ud83d\udd11 connect to the cloud\n\nWithout an API Key, you can access a wide range of pre-trained and foundational models and run public Workflows.\n\nPass an optional [Roboflow API Key](https://app.roboflow.com/settings/api) to the `inference_sdk` or API to access additional features enhanced by Roboflow's Cloud\nplatform. When running with an API Key, usage is metered according to\nRoboflow's [pricing tiers](https://roboflow.com/pricing).\n\n|                         | Open Access | With API Key (Metered) |\n|-------------------------|-------------|--------------|\n| [Pre-Trained Models](https://inference.roboflow.com/quickstart/aliases/#supported-pre-trained-models) | \u2705 | \u2705\n| [Foundation Models](https://inference.roboflow.com/foundation/about/) | \u2705 | \u2705\n| [Video Stream Management](https://inference.roboflow.com/workflows/video_processing/overview/) | \u2705 | \u2705\n| [Dynamic Python Blocks](https://inference.roboflow.com/workflows/custom_python_code_blocks/) | \u2705 | \u2705\n| [Public Workflows](https://inference.roboflow.com/workflows/about/) | \u2705 | \u2705\n| [Private Workflows](https://docs.roboflow.com/workflows/create-a-workflow) |  | \u2705\n| [Fine-Tuned Models](https://roboflow.com/train) |  | \u2705\n| [Universe Models](https://roboflow.com/universe) |  | \u2705\n| [Active Learning](https://inference.roboflow.com/workflows/blocks/roboflow_dataset_upload/) |  | \u2705\n| [Serverless Hosted API](https://docs.roboflow.com/deploy/hosted-api) |  | \u2705\n| [Dedicated Deployments](https://docs.roboflow.com/deploy/dedicated-deployments) |  | \u2705\n| [Commercial Model Licensing](https://roboflow.com/licensing) |  | Paid\n| [Device Management](https://docs.roboflow.com/roboflow-enterprise) |  | Enterprise\n| [Model Monitoring](https://docs.roboflow.com/deploy/model-monitoring) |  | Enterprise\n\n## \ud83c\udf29\ufe0f hosted compute\n\nIf you don't want to manage your own infrastructure for self-hosting, Roboflow offers a hosted Inference Server via [one-click Dedicated Deployments](https://docs.roboflow.com/deploy/dedicated-deployments) (CPU and GPU machines) billed hourly, or simple models and Workflows via our [serverless Hosted API](https://docs.roboflow.com/deploy/hosted-api) billed per API-call.\n\nWe offer a [generous free-tier](https://roboflow.com/pricing) to get started.\n\n## \ud83d\udda5\ufe0f run on-prem or self-hosted\n\nInference is designed to run on a wide range of hardware from beefy cloud servers to tiny edge devices. This lets you easily develop against your local machine or our cloud infrastructure and then seamlessly switch to another device for production deployment.\n\n`inference server start` attempts to automatically choose the optimal container to optimize performance on your machine (including with GPU acceleration via NVIDIA CUDA when available). Special installation notes and performance tips by device are listed below:\n\n* [Linux](https://inference.roboflow.com/install/linux/)\n* [Windows](https://inference.roboflow.com/install/windows/)\n* [Mac](https://inference.roboflow.com/install/mac/)\n* [NVIDIA Jetson](https://inference.roboflow.com/install/jetson/)\n* [Raspberry Pi](https://inference.roboflow.com/install/raspberry-pi/)\n* [Your Own Cloud](https://inference.roboflow.com/install/cloud/)\n* [Other Devices](https://inference.roboflow.com/install/other/)\n\n### \u2b50\ufe0f New: Enterprise Hardware\n\nFor manufacturing and logistics use-cases Roboflow now offers [the NVIDIA Jetson-based Flowbox](https://roboflow.com/industries/manufacturing/box), a ruggedized CV center pre-configured with Inference and optimized for running in secure networks. It has integrated support for machine vision cameras like Basler and Lucid over GigE, supports interfacing with PLCs and HMIs via OPC or MQTT, enables enterprise device management through a DMZ, and comes with the support of our team of computer vision experts to ensure your project is a success.\n\n## \ud83d\udcda documentation\n\nVisit our [documentation](https://inference.roboflow.com) to explore comprehensive guides, detailed API references, and a wide array of tutorials designed to help you harness the full potential of the Inference package.\n\n## \u00a9 license\n\nThe core of Inference is licensed under Apache 2.0.\n\nModels are subject to licensing which respects the underlying architecture. These licenses are listed in [`inference/models`](/inference/models). Paid Roboflow accounts include a commercial license for some models (see [roboflow.com/licensing](https://roboflow.com/licensing) for details).\n\nCloud connected functionality (like our model and Workflows registries, dataset management, model monitoring, device management, and managed infrastructure) requires a Roboflow account and API key & is metered based on usage.\n\nEnterprise functionality is source-available in [`inference/enterprise`](/inference/enterprise/) under an [enterprise license](/inference/enterprise/LICENSE.txt) and usage in production requires an active Enterprise contract in good standing.\n\nSee the \"Self Hosting and Edge Deployment\" section of the [Roboflow Licensing](https://roboflow.com/licensing) documentation for more information on how Roboflow Inference is licensed.\n\n## \ud83c\udfc6 contribution\n\nWe would love your input to improve Roboflow Inference! Please see our [contributing guide](https://github.com/roboflow/inference/blob/master/CONTRIBUTING.md) to get started. Thank you to all of our contributors! \ud83d\ude4f\n\n\n<br>\n\n<div align=\"center\">\n  <div align=\"center\">\n      <a href=\"https://youtube.com/roboflow\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/youtube.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949634652\"\n            width=\"3%\"\n          />\n      </a>\n      <img src=\"https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png\" width=\"3%\"/>\n      <a href=\"https://roboflow.com\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/roboflow-app.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949746649\"\n            width=\"3%\"\n          />\n      </a>\n      <img src=\"https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png\" width=\"3%\"/>\n      <a href=\"https://www.linkedin.com/company/roboflow-ai/\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/linkedin.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949633691\"\n            width=\"3%\"\n          />\n      </a>\n      <img src=\"https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png\" width=\"3%\"/>\n      <a href=\"https://docs.roboflow.com\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/knowledge.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949634511\"\n            width=\"3%\"\n          />\n      </a>\n      <img src=\"https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png\" width=\"3%\"/>\n      <a href=\"https://disuss.roboflow.com\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/forum.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949633584\"\n            width=\"3%\"\n          />\n      <img src=\"https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png\" width=\"3%\"/>\n      <a href=\"https://blog.roboflow.com\">\n          <img\n            src=\"https://media.roboflow.com/notebooks/template/icons/purple/blog.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949633605\"\n            width=\"3%\"\n          />\n      </a>\n      </a>\n  </div>\n</div>\n"
  }
]