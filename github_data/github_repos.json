[
  {
    "name": "davidatoms",
    "url": "https://github.com/davidatoms/davidatoms",
    "description": "Github Readme Profile",
    "type": "original",
    "updated_at": "2025-03-29T01:21:41Z",
    "readme": "# David Adams Automatic GitHub Readme\n\n<p align=\"left\"><b>Last Updated:</b> <!-- last_updated starts -->March 29, 2025 at 01:21 (88/365 (0.241) of the year)<!-- last_updated ends -->\n</p>\n\n<p align=\"left\">\n  <img src=\"https://img.shields.io/badge/Python-3776AB?style=flat&logo=python&logoColor=white\" />\n  <img src=\"https://img.shields.io/badge/Go-00ADD8?style=flat&logo=go&logoColor=white\" />\n  <img src=\"https://img.shields.io/badge/Rust-000000?style=flat&logo=rust&logoColor=white\" />\n  <img src=\"https://img.shields.io/badge/React-20232A?style=flat&logo=react&logoColor=61DAFB\" />\n  <img src=\"https://img.shields.io/badge/Bash-4EAA25?style=flat&logo=gnu-bash&logoColor=white\" />\n</p>\n\nI am passionate about a better future through innovation and investments. \n\n## Recent Repositories\n<!-- recent_repos starts -->\n[**davidatoms**](https://github.com/davidatoms/davidatoms) - This repository showcases David Adams' coding projects, professional profile, and documentation generated using Anthropic's Claude AI and GitHub Actions to update the README dynamically.\n\n[**langchain**](https://github.com/davidatoms/langchain) - LangChain is a framework for building LLM-powered applications, enabling developers to chain together interoperable components and integrations, simplifying AI application development while future-proofing decisions as the underlying technology evolves.\n\n[**dynamo**](https://github.com/davidatoms/dynamo) - NVIDIA Dynamo is a high-performance distributed inference framework designed for serving large language models, enabling disaggregated prefill & decode operations, dynamic GPU scheduling, and optimized data transfers.\n\n[**SpikeStream**](https://github.com/davidatoms/SpikeStream) - SpikeStream is a visualization tool showcasing brain-inspired spiking neural networks as seen on the Cortical Labs website and YouTube channel.\n\n[**Doctor-Dignity**](https://github.com/davidatoms/Doctor-Dignity) - Doctor Dignity is an open-source project aiming to provide personalized medical assistance through a locally hosted, privacy-preserving large language model fine-tuned on medical data.\n\n[**hugo-website**](https://github.com/davidatoms/hugo-website) - A Hugo template tailored for minimalist academic websites, featuring the PaperMod theme, GitHub Pages deployment, and additional customizations for optimal performance across devices.\n\n[**jupyter_tool**](https://github.com/davidatoms/jupyter_tool) - A Python package for programmatically creating, loading, and manipulating Jupyter notebooks, enabling integration into AI agents and workflows.\n\n[**fabric**](https://github.com/davidatoms/fabric) - `fabric` is an open-source AI framework that helps integrate AI capabilities into everyday tasks and workflows through reusable prompt patterns.\n\n[**buster**](https://github.com/davidatoms/buster) - Buster is an AI-powered analytics platform that provides a modern data warehousing solution and a self-serve business intelligence platform for building data applications.\n\n[**gnome-calendar**](https://github.com/davidatoms/gnome-calendar) - GNOME Calendar is an elegant and user-friendly calendar application designed for the GNOME desktop environment, with a strong emphasis on attention to detail and design.\n<!-- recent_repos ends -->\n\n_Project descriptions generated by Anthropic's Claude, backed by GitHub Actions_\n\n<br>\n\n![Star this repository](https://img.shields.io/badge/Star%20this%20repository-FFDD00?style=flat&logo=github&logoColor=white)\n![Profile Views](https://komarev.com/ghpvc/?username=davidatoms&style=flat&color=blue&label=Views)\n"
  },
  {
    "name": "langchain",
    "url": "https://github.com/davidatoms/langchain",
    "description": "\ud83e\udd9c\ud83d\udd17 Build context-aware reasoning applications",
    "type": "fork",
    "updated_at": "2025-03-29T00:26:37Z",
    "readme": "<picture>\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"docs/static/img/logo-dark.svg\">\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/static/img/logo-light.svg\">\n  <img alt=\"LangChain Logo\" src=\"docs/static/img/logo-dark.svg\" width=\"80%\">\n</picture>\n\n<div>\n<br>\n</div>\n\n[![Release Notes](https://img.shields.io/github/release/langchain-ai/langchain?style=flat-square)](https://github.com/langchain-ai/langchain/releases)\n[![CI](https://github.com/langchain-ai/langchain/actions/workflows/check_diffs.yml/badge.svg)](https://github.com/langchain-ai/langchain/actions/workflows/check_diffs.yml)\n[![PyPI - License](https://img.shields.io/pypi/l/langchain-core?style=flat-square)](https://opensource.org/licenses/MIT)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-core?style=flat-square)](https://pypistats.org/packages/langchain-core)\n[![GitHub star chart](https://img.shields.io/github/stars/langchain-ai/langchain?style=flat-square)](https://star-history.com/#langchain-ai/langchain)\n[![Open Issues](https://img.shields.io/github/issues-raw/langchain-ai/langchain?style=flat-square)](https://github.com/langchain-ai/langchain/issues)\n[![Open in Dev Containers](https://img.shields.io/static/v1?label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode&style=flat-square)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/langchain-ai/langchain)\n[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/langchain-ai/langchain)\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)](https://twitter.com/langchainai)\n\n> [!NOTE]\n> Looking for the JS/TS library? Check out [LangChain.js](https://github.com/langchain-ai/langchainjs).\n\nLangChain is a framework for building LLM-powered applications. It helps you chain\ntogether interoperable components and third-party integrations to simplify AI\napplication development \u2014  all while future-proofing decisions as the underlying\ntechnology evolves.\n\n```bash\npip install -U langchain\n```\n\nTo learn more about LangChain, check out\n[the docs](https://python.langchain.com/docs/introduction/). If you\u2019re looking for more\nadvanced customization or agent orchestration, check out\n[LangGraph](https://langchain-ai.github.io/langgraph/), our framework for building\ncontrollable agent workflows.\n\n## Why use LangChain?\n\nLangChain helps developers build applications powered by LLMs through a standard\ninterface for models, embeddings, vector stores, and more. \n\nUse LangChain for:\n- **Real-time data augmentation**. Easily connect LLMs to diverse data sources and\nexternal / internal systems, drawing from LangChain\u2019s vast library of integrations with\nmodel providers, tools, vector stores, retrievers, and more.\n- **Model interoperability**. Swap models in and out as your engineering team\nexperiments to find the best choice for your application\u2019s needs. As the industry\nfrontier evolves, adapt quickly \u2014 LangChain\u2019s abstractions keep you moving without\nlosing momentum.\n\n## LangChain\u2019s ecosystem\nWhile the LangChain framework can be used standalone, it also integrates seamlessly\nwith any LangChain product, giving developers a full suite of tools when building LLM\napplications. \n\nTo improve your LLM application development, pair LangChain with:\n\n- [LangSmith](http://www.langchain.com/langsmith) - Helpful for agent evals and\nobservability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain\nvisibility in production, and improve performance over time.\n- [LangGraph](https://langchain-ai.github.io/langgraph/) - Build agents that can\nreliably handle complex tasks with LangGraph, our low-level agent orchestration\nframework. LangGraph offers customizable architecture, long-term memory, and\nhuman-in-the-loop workflows \u2014 and is trusted in production by companies like LinkedIn,\nUber, Klarna, and GitLab.\n- [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/#langgraph-platform) - Deploy\nand scale agents effortlessly with a purpose-built deployment platform for long\nrunning, stateful workflows. Discover, reuse, configure, and share agents across\nteams \u2014 and iterate quickly with visual prototyping in\n[LangGraph Studio](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/).\n\n## Additional resources\n- [Tutorials](https://python.langchain.com/docs/tutorials/): Simple walkthroughs with\nguided examples on getting started with LangChain.\n- [How-to Guides](https://python.langchain.com/docs/how_to/): Quick, actionable code\nsnippets for topics such as tool calling, RAG use cases, and more.\n- [Conceptual Guides](https://python.langchain.com/docs/concepts/): Explanations of key\nconcepts behind the LangChain framework.\n- [API Reference](https://python.langchain.com/api_reference/): Detailed reference on\nnavigating base packages and integrations for LangChain.\n"
  },
  {
    "name": "dynamo",
    "url": "https://github.com/davidatoms/dynamo",
    "description": "A Datacenter Scale Distributed Inference Serving Framework",
    "type": "fork",
    "updated_at": "2025-03-18T20:52:10Z",
    "readme": "<!--\nSPDX-FileCopyrightText: Copyright (c) 2024-2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\nSPDX-License-Identifier: Apache-2.0\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# NVIDIA Dynamo\n\n[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![GitHub Release](https://img.shields.io/github/v/release/ai-dynamo/dynamo)](https://github.com/ai-dynamo/dynamo/releases/latest)\n[![Discord](https://dcbadge.limes.pink/api/server/D92uqZRjCZ?style=flat)](https://discord.gg/D92uqZRjCZ)\n\n| **[Guides](docs/guides)** | **[Architecture and Features](docs/architecture.md)** | **[APIs](lib/bindings/python/README.md)** | **[SDK](deploy/dynamo/sdk/README.md)** |\n\nNVIDIA Dynamo is a high-throughput low-latency inference framework designed for serving generative AI and reasoning models in multi-node distributed environments. Dynamo is designed to be inference engine agnostic (supports TRT-LLM, vLLM, SGLang or others) and captures LLM-specific capabilities such as:\n\n- **Disaggregated prefill & decode inference** \u2013 Maximizes GPU throughput and facilitates trade off between throughput and latency.\n- **Dynamic GPU scheduling** \u2013 Optimizes performance based on fluctuating demand\n- **LLM-aware request routing** \u2013 Eliminates unnecessary KV cache re-computation\n- **Accelerated data transfer** \u2013 Reduces inference response time using NIXL.\n- **KV cache offloading** \u2013 Leverages multiple memory hierarchies for higher system throughput\n\nBuilt in Rust for performance and in Python for extensibility, Dynamo is fully open-source and driven by a transparent, OSS (Open Source Software) first development approach.\n\n### Installation\n\nThe following examples require a few system level packages.\n\n```\napt-get update\nDEBIAN_FRONTEND=noninteractive apt-get install -yq python3-dev libucx0\n\npip install ai-dynamo[all]\n```\n\n> [!NOTE]\n> TensorRT-LLM Support is currently available on a [branch](https://github.com/ai-dynamo/dynamo/tree/dynamo/trtllm_llmapi_v1/examples/trtllm#building-the-environment)\n\n### Running and Interacting with an LLM Locally\n\nTo run a model and interact with it locally you can call `dynamo\nrun` with a hugging face model. `dynamo run` supports several backends\nincluding: `mistralrs`, `sglang`, `vllm`, and `tensorrtllm`.\n\n#### Example Command\n\n```\ndynamo run out=vllm deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n```\n\n```\n? User \u203a Hello, how are you?\n\u2714 User \u00b7 Hello, how are you?\nOkay, so I'm trying to figure out how to respond to the user's greeting. They said, \"Hello, how are you?\" and then followed it with \"Hello! I'm just a program, but thanks for asking.\" Hmm, I need to come up with a suitable reply. ...\n```\n\n### LLM Serving\n\nDynamo provides a simple way to spin up a local set of inference\ncomponents including:\n\n- **OpenAI Compatible Frontend** \u2013 High performance OpenAI compatible http api server written in Rust.\n- **Basic and Kv Aware Router** \u2013 Route and load balance traffic to a set of workers.\n- **Workers** \u2013 Set of pre-configured LLM serving engines.\n\nTo run a minimal configuration you can use a pre-configured\nexample.\n\n#### Start Dynamo Distributed Runtime Services\n\nFirst start the Dynamo Distributed Runtime services:\n\n```bash\ndocker compose -f deploy/docker-compose.yml up -d\n```\n\n#### Start Dynamo LLM Serving Components\n\nNext serve a minimal configuration with an http server, basic\nround-robin router, and a single worker.\n\n```bash\ncd examples/llm\ndynamo serve graphs.agg:Frontend -f configs/agg.yaml\n```\n\n#### Send a Request\n\n```bash\ncurl localhost:8000/v1/chat/completions   -H \"Content-Type: application/json\"   -d '{\n    \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n    \"messages\": [\n    {\n        \"role\": \"user\",\n        \"content\": \"Hello, how are you?\"\n    }\n    ],\n    \"stream\":false,\n    \"max_tokens\": 300\n  }' | jq\n```\n\n"
  },
  {
    "name": "SpikeStream",
    "url": "https://github.com/davidatoms/SpikeStream",
    "description": "SpikeStream Visualisation Code as seen on https://spikestream.corticallabs.com and https://www.youtube.com/watch?v=9ksLuRoEq6A",
    "type": "fork",
    "updated_at": "2025-03-06T05:18:13Z",
    "readme": "# SpikeStream\nSpikeStream Visualisation Code as seen on https://spikestream.corticallabs.com and https://www.youtube.com/watch?v=9ksLuRoEq6A\n"
  },
  {
    "name": "Doctor-Dignity",
    "url": "https://github.com/davidatoms/Doctor-Dignity",
    "description": "Doctor Dignity is an LLM that can pass the US Medical Licensing Exam. It works offline, it's cross-platform, & your health data stays private.",
    "type": "fork",
    "updated_at": "2025-03-06T03:43:56Z",
    "readme": "# Doctor Dignity\n<p align=\"center\">\n\n\nDISCLAIMER - Do not take any advice from Doctor Dignity seriously yet. This is a work in progress and taking any advice seriously could result in serious injury or even death. \n\n<img src=\"https://i.imgur.com/18jVWiV.png\" width=\"400\" height=\"400\">\n</p>\n\n## Overview\nDoctor Dignity is a Large Language Model that can pass the US Medical Licensing Exam. This is an open-source project with a mission to provide everyone their own private doctor. Doctor Dignity is a version of Meta's [Llama2](https://ai.meta.com/llama/) 7 billion parameter Large Language Model that was fine-tuned on a Medical Dialogue Dataset, then further improved using Reinforcement Learning & Constitutional AI. Since the model is only 3 Gigabytes in size, it fits on any local device, so there is no need to pay an API to use it. It's free, made for offline usage which preserves patient confidentiality, and it's available on iOS, Android, and Web. Pull requests for feature additions and improvements are encouraged.\n\n## Dependencies\n- [Numpy](https://numpy.org/install/)           (Use matrix math operations)\n- [PyTorch](https://pytorch.org/)         (Build Deep Learning models)\n- [Datasets](https://huggingface.co/docs/datasets/index)        (Access datasets from huggingface hub)\n- [Huggingface_hub](https://huggingface.co/docs/huggingface_hub/v0.5.1/en/package_reference/hf_api) (access huggingface data & models) \n- [Transformers](https://huggingface.co/docs/transformers/index)    (Access models from HuggingFace hub)\n- [Trl](https://huggingface.co/docs/trl/index)             (Transformer Reinforcement Learning. And fine-tuning.)\n- [Bitsandbytes](https://github.com/TimDettmers/bitsandbytes)    (makes models smaller, aka 'quantization')\n- [Sentencepiece](https://github.com/google/sentencepiece)       (Byte Pair Encoding scheme aka 'tokenization')\n- [OpenAI](https://openai.com)          (Create synthetic fine-tuning and reward model data)\n- [TVM](https://tvm.apache.org/)             (Tensor Virtual Machine, converts onnx model to efficient cross-platform use)\n- [Peft](https://huggingface.co/blog/peft)            (Parameter Efficient Fine Tuning, use low rank adaption (LoRa) to fine-tune)\n- [Onnx](https://onnx.ai/)            (Convert trained model to universal format)\n\n\n\n## Installation\n\nInstall all dependencies in one line using [pip](https://pip.pypa.io/en/stable/installation/)\n\n```bash\npip install numpy torch datasets huggingface_hub transformers trl bitsandbytes sentencepiece openai tvm peft onnx\n```\n\n## iOS QuickStart v2\n\n1. Clone this repository\n```bash\ngit clone https://github.com/llSourcell/Doctor-Dignity\n```\n2. Download the Weights\n```bash\nmkdir -p dist/prebuilt\ngit clone https://github.com/mlc-ai/binary-mlc-llm-libs.git dist/prebuilt/lib\ncd dist/prebuilt\ngit lfs install\nwget --no-check-certificate 'https://drive.google.com/file/d/1MLy8BDhuTTcXqagzLFMA07JDzqjQYUTB/view?pli=1'\ncd ../..\n```\n3. Build the Tensor Virtual Machine Runtime\n```bash\ngit submodule update --init --recursive\npip install apache-tvm\ncd ./ios\npip install --pre --force-reinstall mlc-ai-nightly mlc-chat-nightly -f https://mlc.ai/wheels \n./prepare_libs.sh\n```\n** Find the right version of MLC LLM for your system [here](https://mlc.ai/package/)\n4. Add Weights to Xcode\n```bash\ncd ./ios\nopen ./prepare_params.sh # make sure builtin_list only contains \"RedPajama-INCITE-Chat-3B-v1-q4f16_1\"\n./prepare_params.sh\n```\n5. Open Xcode Project and run! \n\n\n## DIY Training\n\nIn order to train the model, you can run the training.ipynb notebook locally or remotely via a cloud service like Google Colab Pro. The training process requires a GPU, and if you don't have one then the most accessible option i found was using Google Colab [Pro](https://colab.research.google.com/signup) which costs $10/month. The total training time for Doctor Dignity including supervised fine-tuning of the initial LLama model on custom medical data, as well as further improving it via Reinforcement Learning from Constitional AI Feedback took 24 hours on a paid instance of Google Colab. If you're interested in learning more about how this process works, details are in the training.ipynb notebook. \n\n#### Cloud Training\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/llSourcell/DoctorGPT/blob/main/llama2.ipynb)\nclick here: https://colab.research.google.com/github/llSourcell/Doctor-Dignity/blob/main/llama2.ipynb\n\n#### Local Training\n\n```bash\ngit clone https://github.com/llSourcell/Doctor-Dignity.git\njupyter training.ipynb\n```\nGet jupyter [here](https://jupyter.org/install)\n\n## Usage  https://huggingface.co/llSourcell/medllama2_7b\n\nThere are 2 huggingface repos, one which is quantized for mobile and one that is not.\n\n#### Old iOS app \n   \n- Step 1: [Download](https://github.com/mlc-ai/mlc-llm/tree/main/ios) the iOS Machine Learning Compilation Chat Repository\n- Step 2: Follow the [installation steps](https://mlc.ai/mlc-llm/docs/deploy/ios.html) \n- Step 3: Once the app is running on your iOS device or simulator, tap \"add model variant\"\n- Step 4: Enter the URL for the latest Doctor Dignity model to download it: [https://huggingface.co/llSourcell/doctorGPT_mini] (https://huggingface.co/llSourcell/doctorGPT_mini)\n- Step 5: Tap 'Add Model' and start chatting locally, inference runs on device. No internet connection needed!\n\n#### Android app (TODO)\n\n- Step 1: [Download](https://github.com/mlc-ai/mlc-llm/tree/main/android) the Android Machine Learning Compilation Chat Repository\n- Step 2: Follow the [installation steps]([https://mlc.ai/mlc-llm/docs/deploy/ios.html](https://mlc.ai/mlc-llm/docs/deploy/android.html)) \n- Step 3: Tap \"add model variant\"\n- Step 4: Enter the URL for the latest Doctor Dignity model to download it: [https://huggingface.co/llSourcell/doctorGPT_mini](https://huggingface.co/llSourcell/doctorGPT_mini)\n- Step 5: Tap 'Add Model' and start chatting locally! No internet needed. \n\n#### Web (TODO)\n\nAs an experiment in Online Learning using actual human feedback, i want to deploy the model as a Flask API with a React front-end. In this case, anyone can chat with the model at this URL. After each query, a human can rate the model's response. This rating is then used to further improve the model's performance through reinforcement learning. to run the app, download [flask](https://flask.palletsprojects.com/en/2.3.x/) and then you can run:\n\n```bash\nflask run\n```\n\nThen visit localhost:3000 to interact with it! You can also deploy to [vercel](https://vercel.com/templates/ai)\n\n## Credits\n\nMeta, MedAlpaca, Apache, MLC Chat & OctoML \n\n"
  },
  {
    "name": "hugo-website",
    "url": "https://github.com/davidatoms/hugo-website",
    "description": "Minimalist Hugo template for academic websites",
    "type": "fork",
    "updated_at": "2025-03-05T02:13:24Z",
    "readme": "# Minimalist Hugo Template for Academic Websites\n\nThis repository contains a [Hugo](https://github.com/gohugoio/hugo) template to create a personal academic website. The template uses the [PaperMod theme](https://github.com/adityatelange/hugo-PaperMod) but modifies it in various ways to be more minimalist and more adapted to academic websites. The website is hosted on [GitHub Pages](https://docs.github.com/en/pages/getting-started-with-github-pages/about-github-pages).\n\n## Documentation\n\nThe template is documented at https://pascalmichaillat.org/d5/.\n\n## Illustration\n\nThe website produced by the template can be viewed at https://pascalmichaillat.org/hugo-website/.\n\n## Installation\n\n### On your local machine\n\n+ Clone the repository to your local machine\n+ Install [Hugo](https://gohugo.io/installation/). On a Mac, this is easily done with [Homebrew](https://brew.sh): simply run `brew install hugo` in the terminal.\n+ Since the website is hosted on GitHub Pages, it is convenient to install [GitHub Desktop](https://desktop.github.com). The website can conveniently be updated from your local machine via GitHub Desktop without going to GitHub.\n+ Update the `baseURL` parameter in `config.yml` with the website URL that you plan to use. By default the ULR is `https://username.github.io`.\n\n### On your GitHub account\n\n+ The first time that you push your repository to GitHub, you need to allow GitHub Actions and GitHub Pages so the website can be built and deployed to GitHub Pages.\n+ The first step is to [ask GitHub to publish the website with a GitHub Action](https://docs.github.com/en/pages/getting-started-with-github-pages/configuring-a-publishing-source-for-your-github-pages-site#publishing-with-a-custom-github-actions-workflow).  GitHub offers a ready-made action to publish a Hugo website, called `Deploy Hugo site to Pages`. This action must be enabled in the [Pages Settings](https://github.com/pmichaillat/hugo-website/settings/pages) of your GitHub repository. You can view the workflow triggered by the action in the `.github/workflows/hugo.yml` file.\n+ Once the GitHub Actions are enabled, GitHub will build and publish the website as soon as the repository is updated. \n+ If you would like to update the deployment action (for instance because it became outdated and fails to deploy the site), you can find the [most recent action on GitHub]( https://github.com/actions/starter-workflows/blob/main/pages/hugo.yml). You can place this file directly in the `.github/workflows` folder to replace the old `hugo.yml` file\u2014but make sure to set `push:\n    branches` to `[\"main\"]`.\n\n## Usage\n\n### Development\n\nNavigate to the website directory and run `hugo server` in the terminal. The command builds the website on your machine and makes it available at http://localhost:1313. You can modify the content of the repository and develop your website entirely on your local machine.\n\n### Compilation\n\nOnce your website is ready to be made public, run `hugo` in the terminal from the website directory. When you run the `hugo` command, Hugo processes your content, templates, and other project files and generates a static website. The resulting output is placed in the `public` folder.\n\n### Deployment\n\nWith GitHub Desktop, commit the changes and push them to the website repository on GitHub. Then, the [GitHub Action](https://github.com/pmichaillat/hugo-website/actions/workflows/hugo.yml) builds the website and deploys it to [GitHub Pages](https://github.com/pmichaillat/hugo-website/deployments/github-pages).\n\n## Performance\n\nDespite the modifications to the PaperMod theme, the website continues to perform well on mobile and desktop devices. Here is an overview of the mobile performance from [PageSpeed Insights](https://pagespeed.web.dev/):\n\n<img width=\"470\" alt=\"mobile\" src=\"https://github.com/pmichaillat/hugo-website/assets/85443660/1488df3e-19bb-4f9f-8a86-11f361414d92\">\n\nAnd here is an overview of the desktop performance:\n\n<img width=\"453\" alt=\"desktop\" src=\"https://github.com/pmichaillat/pmichaillat.github.io/assets/85443660/eff134d2-6097-4bc2-bfd7-4f5c18571789\">\n\n## Software\n\nThe website was built with Hugo v0.128.2 on an Apple Silicon Mac running macOS Sonoma 14.5. The website was tested and validated on Safari 17.5 on a Mac and on Safari on an iPhone with iOS 17.5.[^1]\n\n[^1]:  The updates to Hugo v0.120 and then to Hugo v0.123 introduced some issues with previous versions of the template. The issue caused by the update to Hugo v0.120 is [#1325 in the PaperMod repo](https://github.com/adityatelange/hugo-PaperMod/issues/1325). The issue caused by the update to Hugo v0.123 is [#1449 in the PaperMod repo](https://github.com/adityatelange/hugo-PaperMod/issues/1449). These issues are now resolved\n\nWhile the template should also work on other operating systems and with other versions of Hugo, compatibility cannot be guaranteed. Users on Windows or Linux systems, or those using different Hugo versions, may need to make minor adjustments. Similarly, the website should work with other browsers, but minor adjustments might be necessary for perfect compatibility. [Please report](https://github.com/pmichaillat/hugo-website/issues) any compatibility issues or bugs to help improve cross-platform support.\n\n## License\n\nThe content of this repository is licensed under the terms of the MIT License.\n\n## Real-world implementations\n\n+ [Pascal Michaillat's website](https://pascalmichaillat.org/) ([source code](https://github.com/pmichaillat/pmichaillat.github.io))\n+ [Dylan Balla-Elliott's website](https://www.dballaelliott.com) ([source code](https://github.com/dballaelliott/site))\n+ [Rosa van den Ende's website](https://rosavandenende.github.io) ([source code](https://github.com/rosavandenende/rosavandenende.github.io))\n+ [Samia Kabir's website](https://samiakabir.com) ([source code](https://github.com/SamiaKabir/samiakabir.github.io))\n+ [Dylan Laplace Mermoud's website](https://dylanlaplacemermoud.github.io) ([source code](https://github.com/DylanLaplaceMermoud/dylanlaplacemermoud.github.io))\n+ [Maarten Goos's website](https://maartengoos.com) ([source code](https://github.com/MaartenGoos/website))\n+ [Aryan Ahadinia's website](https://aryanahadinia.github.io) ([source code](https://github.com/AryanAhadinia/AryanAhadinia.github.io))\n+ [Jun Wong's website](https://junwong.org) ([source code](https://github.com/junwong97/junwong97.github.io))\n+ [Erling Rennemo Jellum's website](https://erlingrj.github.io) ([source code](https://github.com/erlingrj/erlingrj.github.io))\n+ [Yangkeun Yun's website](https://yangkeunyun.github.io) ([source code](https://github.com/yangkeunyun/yangkeunyun.github.io))\n+ [Maghfira Ramadhani's website](https://maghfiraer.github.io) ([source code](https://github.com/maghfiraer/maghfiraer.github.io))\n+ [Ismael Moreno-Martinez's website](https://ismaelmorenomartinez.eu) ([source code](https://github.com/ismaelmorenomartinez/ismaelmorenomartinez.github.io))\n+ [Lucas Warwar's website](https://lucaswarwar.github.io) ([source code](https://github.com/lucaswarwar/lucaswarwar.github.io))\n+ [Franz Louis Cesista's website](https://leloykun.github.io) ([source code](https://github.com/leloykun/leloykun.github.io))\n+ [Gabe Sekeres's website](https://gabesekeres.com) ([source code](https://github.com/gsekeres/hugo_site))\n+ [Kevin Roice's website](https://kevroi.github.io) ([source code](https://github.com/kevroi/kevroi.github.io))\n+ [Daniel Barbosa's website](https://dacbarbosa.github.io) ([source code](https://github.com/dacbarbosa/dacbarbosa.github.io))\n+ [Wei Zhang's website](https://weizhang-econ.github.io) ([source code](https://github.com/weizhang-econ/weizhang-econ.github.io))\n+ [Ben Hermann's website](http://benhermann.eu) ([source code](https://github.com/bhermann/bhermann.github.io))\n+ [Kostas Bimpikis's website](https://stanford.edu/~kostasb/)\n+ [Pragathi Praveena's website](https://pragathipraveena.com)\n+ [Qiwei He's website](https://www.qiwei-he.com)\n+ [Pierre Bardier's website](https://pierrebard.github.io/pierre-bardier/)\n+ [Marek Wiewi\u00f3rka's website](https://marekwiewiorka.org)\n"
  },
  {
    "name": "jupyter_tool",
    "url": "https://github.com/davidatoms/jupyter_tool",
    "description": null,
    "type": "fork",
    "updated_at": "2025-03-02T03:30:43Z",
    "readme": "# Jupyter Notebook Tool\n\nA Python package providing atomic tools for langchain-based AI agents to manipulate Jupyter notebooks. Built on\nnbclient/nbformat, it enables programmatic notebook creation, loading, and manipulation.\n\n## Installation\n\nYou can install from this repo in development mode with `pip install -e ./src/`\n\n## Core Functionality\n\nThe system maintains notebook sessions using tokens. Each operation requires a valid token obtained from either:\n\n- `create_notebook()`: Creates new empty notebook\n- `load_notebook(url)`: Loads notebook from URL or file path\n\n## Available Operations\n\nWith a valid token, the following operations are supported:\n\n- `list_cells(token)`: Get ordered list of cell IDs\n- `create_cell(token, source, cell_type, position)`: Add new cell\n- `update_cell(token, id, source)`: Modify cell content\n- `execute_cell(token, id)`: Run code cell and get output\n- `delete_cell(token, id)`: Remove cell from notebook\n- `get_notebook(token)`: Retrieve current notebook state\n\n## Error Handling\n\nThe system provides specific error types for common failure cases:\n\n- InvalidTokenError\n- CellNotFoundError\n- CellTypeError\n- NotebookLoadError\n- KernelError\n\n## Worked Example\n\nSee docs/example.ipynb for an example of how to use the package.\n"
  },
  {
    "name": "fabric",
    "url": "https://github.com/davidatoms/fabric",
    "description": "fabric is an open-source framework for augmenting humans using AI. It provides a modular framework for solving specific problems using a crowdsourced set of AI prompts that can be used anywhere.",
    "type": "fork",
    "updated_at": "2025-03-02T03:14:23Z",
    "readme": "<div align=\"center\">\n\n<img src=\"./images/fabric-logo-gif.gif\" alt=\"fabriclogo\" width=\"400\" height=\"400\"/>\n\n# `fabric`\n\n![Static Badge](https://img.shields.io/badge/mission-human_flourishing_via_AI_augmentation-purple)\n<br />\n![GitHub top language](https://img.shields.io/github/languages/top/danielmiessler/fabric)\n![GitHub last commit](https://img.shields.io/github/last-commit/danielmiessler/fabric)\n[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)\n\n<p class=\"align center\">\n<h4><code>fabric</code> is an open-source framework for augmenting humans using AI.</h4>\n</p>\n\n[Updates](#updates) \u2022\n[What and Why](#whatandwhy) \u2022\n[Philosophy](#philosophy) \u2022\n[Installation](#Installation) \u2022\n[Usage](#Usage) \u2022\n[Examples](#examples) \u2022\n[Just Use the Patterns](#just-use-the-patterns) \u2022\n[Custom Patterns](#custom-patterns) \u2022\n[Helper Apps](#helper-apps) \u2022\n[Meta](#meta)\n\n![Screenshot of fabric](images/fabric-summarize.png)\n\n</div>\n\n## Navigation\n\n- [`fabric`](#fabric)\n  - [Navigation](#navigation)\n  - [Updates](#updates)\n  - [Intro videos](#intro-videos)\n  - [What and why](#what-and-why)\n  - [Philosophy](#philosophy)\n    - [Breaking problems into components](#breaking-problems-into-components)\n    - [Too many prompts](#too-many-prompts)\n  - [Installation](#installation)\n    - [Get Latest Release Binaries](#get-latest-release-binaries)\n    - [From Source](#from-source)\n    - [Environment Variables](#environment-variables)\n    - [Setup](#setup)\n    - [Add aliases for all patterns](#add-aliases-for-all-patterns)\n      - [Save your files in markdown using aliases](#save-your-files-in-markdown-using-aliases)\n    - [Migration](#migration)\n    - [Upgrading](#upgrading)\n  - [Usage](#usage)\n  - [Our approach to prompting](#our-approach-to-prompting)\n  - [Examples](#examples)\n  - [Just use the Patterns](#just-use-the-patterns)\n  - [Custom Patterns](#custom-patterns)\n  - [Helper Apps](#helper-apps)\n    - [`to_pdf`](#to_pdf)\n    - [`to_pdf` Installation](#to_pdf-installation)\n  - [pbpaste](#pbpaste)\n  - [Web Interface](#Web_Interface)\n  - [Meta](#meta)\n    - [Primary contributors](#primary-contributors)\n\n<br />\n\n## Updates\n\n> [!NOTE]\n> February 24, 2025\n>\n> - Fabric now supports Sonnet 3.7! Update and use `-S` to select it as your default if you want, or just use the shortcut `-m claude-3-7-sonnet-latest`. Enjoy!\n\n## What and why\n\nSince the start of 2023 and GenAI we've seen a massive number of AI applications for accomplishing tasks. It's powerful, but _it's not easy to integrate this functionality into our lives._\n\n<div align=\"center\">\n<h4>In other words, AI doesn't have a capabilities problem\u2014it has an <em>integration</em> problem.</h4>\n</div>\n\nFabric was created to address this by enabling everyone to granularly apply AI to everyday challenges.\n\n## Intro videos\n\nKeep in mind that many of these were recorded when Fabric was Python-based, so remember to use the current [install instructions](#Installation) below.\n\n- [Network Chuck](https://www.youtube.com/watch?v=UbDyjIIGaxQ)\n- [David Bombal](https://www.youtube.com/watch?v=vF-MQmVxnCs)\n- [My Own Intro to the Tool](https://www.youtube.com/watch?v=wPEyyigh10g)\n- [More Fabric YouTube Videos](https://www.youtube.com/results?search_query=fabric+ai)\n\n## Philosophy\n\n> AI isn't a thing; it's a _magnifier_ of a thing. And that thing is **human creativity**.\n\nWe believe the purpose of technology is to help humans flourish, so when we talk about AI we start with the **human** problems we want to solve.\n\n### Breaking problems into components\n\nOur approach is to break problems into individual pieces (see below) and then apply AI to them one at a time. See below for some examples.\n\n<img width=\"2078\" alt=\"augmented_challenges\" src=\"https://github.com/danielmiessler/fabric/assets/50654/31997394-85a9-40c2-879b-b347e4701f06\">\n\n### Too many prompts\n\nPrompts are good for this, but the biggest challenge I faced in 2023\u2014\u2014which still exists today\u2014is **the sheer number of AI prompts out there**. We all have prompts that are useful, but it's hard to discover new ones, know if they are good or not, _and manage different versions of the ones we like_.\n\nOne of <code>fabric</code>'s primary features is helping people collect and integrate prompts, which we call _Patterns_, into various parts of their lives.\n\nFabric has Patterns for all sorts of life and work activities, including:\n\n- Extracting the most interesting parts of YouTube videos and podcasts\n- Writing an essay in your own voice with just an idea as an input\n- Summarizing opaque academic papers\n- Creating perfectly matched AI art prompts for a piece of writing\n- Rating the quality of content to see if you want to read/watch the whole thing\n- Getting summaries of long, boring content\n- Explaining code to you\n- Turning bad documentation into usable documentation\n- Creating social media posts from any content input\n- And a million more\u2026\n\n## Installation\n\nTo install Fabric, you can use the latest release binaries or install it from the source.\n\n### Get Latest Release Binaries\n\n#### Windows:\n`https://github.com/danielmiessler/fabric/releases/latest/download/fabric-windows-amd64.exe`\n\n#### MacOS (arm64):\n`curl -L https://github.com/danielmiessler/fabric/releases/latest/download/fabric-darwin-arm64 > fabric && chmod +x fabric && ./fabric --version`\n\n#### MacOS (amd64):\n`curl -L https://github.com/danielmiessler/fabric/releases/latest/download/fabric-darwin-amd64 > fabric && chmod +x fabric && ./fabric --version`\n\n#### Linux (amd64):\n`curl -L https://github.com/danielmiessler/fabric/releases/latest/download/fabric-linux-amd64 > fabric && chmod +x fabric && ./fabric --version`\n\n#### Linux (arm64):\n`curl -L https://github.com/danielmiessler/fabric/releases/latest/download/fabric-linux-arm64 > fabric && chmod +x fabric && ./fabric --version`\n\n### From Source\n\nTo install Fabric, [make sure Go is installed](https://go.dev/doc/install), and then run the following command.\n\n```bash\n# Install Fabric directly from the repo\ngo install github.com/danielmiessler/fabric@latest\n```\n\n### Environment Variables\n\nYou may need to set some environment variables in your `~/.bashrc` on linux or `~/.zshrc` file on mac to be able to run the `fabric` command. Here is an example of what you can add:\n\nFor Intel based macs or linux\n\n```bash\n# Golang environment variables\nexport GOROOT=/usr/local/go\nexport GOPATH=$HOME/go\n\n# Update PATH to include GOPATH and GOROOT binaries\nexport PATH=$GOPATH/bin:$GOROOT/bin:$HOME/.local/bin:$PATH\n```\n\nfor Apple Silicon based macs\n\n```bash\n# Golang environment variables\nexport GOROOT=$(brew --prefix go)/libexec\nexport GOPATH=$HOME/go\nexport PATH=$GOPATH/bin:$GOROOT/bin:$HOME/.local/bin:$PATH\n```\n\n### Setup\n\nNow run the following command\n\n```bash\n# Run the setup to set up your directories and keys\nfabric --setup\n```\n\nIf everything works you are good to go.\n\n### Add aliases for all patterns\n\nIn order to add aliases for all your patterns and use them directly as commands ie. `summarize` instead of `fabric --pattern summarize`\nYou can add the following to your `.zshrc` or `.bashrc` file.\n\n```bash\n# Loop through all files in the ~/.config/fabric/patterns directory\nfor pattern_file in $HOME/.config/fabric/patterns/*; do\n    # Get the base name of the file (i.e., remove the directory path)\n    pattern_name=$(basename \"$pattern_file\")\n\n    # Create an alias in the form: alias pattern_name=\"fabric --pattern pattern_name\"\n    alias_command=\"alias $pattern_name='fabric --pattern $pattern_name'\"\n\n    # Evaluate the alias command to add it to the current shell\n    eval \"$alias_command\"\ndone\n\nyt() {\n    local video_link=\"$1\"\n    fabric -y \"$video_link\" --transcript\n}\n```\n\nYou can add the below code for the equivalent aliases inside PowerShell by running `notepad $PROFILE` inside a PowerShell window:\n\n```powershell\n# Path to the patterns directory\n$patternsPath = Join-Path $HOME \".config/fabric/patterns\"\nforeach ($patternDir in Get-ChildItem -Path $patternsPath -Directory) {\n    $patternName = $patternDir.Name\n\n    # Dynamically define a function for each pattern\n    $functionDefinition = @\"\nfunction $patternName {\n    [CmdletBinding()]\n    param(\n        [Parameter(ValueFromPipeline = `$true)]\n        [string] `$InputObject,\n\n        [Parameter(ValueFromRemainingArguments = `$true)]\n        [String[]] `$patternArgs\n    )\n\n    begin {\n        # Initialize an array to collect pipeline input\n        `$collector = @()\n    }\n\n    process {\n        # Collect pipeline input objects\n        if (`$InputObject) {\n            `$collector += `$InputObject\n        }\n    }\n\n    end {\n        # Join all pipeline input into a single string, separated by newlines\n        `$pipelineContent = `$collector -join \"`n\"\n\n        # If there's pipeline input, include it in the call to fabric\n        if (`$pipelineContent) {\n            `$pipelineContent | fabric --pattern $patternName `$patternArgs\n        } else {\n            # No pipeline input; just call fabric with the additional args\n            fabric --pattern $patternName `$patternArgs\n        }\n    }\n}\n\"@\n    # Add the function to the current session\n    Invoke-Expression $functionDefinition\n}\n\n# Define the 'yt' function as well\nfunction yt {\n    [CmdletBinding()]\n    param(\n        [Parameter(Mandatory = $true)]\n        [string]$videoLink\n    )\n    fabric -y $videoLink --transcript\n}\n```\n\nThis also creates a `yt` alias that allows you to use `yt https://www.youtube.com/watch?v=4b0iet22VIk` to get transcripts, comments, and metadata.\n\n#### Save your files in markdown using aliases\n\nIf in addition to the above aliases you would like to have the option to save the output to your favourite markdown note vault like Obsidian then instead of the above add the following to your `.zshrc` or `.bashrc` file:\n\n```bash\n# Define the base directory for Obsidian notes\nobsidian_base=\"/path/to/obsidian\"\n\n# Loop through all files in the ~/.config/fabric/patterns directory\nfor pattern_file in ~/.config/fabric/patterns/*; do\n    # Get the base name of the file (i.e., remove the directory path)\n    pattern_name=$(basename \"$pattern_file\")\n\n    # Unalias any existing alias with the same name\n    unalias \"$pattern_name\" 2>/dev/null\n\n    # Define a function dynamically for each pattern\n    eval \"\n    $pattern_name() {\n        local title=\\$1\n        local date_stamp=\\$(date +'%Y-%m-%d')\n        local output_path=\\\"\\$obsidian_base/\\${date_stamp}-\\${title}.md\\\"\n\n        # Check if a title was provided\n        if [ -n \\\"\\$title\\\" ]; then\n            # If a title is provided, use the output path\n            fabric --pattern \\\"$pattern_name\\\" -o \\\"\\$output_path\\\"\n        else\n            # If no title is provided, use --stream\n            fabric --pattern \\\"$pattern_name\\\" --stream\n        fi\n    }\n    \"\ndone\n\nyt() {\n    local video_link=\"$1\"\n    fabric -y \"$video_link\" --transcript\n}\n```\n\nThis will allow you to use the patterns as aliases like in the above for example `summarize` instead of `fabric --pattern summarize --stream`, however if you pass in an extra argument like this `summarize \"my_article_title\"` your output will be saved in the destination that you set in `obsidian_base=\"/path/to/obsidian\"` in the following format `YYYY-MM-DD-my_article_title.md` where the date gets autogenerated for you.\nYou can tweak the date format by tweaking the `date_stamp` format.\n\n### Migration\n\nIf you have the Legacy (Python) version installed and want to migrate to the Go version, here's how you do it. It's basically two steps: 1) uninstall the Python version, and 2) install the Go version.\n\n```bash\n# Uninstall Legacy Fabric\npipx uninstall fabric\n\n# Clear any old Fabric aliases\n(check your .bashrc, .zshrc, etc.)\n# Install the Go version\ngo install github.com/danielmiessler/fabric@latest\n# Run setup for the new version. Important because things have changed\nfabric --setup\n```\n\nThen [set your environmental variables](#environmental-variables) as shown above.\n\n### Upgrading\n\nThe great thing about Go is that it's super easy to upgrade. Just run the same command you used to install it in the first place and you'll always get the latest version.\n\n```bash\ngo install github.com/danielmiessler/fabric@latest\n```\n\n## Usage\n\nOnce you have it all set up, here's how to use it.\n\n```bash\nfabric -h\n```\n\n```bash\n\nUsage:\n  fabric [OPTIONS]\n\nApplication Options:\n  -p, --pattern=             Choose a pattern from the available patterns\n  -v, --variable=            Values for pattern variables, e.g. -v=#role:expert -v=#points:30\"\n  -C, --context=             Choose a context from the available contexts\n      --session=             Choose a session from the available sessions\n  -a, --attachment=          Attachment path or URL (e.g. for OpenAI image recognition messages)\n  -S, --setup                Run setup for all reconfigurable parts of fabric\n  -t, --temperature=         Set temperature (default: 0.7)\n  -T, --topp=                Set top P (default: 0.9)\n  -s, --stream               Stream\n  -P, --presencepenalty=     Set presence penalty (default: 0.0)\n  -r, --raw                  Use the defaults of the model without sending chat options (like temperature etc.) and use the user role instead of the system role for patterns.\n  -F, --frequencypenalty=    Set frequency penalty (default: 0.0)\n  -l, --listpatterns         List all patterns\n  -L, --listmodels           List all available models\n  -x, --listcontexts         List all contexts\n  -X, --listsessions         List all sessions\n  -U, --updatepatterns       Update patterns\n  -c, --copy                 Copy to clipboard\n  -m, --model=               Choose model\n  -o, --output=              Output to file\n      --output-session       Output the entire session (also a temporary one) to the output file\n  -n, --latest=              Number of latest patterns to list (default: 0)\n  -d, --changeDefaultModel   Change default model\n  -y, --youtube=             YouTube video \"URL\" to grab transcript, comments from it and send to chat\n      --transcript           Grab transcript from YouTube video and send to chat (it used per default).\n      --comments             Grab comments from YouTube video and send to chat\n      --metadata             Grab metadata from YouTube video and send to chat\n  -g, --language=            Specify the Language Code for the chat, e.g. -g=en -g=zh\n  -u, --scrape_url=          Scrape website URL to markdown using Jina AI\n  -q, --scrape_question=     Search question using Jina AI\n  -e, --seed=                Seed to be used for LMM generation\n  -w, --wipecontext=         Wipe context\n  -W, --wipesession=         Wipe session\n      --printcontext=        Print context\n      --printsession=        Print session\n      --readability          Convert HTML input into a clean, readable view\n      --serve                Initiate the API server\n      --dry-run              Show what would be sent to the model without actually sending it\n      --version              Print current version\n\nHelp Options:\n  -h, --help                 Show this help message\n\n```\n\n## Our approach to prompting\n\nFabric _Patterns_ are different than most prompts you'll see.\n\n- **First, we use `Markdown` to help ensure maximum readability and editability**. This not only helps the creator make a good one, but also anyone who wants to deeply understand what it does. _Importantly, this also includes the AI you're sending it to!_\n\nHere's an example of a Fabric Pattern.\n\n```bash\nhttps://github.com/danielmiessler/fabric/blob/main/patterns/extract_wisdom/system.md\n```\n\n<img width=\"1461\" alt=\"pattern-example\" src=\"https://github.com/danielmiessler/fabric/assets/50654/b910c551-9263-405f-9735-71ca69bbab6d\">\n\n- **Next, we are extremely clear in our instructions**, and we use the Markdown structure to emphasize what we want the AI to do, and in what order.\n\n- **And finally, we tend to use the System section of the prompt almost exclusively**. In over a year of being heads-down with this stuff, we've just seen more efficacy from doing that. If that changes, or we're shown data that says otherwise, we will adjust.\n\n## Examples\n\n> The following examples use the macOS `pbpaste` to paste from the clipboard. See the [pbpaste](#pbpaste) section below for Windows and Linux alternatives.\n\nNow let's look at some things you can do with Fabric.\n\n1. Run the `summarize` Pattern based on input from `stdin`. In this case, the body of an article.\n\n```bash\npbpaste | fabric --pattern summarize\n```\n\n2. Run the `analyze_claims` Pattern with the `--stream` option to get immediate and streaming results.\n\n```bash\npbpaste | fabric --stream --pattern analyze_claims\n```\n\n3. Run the `extract_wisdom` Pattern with the `--stream` option to get immediate and streaming results from any Youtube video (much like in the original introduction video).\n\n```bash\nfabric -y \"https://youtube.com/watch?v=uXs-zPc63kM\" --stream --pattern extract_wisdom\n```\n\n\n4. Create patterns- you must create a .md file with the pattern and save it to `~/.config/fabric/patterns/[yourpatternname]`.\n\n\n5. Run a `analyze_claims` pattern on a website. Fabric uses Jina AI to scrape the URL into markdown format before sending it to the model.\n\n```bash\nfabric -u https://github.com/danielmiessler/fabric/ -p analyze_claims\n```\n\n## Just use the Patterns\n\n<img width=\"1173\" alt=\"fabric-patterns-screenshot\" src=\"https://github.com/danielmiessler/fabric/assets/50654/9186a044-652b-4673-89f7-71cf066f32d8\">\n\n<br />\n<br />\n\nIf you're not looking to do anything fancy, and you just want a lot of great prompts, you can navigate to the [`/patterns`](https://github.com/danielmiessler/fabric/tree/main/patterns) directory and start exploring!\n\nWe hope that if you used nothing else from Fabric, the Patterns by themselves will make the project useful.\n\nYou can use any of the Patterns you see there in any AI application that you have, whether that's ChatGPT or some other app or website. Our plan and prediction is that people will soon be sharing many more than those we've published, and they will be way better than ours.\n\nThe wisdom of crowds for the win.\n\n## Custom Patterns\n\nYou may want to use Fabric to create your own custom Patterns\u2014but not share them with others. No problem!\n\nJust make a directory in `~/.config/custompatterns/` (or wherever) and put your `.md` files in there.\n\nWhen you're ready to use them, copy them into:\n\n```\n~/.config/fabric/patterns/\n```\n\nYou can then use them like any other Patterns, but they won't be public unless you explicitly submit them as Pull Requests to the Fabric project. So don't worry\u2014they're private to you.\n\n\n## Helper Apps\n\nFabric also makes use of some core helper apps (tools) to make it easier to integrate with your various workflows. Here are some examples:\n\n### `to_pdf`\n\n`to_pdf` is a helper command that converts LaTeX files to PDF format. You can use it like this:\n\n```bash\nto_pdf input.tex\n```\n\nThis will create a PDF file from the input LaTeX file in the same directory.\n\nYou can also use it with stdin which works perfectly with the `write_latex` pattern:\n\n```bash\necho \"ai security primer\" | fabric --pattern write_latex | to_pdf\n```\n\nThis will create a PDF file named `output.pdf` in the current directory.\n\n### `to_pdf` Installation\n\nTo install `to_pdf`, install it the same way as you install Fabric, just with a different repo name.\n\n```bash\ngo install github.com/danielmiessler/fabric/plugins/tools/to_pdf@latest\n```\n\nMake sure you have a LaTeX distribution (like TeX Live or MiKTeX) installed on your system, as `to_pdf` requires `pdflatex` to be available in your system's PATH.\n\n## pbpaste\n\nThe [examples](#examples) use the macOS program `pbpaste` to paste content from the clipboard to pipe into `fabric` as the input. `pbpaste` is not available on Windows or Linux, but there are alternatives.\n\nOn Windows, you can use the PowerShell command `Get-Clipboard` from a PowerShell command prompt. If you like, you can also alias it to `pbpaste`. If you are using classic PowerShell, edit the file `~\\Documents\\WindowsPowerShell\\.profile.ps1`, or if you are using PowerShell Core, edit `~\\Documents\\PowerShell\\.profile.ps1` and add the alias,\n\n```powershell\nSet-Alias pbpaste Get-Clipboard\n```\n\nOn Linux, you can use `xclip -selection clipboard -o` to paste from the clipboard. You will likely need to install `xclip` with your package manager. For Debian based systems including Ubuntu,\n\n```sh\nsudo apt update\nsudo apt install xclip -y\n```\n\nYou can also create an alias by editing `~/.bashrc` or `~/.zshrc` and adding the alias,\n\n```sh\nalias pbpaste='xclip -selection clipboard -o'\n```\n\n## Web Interface\n\nFabric now includes a built-in web interface that provides a GUI alternative to the command-line interface and an out-of-the-box website for those who want to get started with web development or blogging.  \nYou can use this app as a GUI interface for Fabric, a ready to go blog-site, or a website template for your own projects.\n\nThe `web/src/lib/content` directory includes starter `.obsidian/` and `templates/` directories, allowing you to open up the `web/src/lib/content/` directory as an [Obsidian.md](https://obsidian.md) vault. You can place your posts in the posts directory when you're ready to publish.\n\n### Installing\n\nThe GUI can be installed by navigating to the\u00a0`web`\u00a0directory and using\u00a0`npm install`,\u00a0`pnpm install`, or your favorite package manager. Then simply run\u00a0the development server to start the app.\n\n_You will need to run fabric in a separate terminal with the\u00a0`fabric --serve`\u00a0command._\n\n**From the fabric project `web/` directory:**\n\n```shell\nnpm run dev\n\n## or ##\n\npnpm run dev\n\n## or your equivalent\n```\n\n### Streamlit UI\n\nTo run the Streamlit user interface:\n\n```bash\n# Install required dependencies\npip install streamlit pandas matplotlib seaborn numpy python-dotenv\n\n# Run the Streamlit app\nstreamlit run streamlit.py\n```\n\nThe Streamlit UI provides a user-friendly interface for:\n\n- Running and chaining patterns\n- Managing pattern outputs\n- Creating and editing patterns\n- Analyzing pattern results\n\n## Meta\n\n> [!NOTE]\n> Special thanks to the following people for their inspiration and contributions!\n\n- _Jonathan Dunn_ for being the absolute MVP dev on the project, including spearheading the new Go version, as well as the GUI! All this while also being a full-time medical doctor!\n- _Caleb Sima_ for pushing me over the edge of whether to make this a public project or not.\n- _Eugen Eisler_ and _Frederick Ros_ for their invaluable contributions to the Go version\n- _David Peters_ for his work on the web interface.\n- _Joel Parish_ for super useful input on the project's Github directory structure..\n- _Joseph Thacker_ for the idea of a `-c` context flag that adds pre-created context in the `./config/fabric/` directory to all Pattern queries.\n- _Jason Haddix_ for the idea of a stitch (chained Pattern) to filter content using a local model before sending on to a cloud model, i.e., cleaning customer data using `llama2` before sending on to `gpt-4` for analysis.\n- _Andre Guerra_ for assisting with numerous components to make things simpler and more maintainable.\n\n### Primary contributors\n\n<a href=\"https://github.com/danielmiessler\"><img src=\"https://avatars.githubusercontent.com/u/50654?v=4\" title=\"Daniel Miessler\" width=\"50\" height=\"50\"></a>\n<a href=\"https://github.com/xssdoctor\"><img src=\"https://avatars.githubusercontent.com/u/9218431?v=4\" title=\"Jonathan Dunn\" width=\"50\" height=\"50\"></a>\n<a href=\"https://github.com/sbehrens\"><img src=\"https://avatars.githubusercontent.com/u/688589?v=4\" title=\"Scott Behrens\" width=\"50\" height=\"50\"></a>\n<a href=\"https://github.com/agu3rra\"><img src=\"https://avatars.githubusercontent.com/u/10410523?v=4\" title=\"Andre Guerra\" width=\"50\" height=\"50\"></a>\n\n`fabric` was created by <a href=\"https://danielmiessler.com/subscribe\" target=\"_blank\">Daniel Miessler</a> in January of 2024.\n<br /><br />\n<a href=\"https://twitter.com/intent/user?screen_name=danielmiessler\">![X (formerly Twitter) Follow](https://img.shields.io/twitter/follow/danielmiessler)</a>\n"
  },
  {
    "name": "buster",
    "url": "https://github.com/davidatoms/buster",
    "description": "The open-source, AI-native data stack",
    "type": "fork",
    "updated_at": "2025-03-01T07:50:18Z",
    "readme": "![Buster GitHub Banner](/assets/image.png)\n\n<div align=\"center\"><h1>The Buster Platform</h1></div>\n<div align=\"center\"><h4>A modern analytics platform for AI-powered data applications</h4></div>\n\n<div align=\"center\">\n   <div>\n      <h3>\n         <a href=\"https://www.buster.so/get-started\">\n            <strong>Sign up</strong>\n         </a> \u00b7 \n         <a href=\"#quickstart\">\n            <strong>Quickstart</strong>\n         </a> \u00b7 \n         <a href=\"#deployment\">\n            <strong>Deployment</strong>\n         </a>\n      </h3>\n   </div>\n\n   <div>\n      <a href=\"https://github.com/buster-so/warehouse/blob/main/LICENSE\"><img alt=\"MIT License\" src=\"https://img.shields.io/badge/License-MIT-red.svg?style=flat-square\" ></a>\n      <a href=\"https://www.ycombinator.com/companies/buster\"><img src=\"https://img.shields.io/badge/Y%20Combinator-W24-orange?style=flat-square\" alt=\"Y Combinator W24\"></a>\n   </div>\n</div>\n</br>\n\n## What is Buster?\n\nBuster is a modern analytics platform built from the ground up with AI in mind.\n\nWe've spent the last two years working with companies to help them implement Large Language Models in their data stack.  This has mainly revolved around truly self-serve experiences that are powered by Large Language Models.  We've noticed a few pain points when it comes to the tools that are available today:\n\n1. Slapping an AI copilot on top of existing BI tools can often result in a subpar experience for users. To deploy a powerful analytics experience, we believe that the entire app needs to be built from the ground up with AI in mind. \n2. Most organizations can't deploy ad-hoc, self-serve experiences for their users because their warehousing costs/performance are too prohibitive.  We believe that new storage formats like Apache Iceberg and query engines like Starrocks and DuckDB have the potential to change data warehousing and make it more accessible for the type of workloads that come with AI-powered analytics experiences.\n3. The current CI/CD process for most analytics stacks struggle to keep up with changes and often result in broken dashboards, slow query performance, and other issues.  Introducing hundreds, if not thousands of user queries made with Large Language Models can exacerbate these issues and make it nearly impossible to maintain. We believe there is a huge opportunity to rethink how Large Language Models can be used to improve this process with workflows around self-healing, model suggestions, and more.\n4. Current tools don't have tooling or workflows built around augmenting data teams.  They are designed for the analyst to continue working as they did before, instead of helping them build powerful data experiences for their users.  We believe that instead of spending hours and hours building out unfulfilling dashboards, data teams should be empowered to build out powerful, self-serve experiences for their users.\n\nUltimately, we believe that the future of AI analytics is about helping data teams build powerful, self-serve experiences for their users. We think that requires a new approach to the analytics stack.  One that allows for deep integrations between products and allows data teams to truly own their entire experience.\n\n## Roadmap\n\nCurrently, we are in the process of open-sourcing the platform.  This includes:\n\n- [Warehouse](/warehouse) \u2705\n- [BI platform](https://buster.so) \u23f0\n\nAfter that, we will release an official roadmap.\n\n## How We Plan to Make Money\n\nCurrently, we offer a few commercial products:\n\n- [Cloud-Hosted Versions](https://buster.so)\n  - Warehouse\n    - Cluster\n    - Serverless\n  - BI Platform\n- Managed Self-Hosted Version of the Warehouse product.\n\n## Support and feedback\n\nYou can contact us through either:\n\n- [Github Discussions](https://github.com/orgs/buster-so/discussions)\n- Email us at founders at buster dot com\n\n## License\n\nThis repository is MIT licensed, except for the `ee` folders. See [LICENSE](LICENSE) for more details."
  },
  {
    "name": "gnome-calendar",
    "url": "https://github.com/davidatoms/gnome-calendar",
    "description": "Read-only mirror of https://gitlab.gnome.org/GNOME/gnome-calendar",
    "type": "fork",
    "updated_at": "2025-02-26T22:56:53Z",
    "readme": "# GNOME Calendar\n\nGNOME Calendar is a simple and beautiful calendar application for GNOME. We give\na lot of attention to details, and as such, design is an essential and ongoing\neffort.\n\n[![Flatpak](https://flathub.org/api/badge?svg&locale=en)](https://flathub.org/apps/details/org.gnome.Calendar)\n\n\n## Useful links\n\n- Homepage: <https://apps.gnome.org/Calendar/>\n- Report issues: <https://gitlab.gnome.org/GNOME/gnome-calendar/issues/>\n- Donate: <https://www.gnome.org/donate/>\n- Translate: <https://l10n.gnome.org/module/gnome-calendar/>\n"
  }
]