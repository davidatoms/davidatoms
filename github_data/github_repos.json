[
  {
    "name": "lithium-lifecycle",
    "url": "https://github.com/davidatoms/lithium-lifecycle",
    "description": null,
    "type": "original",
    "readme": "# Lithium Research Project\n\n## Overview\n\nThis project is a research project on the lithium industry. It is a collection of notebooks and scripts that I have been using to learn about the lithium industry.\n\n## Structure\n\n- `lithium-lifecycle.mmd`: A mermaid diagram of the lithium lifecycle.\n- `lithium-lifecycle.json`: A json file of the lithium lifecycle.\n- `lithium-lifecycle.html`: A html file of the lithium lifecycle.\n"
  },
  {
    "name": "shiftdating",
    "url": "https://github.com/davidatoms/shiftdating",
    "description": null,
    "type": "original",
    "readme": "# dating_app\n\nA new Flutter project.\n\n## Getting Started\n\nThis project is a starting point for a Flutter application.\n\nA few resources to get you started if this is your first Flutter project:\n\n- [Lab: Write your first Flutter app](https://docs.flutter.dev/get-started/codelab)\n- [Cookbook: Useful Flutter samples](https://docs.flutter.dev/cookbook)\n\nFor help getting started with Flutter development, view the\n[online documentation](https://docs.flutter.dev/), which offers tutorials,\nsamples, guidance on mobile development, and a full API reference.\n"
  },
  {
    "name": "davidatoms",
    "url": "https://github.com/davidatoms/davidatoms",
    "description": null,
    "type": "original",
    "readme": "# DAVID ADAMS\n\n"
  },
  {
    "name": "openfda-backend",
    "url": "https://github.com/davidatoms/openfda-backend",
    "description": null,
    "type": "original",
    "readme": "# OpenFDA backend application\n\nA go-based backend for querying and processing OpenFDA data. This is a data fetch project \\n\nused to fetch the data. \n\n## Feautres\n- Query OpenFDA for medical device, pharmaceutical, and other data\n- Filters\n- Backend API built with Gin\n"
  },
  {
    "name": "davidadams.xyz",
    "url": "https://github.com/davidatoms/davidadams.xyz",
    "description": "Personal website",
    "type": "original",
    "readme": "# davidadams.xyz\nPersonal website\n"
  },
  {
    "name": "heritrix3",
    "url": "https://github.com/davidatoms/heritrix3",
    "description": "Heritrix is the Internet Archive's open-source, extensible, web-scale, archival-quality web crawler project.  ",
    "type": "fork",
    "readme": "# Heritrix\n[![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.archive/heritrix/badge.svg)](https://maven-badges.herokuapp.com/maven-central/org.archive/heritrix)\n[![Docker](https://img.shields.io/docker/v/iipc/heritrix/latest?label=docker)](https://hub.docker.com/r/iipc/heritrix)\n[![Javadoc](https://javadoc.io/badge2/org.archive/heritrix/javadoc.svg)](https://www.javadoc.io/doc/org.archive.heritrix/heritrix-engine)\n[![LICENSE](https://img.shields.io/badge/license-Apache-blue.svg?style=flat-square)](./LICENSE)\n\n## Introduction\n\nHeritrix is the Internet Archive's open-source, extensible, web-scale, archival-quality web crawler project. Heritrix (sometimes spelled heretrix, or misspelled or missaid as heratrix/heritix/heretix/heratix) is an archaic word for heiress (woman who inherits). Since our crawler seeks to collect and preserve the digital artifacts of our culture for the benefit of future researchers and generations, this name seemed apt.\n\n## Crawl Operators!\n\nHeritrix is designed to respect the [`robots.txt`](http://www.robotstxt.org/robotstxt.html) exclusion directives<sup>\u2020</sup> and [META nofollow tags](http://www.robotstxt.org/meta.html). Please consider the\nload your crawl will place on seed sites and set politeness policies accordingly. Also, always identify your crawl with contact information in the `User-Agent` so sites that may be adversely affected by your crawl can contact you or adapt their server behavior accordingly.\n\n<sup>\u2020</sup> The newer wildcard extension to robots.txt is [not yet](https://github.com/internetarchive/heritrix3/issues/250) supported.\n\n## Documentation\n\n- [Getting Started](https://heritrix.readthedocs.io/en/latest/getting-started.html)\n- [Operating Heritrix](https://heritrix.readthedocs.io/en/latest/operating.html)\n- [Configuring Crawl Jobs](https://heritrix.readthedocs.io/en/latest/configuring-jobs.html)\n- [Bean Reference](https://heritrix.readthedocs.io/en/latest/bean-reference.html)\n- [Wiki](https://github.com/internetarchive/heritrix3/wiki)\n\n## Developer Documentation\n\n- [Developer Manual](http://crawler.archive.org/articles/developer_manual/index.html)\n- [REST API documentation](https://heritrix.readthedocs.io/en/latest/api.html)\n- JavaDoc: [engine](https://www.javadoc.io/doc/org.archive.heritrix/heritrix-engine), [modules](https://www.javadoc.io/doc/org.archive.heritrix/heritrix-modules), [commons](https://www.javadoc.io/doc/org.archive.heritrix/heritrix-commons), [contrib](https://www.javadoc.io/doc/org.archive.heritrix/heritrix-contrib)\n\n\n## Latest Releases\n\nInformation about releases can be found [here](https://github.com/internetarchive/heritrix3/wiki#latest-releases).\n\n## License\n\nHeritrix is free software; you can redistribute it and/or modify it under the terms of the [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0)\n\nSome individual source code files are subject to or offered under other licenses. See the included [`LICENSE.txt`](./LICENSE) file for more information.\n\nHeritrix is distributed with the libraries it depends upon. The libraries can be found under the `lib` directory in the release distribution, and are used under the terms of their respective licenses, which are included alongside the libraries in the `lib` directory.\n"
  },
  {
    "name": "Tanuki",
    "url": "https://github.com/davidatoms/Tanuki",
    "description": "A trustless wallet scoring protocol",
    "type": "fork",
    "readme": "# Tanuki\n\n## \ud83c\udfc6 Main Prize Winner ETHGlobal Tokyo\n\nETHGlobal Tokyo Demo: https://ethglobal.com/showcase/tanuki-s8ew9\n\n## Overview\n\nTanuki is a trustless wallet scoring protocol. It uses ZKP\u2019s to trustlessly read historical on-chain activity to calculate wallet scores. Wallet scores are modular, enabling use cases like paymasters with loyalty-based subsidies and lending protocols with variable rates.\n\n## Architecture\n\n![Tanuki Architecture](./architecture.png)\n\n## Use Cases\n\n1. Reputation scores, e.g. [on-chain PageRank](https://twitter.com/brian_armstrong/status/1696923807855042601)\n2. Credit score protocols which offer different terms to users based on their scores\n3. NFT pricing models that consider the wallets' past interactions with the NFT to accurately estimate price.\n4. Paymasters that determine the amount of gas fee reimbursement for each user. Currently, these methods are performed off-chain, resulting in centralized, less transparent, and trust-dependent systems.\n5. Spam prevention on Lens: create a social credit score for accounts, only allow posting / interaction based on certain minimum score\n\n\n### Deployments\n\n#### Polygon Mumbai\n[Twitter Link](https://twitter.com/0xminmi/status/1647453283936780289)\n\n[GovernanceToken](https://mumbai.polygonscan.com/address/0x64845507e9304ea3427440837d5a000D9Aa08fc0)\n\n[SimpleScoreProvider](https://mumbai.polygonscan.com/address/0x3baEf58f49e836d5127a161Ce827ecBa3577B73d)\n\n#### Scroll Alpha\n\n[GovernanceToken](https://blockscout.scroll.io/address/0x17C9378f864f04B6FaFB4783755d14e15c61a3b2)\n\n[SimpleScoreProvider](https://blockscout.scroll.io/address/0xa23cD8e97730C4B099576a24816e6f9BCa2A395a)G\n\n#### Taiko Hackathon Network\n\n[GovernanceToken](https://l2explorer.hackathon.taiko.xyz/address/0xa23cD8e97730C4B099576a24816e6f9BCa2A395a)\n\n[SimpleScoreProvider](https://l2explorer.hackathon.taiko.xyz/address/0xb3fdBFf207be9d12006C7eaE6635226280DE8556)\n\n\n#### Mantle Test Net deployments\n\n[GovernanceToken](https://explorer.testnet.mantle.xyz/address/0x64845507e9304ea3427440837d5a000D9Aa08fc0)\n\n[SimpleScoreProvider](hhttps://explorer.testnet.mantle.xyz/address/0x3baEf58f49e836d5127a161Ce827ecBa3577B73d)\n\n[SimpleScoreProvider](hhttps://explorer.testnet.mantle.xyz/address/0xd5bBa668908C2AB9c9926A505A24C368e401181a)\n"
  },
  {
    "name": "exo",
    "url": "https://github.com/davidatoms/exo",
    "description": "Run your own AI cluster at home with everyday devices \ud83d\udcf1\ud83d\udcbb \ud83d\udda5\ufe0f\u231a",
    "type": "fork",
    "readme": "<div align=\"center\">\n\n<picture>\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"/docs/exo-logo-black-bg.jpg\">\n  <img alt=\"exo logo\" src=\"/docs/exo-logo-transparent.png\" width=\"50%\" height=\"50%\">\n</picture>\n\nexo: Run your own AI cluster at home with everyday devices. Maintained by [exo labs](https://x.com/exolabs).\n\n\n<h3>\n\n[Discord](https://discord.gg/EUnjGpsmWw) | [Telegram](https://t.me/+Kh-KqHTzFYg3MGNk) | [X](https://x.com/exolabs)\n\n</h3>\n\n[![GitHub Repo stars](https://img.shields.io/github/stars/exo-explore/exo)](https://github.com/exo-explore/exo/stargazers)\n[![Tests](https://dl.circleci.com/status-badge/img/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main.svg?style=svg)](https://dl.circleci.com/status-badge/redirect/circleci/TrkofJDoGzdQAeL6yVHKsg/4i5hJuafuwZYZQxbRAWS71/tree/main)\n[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)\n\n<a href=\"https://trendshift.io/repositories/11849\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/11849\" alt=\"exo-explore%2Fexo | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n</div>\n\n---\n\nUnify your existing devices into one powerful GPU: iPhone, iPad, Android, Mac, NVIDIA, Raspberry Pi, pretty much any device!\n\n<div align=\"center\">\n  <h2>Update: exo is hiring. See <a href=\"https://exolabs.net\">here</a> for more details.</h2>\n  <h2>Interested in running exo in your business? <a href=\"mailto:hello@exolabs.net\">Contact us</a> to discuss.</h2>\n</div>\n\n## Get Involved\n\nexo is **experimental** software. Expect bugs early on. Create issues so they can be fixed. The [exo labs](https://x.com/exolabs) team will strive to resolve issues quickly.\n\nWe also welcome contributions from the community. We have a list of bounties in [this sheet](https://docs.google.com/spreadsheets/d/1cTCpTIp48UnnIvHeLEUNg1iMy_Q6lRybgECSFCoVJpE/edit?usp=sharing).\n\n## Features\n\n### Wide Model Support\n\nexo supports different models including LLaMA ([MLX](exo/inference/mlx/models/llama.py) and [tinygrad](exo/inference/tinygrad/models/llama.py)), Mistral, LlaVA, Qwen, and Deepseek.\n\n### Dynamic Model Partitioning\n\nexo [optimally splits up models](exo/topology/ring_memory_weighted_partitioning_strategy.py) based on the current network topology and device resources available. This enables you to run larger models than you would be able to on any single device.\n\n### Automatic Device Discovery\n\nexo will [automatically discover](https://github.com/exo-explore/exo/blob/945f90f676182a751d2ad7bcf20987ab7fe0181e/exo/orchestration/node.py#L154) other devices using the best method available. Zero manual configuration.\n\n### ChatGPT-compatible API\n\nexo provides a [ChatGPT-compatible API](exo/api/chatgpt_api.py) for running models. It's a [one-line change](examples/chatgpt_api.sh) in your application to run models on your own hardware using exo.\n\n### Device Equality\n\nUnlike other distributed inference frameworks, exo does not use a master-worker architecture. Instead, exo devices [connect p2p](https://github.com/exo-explore/exo/blob/945f90f676182a751d2ad7bcf20987ab7fe0181e/exo/orchestration/node.py#L161). As long as a device is connected somewhere in the network, it can be used to run models.\n\nExo supports different [partitioning strategies](exo/topology/partitioning_strategy.py) to split up a model across devices. The default partitioning strategy is [ring memory weighted partitioning](exo/topology/ring_memory_weighted_partitioning_strategy.py). This runs an inference in a ring where each device runs a number of model layers proportional to the memory of the device.\n\n![\"A screenshot of exo running 5 nodes](docs/exo-screenshot.jpg)\n\n## Installation\n\nThe current recommended way to install exo is from source.\n\n### Prerequisites\n\n- Python>=3.12.0 is required because of [issues with asyncio](https://github.com/exo-explore/exo/issues/5) in previous versions.\n- For Linux with NVIDIA GPU support (Linux-only, skip if not using Linux or NVIDIA):\n  - NVIDIA driver - verify with `nvidia-smi`\n  - CUDA toolkit - install from [NVIDIA CUDA guide](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#cuda-cross-platform-installation), verify with `nvcc --version`\n  - cuDNN library - download from [NVIDIA cuDNN page](https://developer.nvidia.com/cudnn-downloads), verify installation by following [these steps](https://docs.nvidia.com/deeplearning/cudnn/latest/installation/linux.html#verifying-the-install-on-linux:~:text=at%20a%20time.-,Verifying%20the%20Install%20on%20Linux,Test%20passed!,-Upgrading%20From%20Older)\n\n### Hardware Requirements\n\n- The only requirement to run exo is to have enough memory across all your devices to fit the entire model into memory. For example, if you are running llama 3.1 8B (fp16), you need 16GB of memory across all devices. Any of the following configurations would work since they each have more than 16GB of memory in total:\n  - 2 x 8GB M3 MacBook Airs\n  - 1 x 16GB NVIDIA RTX 4070 Ti Laptop\n  - 2 x Raspberry Pi 400 with 4GB of RAM each (running on CPU) + 1 x 8GB Mac Mini\n- exo is designed to run on devices with heterogeneous capabilities. For example, you can have some devices with powerful GPUs and others with integrated GPUs or even CPUs. Adding less capable devices will slow down individual inference latency but will increase the overall throughput of the cluster.\n\n### From source\n\n\n```sh\ngit clone https://github.com/exo-explore/exo.git\ncd exo\npip install -e .\n# alternatively, with venv\nsource install.sh\n```\n\n\n### Troubleshooting\n\n- If running on Mac, MLX has an [install guide](https://ml-explore.github.io/mlx/build/html/install.html) with troubleshooting steps.\n\n### Performance\n\n- There are a number of things users have empirically found to improve performance on Apple Silicon Macs:\n\n1. Upgrade to the latest version of macOS Sequoia.\n2. Run `./configure_mlx.sh`. This runs commands to optimize GPU memory allocation on Apple Silicon Macs.\n\n\n## Documentation\n\n### Example Usage on Multiple macOS Devices\n\n#### Device 1:\n\n```sh\nexo\n```\n\n#### Device 2:\n```sh\nexo\n```\n\nThat's it! No configuration required - exo will automatically discover the other device(s).\n\nexo starts a ChatGPT-like WebUI (powered by [tinygrad tinychat](https://github.com/tinygrad/tinygrad/tree/master/examples/tinychat)) on http://localhost:52415\n\nFor developers, exo also starts a ChatGPT-compatible API endpoint on http://localhost:52415/v1/chat/completions. Examples with curl:\n\n#### Llama 3.2 3B:\n\n```sh\ncurl http://localhost:52415/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n     \"model\": \"llama-3.2-3b\",\n     \"messages\": [{\"role\": \"user\", \"content\": \"What is the meaning of exo?\"}],\n     \"temperature\": 0.7\n   }'\n```\n\n#### Llama 3.1 405B:\n\n```sh\ncurl http://localhost:52415/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n     \"model\": \"llama-3.1-405b\",\n     \"messages\": [{\"role\": \"user\", \"content\": \"What is the meaning of exo?\"}],\n     \"temperature\": 0.7\n   }'\n```\n\n#### DeepSeek R1 (full 671B):\n\n```sh\ncurl http://localhost:52415/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n     \"model\": \"deepseek-r1\",\n     \"messages\": [{\"role\": \"user\", \"content\": \"What is the meaning of exo?\"}],\n     \"temperature\": 0.7\n   }'\n```\n\n#### Llava 1.5 7B (Vision Language Model):\n\n```sh\ncurl http://localhost:52415/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n     \"model\": \"llava-1.5-7b-hf\",\n     \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"type\": \"text\",\n            \"text\": \"What are these?\"\n          },\n          {\n            \"type\": \"image_url\",\n            \"image_url\": {\n              \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n            }\n          }\n        ]\n      }\n    ],\n     \"temperature\": 0.0\n   }'\n```\n\n### Example Usage on Multiple Heterogenous Devices (macOS + Linux)\n\n#### Device 1 (macOS):\n\n```sh\nexo\n```\n\nNote: We don't need to explicitly tell exo to use the **tinygrad** inference engine. **MLX** and **tinygrad** are interoperable!\n\n#### Device 2 (Linux):\n```sh\nexo\n```\n\nLinux devices will automatically default to using the **tinygrad** inference engine.\n\nYou can read about tinygrad-specific env vars [here](https://docs.tinygrad.org/env_vars/). For example, you can configure tinygrad to use the cpu by specifying `CLANG=1`.\n\n### Example Usage on a single device with \"exo run\" command\n\n```sh\nexo run llama-3.2-3b\n```\n\nWith a custom prompt:\n\n```sh\nexo run llama-3.2-3b --prompt \"What is the meaning of exo?\"\n```\n\n### Model Storage\n\nModels by default are stored in `~/.cache/exo/downloads`.\n\nYou can set a different model storage location by setting the `EXO_HOME` env var.\n\n## Model Downloading\n\nModels are downloaded from Hugging Face. If you are running exo in a country with strict internet censorship, you may need to download the models manually and put them in the `~/.cache/exo/downloads` directory.\n\nTo download models from a proxy endpoint, set the `HF_ENDPOINT` environment variable. For example, to run exo with the huggingface mirror endpoint:\n\n```sh\nHF_ENDPOINT=https://hf-mirror.com exo\n```\n\n## Debugging\n\nEnable debug logs with the DEBUG environment variable (0-9).\n\n```sh\nDEBUG=9 exo\n```\n\nFor the **tinygrad** inference engine specifically, there is a separate DEBUG flag `TINYGRAD_DEBUG` that can be used to enable debug logs (1-6).\n\n```sh\nTINYGRAD_DEBUG=2 exo\n```\n\n## Formatting\n\nWe use [yapf](https://github.com/google/yapf) to format the code. To format the code, first install the formatting requirements:\n\n```sh\npip3 install -e '.[formatting]'\n```\n\nThen run the formatting script:\n\n```sh\npython3 format.py ./exo\n```\n\n## Known Issues\n\n- On certain versions of Python on macOS, certificates may not installed correctly, potentially causing SSL errors (e.g., when accessing huggingface.co). To resolve this, run the `Install Certificates` command, typicall as follows:\n\n```sh\n/Applications/Python 3.x/Install Certificates.command\n```\n\n- \ud83d\udea7 As the library is evolving so quickly, the iOS implementation has fallen behind Python. We have decided for now not to put out the buggy iOS version and receive a bunch of GitHub issues for outdated code. We are working on solving this properly and will make an announcement when it's ready. If you would like access to the iOS implementation now, please email alex@exolabs.net with your GitHub username explaining your use-case and you will be granted access on GitHub.\n\n## Inference Engines\n\nexo supports the following inference engines:\n\n- \u2705 [MLX](exo/inference/mlx/sharded_inference_engine.py)\n- \u2705 [tinygrad](exo/inference/tinygrad/inference.py)\n- \ud83d\udea7 [PyTorch](https://github.com/exo-explore/exo/pull/139)\n- \ud83d\udea7 [llama.cpp](https://github.com/exo-explore/exo/issues/167)\n\n## Discovery Modules\n\n- \u2705 [UDP](exo/networking/udp)\n- \u2705 [Manual](exo/networking/manual)\n- \u2705 [Tailscale](exo/networking/tailscale)\n- \ud83d\udea7 [Radio](TODO)\n- \ud83d\udea7 [Bluetooth](TODO)\n\n# Peer Networking Modules\n\n- \u2705 [GRPC](exo/networking/grpc)\n- \ud83d\udea7 [NCCL](TODO)\n"
  },
  {
    "name": "graphiti",
    "url": "https://github.com/davidatoms/graphiti",
    "description": "Build and query dynamic, temporally-aware Knowledge Graphs",
    "type": "fork",
    "readme": "<div align=\"center\">\n\n# Graphiti\n\n## Temporal Knowledge Graphs for Agentic Applications\n\n<br />\n\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/getzep/Graphiti)\n\n<br />\n\n</div>\n\nGraphiti builds dynamic, temporally aware Knowledge Graphs that represent complex, evolving relationships between\nentities over time. Graphiti ingests both unstructured and structured data, and the resulting graph may be queried using\na fusion of time, full-text, semantic, and graph algorithm approaches.\n\n<br />\n\n<p align=\"center\">\n    <img src=\"images/graphiti-graph-intro.gif\" alt=\"Graphiti temporal walkthrough\" width=\"700px\">   \n</p>\n\n<br />\n\nGraphiti helps you create and query Knowledge Graphs that evolve over time. A knowledge graph is a network of\ninterconnected facts, such as _\u201cKendra loves Adidas shoes.\u201d_ Each fact is a \u201ctriplet\u201d represented by two entities, or\nnodes (_\u201dKendra\u201d_, _\u201cAdidas shoes\u201d_), and their relationship, or edge (_\u201dloves\u201d_). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.\n\nWith Graphiti, you can build LLM applications such as:\n\n- Assistants that learn from user interactions, fusing personal knowledge with dynamic data from business systems like\n  CRMs and billing platforms.\n- Agents that autonomously execute complex tasks, reasoning with state changes from multiple dynamic sources.\n\nGraphiti supports a wide range of applications in sales, customer service, health, finance, and more, enabling long-term\nrecall and state-based reasoning for both assistants and agents.\n\n## Graphiti and Zep Memory\n\nGraphiti powers the core of [Zep's memory layer](https://www.getzep.com) for LLM-powered Assistants and Agents.\n\nUsing Graphiti, we've demonstrated Zep is the [State of the Art in Agent Memory](https://blog.getzep.com/state-of-the-art-agent-memory/).\n\nRead our paper: [Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956).\n\nWe're excited to open-source Graphiti, believing its potential reaches far beyond memory applications.\n\n<p align=\"center\">\n    <a href=\"https://arxiv.org/abs/2501.13956\"><img src=\"images/arxiv-screenshot.png\" alt=\"Zep: A Temporal Knowledge Graph Architecture for Agent Memory\" width=\"700px\"></a>\n</p>\n\n## Why Graphiti?\n\nWe were intrigued by Microsoft\u2019s GraphRAG, which expanded on RAG text chunking by using a graph to better model a\ndocument corpus and making this representation available via semantic and graph search techniques. However, GraphRAG did\nnot address our core problem: It's primarily designed for static documents and doesn't inherently handle temporal\naspects of data.\n\nGraphiti is designed from the ground up to handle constantly changing information, hybrid semantic and graph search, and\nscale:\n\n- **Temporal Awareness:** Tracks changes in facts and relationships over time, enabling point-in-time queries. Graph\n  edges include temporal metadata to record relationship lifecycles.\n- **Episodic Processing:** Ingests data as discrete episodes, maintaining data provenance and allowing incremental\n  entity and relationship extraction.\n- **Hybrid Search:** Combines semantic and BM25 full-text search, with the ability to rerank results by distance from a\n  central node e.g. \u201cKendra\u201d.\n- **Scalable:** Designed for processing large datasets, with parallelization of LLM calls for bulk processing while\n  preserving the chronology of events.\n- **Supports Varied Sources:** Can ingest both unstructured text and structured JSON data.\n\n<p align=\"center\">\n    <img src=\"/images/graphiti-intro-slides-stock-2.gif\" alt=\"Graphiti structured + unstructured demo\" width=\"700px\">   \n</p>\n\n## Installation\n\nRequirements:\n\n- Python 3.10 or higher\n- Neo4j 5.21 or higher\n- OpenAI API key (for LLM inference and embedding)\n\nOptional:\n\n- Anthropic or Groq API key (for alternative LLM providers)\n\n> [!TIP]\n> The simplest way to install Neo4j is via [Neo4j Desktop](https://neo4j.com/download/). It provides a user-friendly\n> interface to manage Neo4j instances and databases.\n\n```bash\npip install graphiti-core\n```\n\nor\n\n```bash\npoetry add graphiti-core\n```\n\n## Quick Start\n\n> [!IMPORTANT]\n> Graphiti uses OpenAI for LLM inference and embedding. Ensure that an `OPENAI_API_KEY` is set in your environment.\n> Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI compatible APIs.\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.nodes import EpisodeType\nfrom datetime import datetime, timezone\n\n# Initialize Graphiti\ngraphiti = Graphiti(\"bolt://localhost:7687\", \"neo4j\", \"password\")\n\n# Initialize the graph database with Graphiti's indices. This only needs to be done once.\ngraphiti.build_indices_and_constraints()\n\n# Add episodes\nepisodes = [\n    \"Kamala Harris is the Attorney General of California. She was previously \"\n    \"the district attorney for San Francisco.\",\n    \"As AG, Harris was in office from January 3, 2011 \u2013 January 3, 2017\",\n]\nfor i, episode in enumerate(episodes):\n    await graphiti.add_episode(\n        name=f\"Freakonomics Radio {i}\",\n        episode_body=episode,\n        source=EpisodeType.text,\n        source_description=\"podcast\",\n        reference_time=datetime.now(timezone.utc)\n    )\n\n# Search the graph\n# Execute a hybrid search combining semantic similarity and BM25 retrieval\n# Results are combined and reranked using Reciprocal Rank Fusion\nresults = await graphiti.search('Who was the California Attorney General?')\n[\n    EntityEdge(\n\u2502   uuid = '3133258f738e487383f07b04e15d4ac0',\n\u2502   source_node_uuid = '2a85789b318d4e418050506879906e62',\n\u2502   target_node_uuid = 'baf7781f445945989d6e4f927f881556',\n\u2502   created_at = datetime.datetime(2024, 8, 26, 13, 13, 24, 861097),\n\u2502   name = 'HELD_POSITION',\n# the fact reflects the updated state that Harris is\n# no longer the AG of California\n\u2502   fact = 'Kamala Harris was the Attorney General of California',\n\u2502   fact_embedding = [\n\u2502   \u2502   -0.009955154731869698,\n\u2502       ...\n\u2502   \u2502   0.00784289836883545\n\u2502],\n\u2502   episodes = ['b43e98ad0a904088a76c67985caecc22'],\n\u2502   expired_at = datetime.datetime(2024, 8, 26, 20, 18, 1, 53812),\n# These dates represent the date this edge was true.\n\u2502   valid_at = datetime.datetime(2011, 1, 3, 0, 0, tzinfo= < UTC >),\n\u2502   invalid_at = datetime.datetime(2017, 1, 3, 0, 0, tzinfo= < UTC >)\n)\n]\n\n# Rerank search results based on graph distance\n# Provide a node UUID to prioritize results closer to that node in the graph.\n# Results are weighted by their proximity, with distant edges receiving lower scores.\nawait graphiti.search('Who was the California Attorney General?', center_node_uuid)\n\n# Close the connection\ngraphiti.close()\n```\n\n## Graph Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j's parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Documentation\n\n- [Guides and API documentation](https://help.getzep.com/graphiti).\n- [Quick Start](https://help.getzep.com/graphiti/graphiti/quick-start)\n- [Building an agent with LangChain's LangGraph and Graphiti](https://help.getzep.com/graphiti/graphiti/lang-graph-agent)\n\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [ ] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!\n"
  },
  {
    "name": "zep-python",
    "url": "https://github.com/davidatoms/zep-python",
    "description": "Zep: Long-Term Memory for \u200dAI Assistants (Python Client)",
    "type": "fork",
    "readme": "\n[![Release to PyPI](https://github.com/getzep/zep-python/actions/workflows/release.yml/badge.svg)](https://github.com/getzep/zep-python/actions/workflows/release.yml) ![GitHub](https://img.shields.io/github/license/getzep/zep-python?color=blue) [![fern shield](https://img.shields.io/badge/%F0%9F%8C%BF-SDK%20generated%20by%20Fern-brightgreen)](https://github.com/fern-api/fern)\n\n\n<p align=\"center\">\n  <a href=\"https://www.getzep.com/\">\n    <img src=\"https://raw.githubusercontent.com/getzep/zep/main/assets/zep-logo-icon-gradient-rgb.svg\" width=\"150\" alt=\"Zep Logo\">\n  </a>\n</p>\n\n<h1 align=\"center\">\nZep: Long-Term Memory for \u200dAI Assistants.\n</h1>\n<h2 align=\"center\">Recall, understand, and extract data from chat histories. Power personalized AI experiences.</h2>\n<br />\n\n<p align=\"center\">\n<a href=\"https://docs.getzep.com/deployment/quickstart/\">Quick Start</a> | \n<a href=\"https://docs.getzep.com/\">Documentation</a> | \n<a href=\"https://docs.getzep.com/sdk/langchain/\">LangChain</a> and \n<a href=\"https://docs.getzep.com/sdk/langchain/\">LlamaIndex</a> Support | \n<a href=\"https://discord.gg/W8Kw6bsgXQ\">Discord</a><br />\n<a href=\"https://www.getzep.com\">www.getzep.com</a>\n</p>\n\n## What is Zep? \ud83d\udcac\nZep is a long-term memory service for AI Assistant apps. With Zep, you can provide AI assistants with the ability to recall past conversations, no matter how distant, while also reducing hallucinations, latency, and cost.\n\n### Cloud Installation\nYou can install the Zep Cloud SDK by running:\n```bash\npip install zep-cloud\n```\n> [!NOTE]\n> Zep Cloud [overview](https://help.getzep.com/concepts) and [cloud sdk guide](https://help.getzep.com/sdks).\n\n### Community Installation\n```bash\npip install zep-python\n```\n> [!NOTE]\n> Zep Community Edition [quick start](https://help.getzep.com/ce/quickstart) and [sdk guide](https://help.getzep.com/ce/sdks).\n\n### Zep v0.x Compatible SDK\nYou can install Zep v0.x compatible sdk by running:\n```bash\npip install \"zep-python>=1.5.0,<2.0.0\"\n```\n> [!NOTE]\n> Zep v0.x [quick start](https://help.getzep.com/ce/legacy/deployment/quickstart) and [sdk guide](https://help.getzep.com/ce/legacy/sdk).\n\n### How Zep works\n\nZep persists and recalls chat histories, and automatically generates summaries and other artifacts from these chat histories. It also embeds messages and summaries, enabling you to search Zep for relevant context from past conversations. Zep does all of this asynchronously, ensuring these operations don't impact your user's chat experience. Data is persisted to database, allowing you to scale out when growth demands.\n\nZep also provides a simple, easy to use abstraction for document vector search called Document Collections. This is designed to complement Zep's core memory features, but is not designed to be a general purpose vector database.\n\nZep allows you to be more intentional about constructing your prompt:\n1. automatically adding a few recent messages, with the number customized for your app;\n2. a summary of recent conversations prior to the messages above;\n3. and/or contextually relevant summaries or messages surfaced from the entire chat session.\n4. and/or relevant Business data from Zep Document Collections.\n\nZep Cloud offers:\n- **Fact Extraction:** Automatically build fact tables from conversations, without having to define a data schema upfront.\n- **Dialog Classification:** Instantly and accurately classify chat dialog. Understand user intent and emotion, segment users, and more. Route chains based on semantic context, and trigger events.\n- **Structured Data Extraction:** Quickly extract business data from chat conversations using a schema you define. Understand what your Assistant should ask for next in order to complete its task.\n\nYou will also need to provide a Zep Project API key to your zep client.\nYou can find out about zep projects in our [cloud docs](https://help.getzep.com/projects.html)\n\n### Using LangChain Zep Classes with `zep-python`\n\n(Currently only available on release candidate versions)\n\nIn the pre-release version `zep-python` sdk comes with `ZepChatMessageHistory` and `ZepVectorStore`\nclasses that are compatible with [LangChain's Python expression language](https://python.langchain.com/docs/expression_language/)\n\nIn order to use these classes in your application, you need to make sure that you have\n`langchain_core` package installed, please refer to [Langchain's docs installation section](https://python.langchain.com/docs/get_started/installation#langchain-core).\n\nWe support `langchain_core@>=0.1.3<0.2.0`\n\nYou can import these classes in the following way:\n\n```python\nfrom zep_cloud.langchain import ZepChatMessageHistory, ZepVectorStore\n```\n\n### Running Examples\nYou will need to set the following environment variables to run examples in the `examples` directory:\n\n```dotenv\n# Please use examples/.env.example as a template for .env file\n\n# Required\nZEP_API_KEY=<zep-project-api-key># Your Zep Project API Key\nZEP_COLLECTION=<zep-collection-name># used in ingestion script and in vector store examples\nOPENAI_API_KEY=<openai-api-key># Your OpenAI API Key\n\n# Optional (If you want to use langsmith with LangServe Sample App)\nLANGCHAIN_TRACING_V2=true\nLANGCHAIN_API_KEY=<your-langchain-api-key>\nLANGCHAIN_PROJECT=<your-langchain-project-name># If not specified, defaults to \"default\"\n```\n\n\n"
  }
]