[
  {
    "name": "browser-use",
    "url": "https://github.com/davidatoms/browser-use",
    "description": "\ud83c\udf10 Make websites accessible for AI agents. Automate tasks online with ease.",
    "type": "fork",
    "updated_at": "2025-06-01T06:12:49Z",
    "readme": "<picture>\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"./static/browser-use-dark.png\">\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"./static/browser-use.png\">\n  <img alt=\"Shows a black Browser Use Logo in light color mode and a white one in dark color mode.\" src=\"./static/browser-use.png\"  width=\"full\">\n</picture>\n\n<h1 align=\"center\">Enable AI to control your browser \ud83e\udd16</h1>\n\n[![GitHub stars](https://img.shields.io/github/stars/gregpr07/browser-use?style=social)](https://github.com/gregpr07/browser-use/stargazers)\n[![Discord](https://img.shields.io/discord/1303749220842340412?color=7289DA&label=Discord&logo=discord&logoColor=white)](https://link.browser-use.com/discord)\n[![Cloud](https://img.shields.io/badge/Cloud-\u2601\ufe0f-blue)](https://cloud.browser-use.com)\n[![Documentation](https://img.shields.io/badge/Documentation-\ud83d\udcd5-blue)](https://docs.browser-use.com)\n[![Twitter Follow](https://img.shields.io/twitter/follow/Gregor?style=social)](https://x.com/gregpr07)\n[![Twitter Follow](https://img.shields.io/twitter/follow/Magnus?style=social)](https://x.com/mamagnus00)\n[![Weave Badge](https://img.shields.io/endpoint?url=https%3A%2F%2Fapp.workweave.ai%2Fapi%2Frepository%2Fbadge%2Forg_T5Pvn3UBswTHIsN1dWS3voPg%2F881458615&labelColor=#EC6341)](https://app.workweave.ai/reports/repository/org_T5Pvn3UBswTHIsN1dWS3voPg/881458615)\n\n\ud83c\udf10 Browser-use is the easiest way to connect your AI agents with the browser.\n\n\ud83d\udca1 See what others are building and share your projects in our [Discord](https://link.browser-use.com/discord)! Want Swag? Check out our [Merch store](https://browsermerch.com).\n\n\ud83c\udf24\ufe0f Skip the setup - try our <b>hosted version</b> for instant browser automation! <b>[Try the cloud \u2601\ufe0e](https://cloud.browser-use.com)</b>.\n\n# Quick start\n\nWith pip (Python>=3.11):\n\n```bash\npip install browser-use\n```\n\nFor memory functionality (requires Python<3.13 due to PyTorch compatibility):  \n\n```bash\npip install \"browser-use[memory]\"\n```\n\nInstall the browser:\n```bash\nplaywright install chromium --with-deps --no-shell\n```\n\nSpin up your agent:\n\n```python\nimport asyncio\nfrom dotenv import load_dotenv\nload_dotenv()\nfrom browser_use import Agent\nfrom langchain_openai import ChatOpenAI\n\nasync def main():\n    agent = Agent(\n        task=\"Compare the price of gpt-4o and DeepSeek-V3\",\n        llm=ChatOpenAI(model=\"gpt-4o\"),\n    )\n    await agent.run()\n\nasyncio.run(main())\n```\n\nAdd your API keys for the provider you want to use to your `.env` file.\n\n```bash\nOPENAI_API_KEY=\nANTHROPIC_API_KEY=\nAZURE_OPENAI_ENDPOINT=\nAZURE_OPENAI_KEY=\nGOOGLE_API_KEY=\nDEEPSEEK_API_KEY=\nGROK_API_KEY=\nNOVITA_API_KEY=\n```\n\nFor other settings, models, and more, check out the [documentation \ud83d\udcd5](https://docs.browser-use.com).\n\n### Test with UI\n\nYou can test browser-use using its [Web UI](https://github.com/browser-use/web-ui) or [Desktop App](https://github.com/browser-use/desktop).\n\n### Test with an interactive CLI\n\nYou can also use our `browser-use` interactive CLI (similar to `claude` code):\n\n```bash\npip install browser-use[cli]\nbrowser-use\n```\n\n# Demos\n\n<br/><br/>\n\n[Task](https://github.com/browser-use/browser-use/blob/main/examples/use-cases/shopping.py): Add grocery items to cart, and checkout.\n\n[![AI Did My Groceries](https://github.com/user-attachments/assets/a0ffd23d-9a11-4368-8893-b092703abc14)](https://www.youtube.com/watch?v=L2Ya9PYNns8)\n\n<br/><br/>\n\nPrompt: Add my latest LinkedIn follower to my leads in Salesforce.\n\n![LinkedIn to Salesforce](https://github.com/user-attachments/assets/50d6e691-b66b-4077-a46c-49e9d4707e07)\n\n<br/><br/>\n\n[Prompt](https://github.com/browser-use/browser-use/blob/main/examples/use-cases/find_and_apply_to_jobs.py): Read my CV & find ML jobs, save them to a file, and then start applying for them in new tabs, if you need help, ask me.'\n\nhttps://github.com/user-attachments/assets/171fb4d6-0355-46f2-863e-edb04a828d04\n\n<br/><br/>\n\n[Prompt](https://github.com/browser-use/browser-use/blob/main/examples/browser/real_browser.py): Write a letter in Google Docs to my Papa, thanking him for everything, and save the document as a PDF.\n\n![Letter to Papa](https://github.com/user-attachments/assets/242ade3e-15bc-41c2-988f-cbc5415a66aa)\n\n<br/><br/>\n\n[Prompt](https://github.com/browser-use/browser-use/blob/main/examples/custom-functions/save_to_file_hugging_face.py): Look up models with a license of cc-by-sa-4.0 and sort by most likes on Hugging face, save top 5 to file.\n\nhttps://github.com/user-attachments/assets/de73ee39-432c-4b97-b4e8-939fd7f323b3\n\n<br/><br/>\n\n## More examples\n\nFor more examples see the [examples](examples) folder or join the [Discord](https://link.browser-use.com/discord) and show off your project. You can also see our [`awesome-prompts`](https://github.com/browser-use/awesome-prompts) repo for prompting inspiration.\n\n# Vision\n\nTell your computer what to do, and it gets it done.\n\n## Roadmap\n\n### Agent\n\n- [ ] Improve agent memory to handle +100 steps\n- [ ] Enhance planning capabilities (load website specific context)\n- [ ] Reduce token consumption (system prompt, DOM state)\n\n### DOM Extraction\n\n- [ ] Enable detection for all possible UI elements\n- [ ] Improve state representation for UI elements so that all LLMs can understand what's on the page\n\n### Workflows\n\n- [ ] Let user record a workflow - which we can rerun with browser-use as a fallback\n- [ ] Make rerunning of workflows work, even if pages change\n\n### User Experience\n\n- [ ] Create various templates for tutorial execution, job application, QA testing, social media, etc. which users can just copy & paste.\n- [ ] Improve docs\n- [ ] Make it faster\n\n### Parallelization\n\n- [ ] Human work is sequential. The real power of a browser agent comes into reality if we can parallelize similar tasks. For example, if you want to find contact information for 100 companies, this can all be done in parallel and reported back to a main agent, which processes the results and kicks off parallel subtasks again.\n\n\n## Contributing\n\nWe love contributions! Feel free to open issues for bugs or feature requests. To contribute to the docs, check out the `/docs` folder.\n\n## Local Setup\n\nTo learn more about the library, check out the [local setup \ud83d\udcd5](https://docs.browser-use.com/development/local-setup).\n\n\n`main` is the primary development branch with frequent changes. For production use, install a stable [versioned release](https://github.com/browser-use/browser-use/releases) instead.\n\n---\n\n## Swag\n\nWant to show off your Browser-use swag? Check out our [Merch store](https://browsermerch.com). Good contributors will receive swag for free \ud83d\udc40.\n\n## Citation\n\nIf you use Browser Use in your research or project, please cite:\n\n```bibtex\n@software{browser_use2024,\n  author = {M\u00fcller, Magnus and \u017duni\u010d, Gregor},\n  title = {Browser Use: Enable AI to control your browser},\n  year = {2024},\n  publisher = {GitHub},\n  url = {https://github.com/browser-use/browser-use}\n}\n```\n\n <div align=\"center\"> <img src=\"https://github.com/user-attachments/assets/06fa3078-8461-4560-b434-445510c1766f\" width=\"400\"/> \n \n[![Twitter Follow](https://img.shields.io/twitter/follow/Gregor?style=social)](https://x.com/gregpr07)\n[![Twitter Follow](https://img.shields.io/twitter/follow/Magnus?style=social)](https://x.com/mamagnus00)\n \n </div>\n\n<div align=\"center\">\nMade with \u2764\ufe0f in Zurich and San Francisco\n </div>\n"
  },
  {
    "name": "dgm",
    "url": "https://github.com/davidatoms/dgm",
    "description": "Darwin G\u00f6del Machine: Open-Ended Evolution of Self-Improving Agents",
    "type": "fork",
    "updated_at": "2025-05-30T03:09:44Z",
    "readme": "<h1 align=\"center\">\n    Darwin G\u00f6del Machine:<br/>Open-Ended Evolution of Self-Improving Agents\n</h1>\n\n<p align=\"center\">\n  <a href=\"https://github.com/jennyzzt/dgm/blob/main/LICENSE\"><img src=\"https://img.shields.io/badge/License-Apache%202.0-blue.svg?style=for-the-badge\"></a>\n  <a href=\"https://arxiv.org/abs/2505.22954\"><img src=\"https://img.shields.io/badge/arXiv-2505.22954-b31b1b.svg?logo=arxiv&style=for-the-badge\"></a>\n  <a href=\"https://sakana.ai/dgm/\"><img src=\"https://img.shields.io/badge/-Blog-%238D6748?style=for-the-badge&logo=Website&logoColor=white\"></a>\n  <a href=\"https://x.com/SakanaAILabs/status/1928272612431646943\"><img src=\"https://img.shields.io/badge/twitter-%230077B5.svg?&style=for-the-badge&logo=twitter&logoColor=white&color=00acee\"></a>\n  <a href=\"https://drive.google.com/drive/folders/1Kcu9TbIa9Z50pJ7S6hH9omzzD1pxIYZC?usp=sharing\"><img src=\"https://img.shields.io/badge/Experiment%20Logs-4285F4?style=for-the-badge&logo=googledrive&logoColor=white\"></a>\n</p>\n\n<p align=\"center\">\n  <img src=\"./misc/overview.gif\" width=\"100%\" height=\"auto\" />\n</p>\n\nRepository for **Darwin G\u00f6del Machine (DGM)**, a novel self-improving system that iteratively modifies its own code (thereby also improving its ability to modify its own codebase) and empirically validates each change using coding benchmarks.\n\n<p align=\"center\">\n<img src=\"./misc/conceptual.svg\"/></a><br>\n</p>\n\n\n## Setup\n```bash\n# API keys, add to ~/.bashrc\nexport OPENAI_API_KEY='...'\nexport ANTHROPIC_API_KEY='...'\n```\n\n```bash\n# Verify that Docker is properly configured in your environment.\ndocker run hello-world\n \n# If a permission error occurs, add the user to the Docker group\nsudo usermod -aG docker $USER\nnewgrp docker\n```\n\n```bash\n# Install dependencies\npython3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n\n# Optional: for running analysis\nsudo apt-get install graphviz graphviz-dev\npip install -r requirements_dev.txt\n```\n\n```bash\n# Clone SWE-bench\ncd swe_bench\ngit clone https://github.com/princeton-nlp/SWE-bench.git\ncd SWE-bench\ngit checkout dc4c087c2b9e4cefebf2e3d201d27e36\npip install -e .\ncd ../../\n\n# Prepare Polyglot\n# Make sure git is properly configured in your environment with username and email\npython polyglot/prepare_polyglot_dataset.py\n```\n\n## Running the DGM\n```bash\npython DGM_outer.py\n```\nBy default, outputs will be saved in the `output_dgm/` directory.\n\n## File Structure\n- `analysis/` scripts used for plotting and analysis\n- `initial/` SWE-bench logs and performance of the initial agent\n- `initial_polyglot/` Polyglot logs and performance of the initial agent\n- `swe_bench/` code needed for SWE-bench evaluation\n- `polyglot/` code needed for Polyglot evaluation\n- `prompts/` prompts used for foundation models\n- `tests/` tests for the DGM system\n- `tools/` tools available to the foundation models\n- `coding_agent.py` main implementation of the initial coding agent\n- `DGM_outer.py` entry point for running the DGM algorithm\n\n## Logs from Experiments\nThis [google drive folder](https://drive.google.com/drive/folders/1Kcu9TbIa9Z50pJ7S6hH9omzzD1pxIYZC?usp=sharing) contains all the foundation model output logs from the experiments shown in the paper.\n\n## Safety Consideration\n> [!WARNING]  \n> This repository involves executing untrusted, model-generated code. We strongly advise users to be aware of the associated safety risks. While it is highly unlikely that such code will perform overtly malicious actions under our current settings and with the models we use, it may still behave destructively due to limitations in model capability or alignment. By using this repository, you acknowledge and accept these risks.\n\n## Acknowledgement\n\nThe evaluation framework implementations are based on the [SWE-bench](https://github.com/swe-bench/SWE-bench) and [polyglot-benchmark](https://github.com/Aider-AI/polyglot-benchmark) repositories.\n\n## Citing\nIf you find this project useful, please consider citing:\n```bibtex\n@article{zhang2025darwin,\n  title={Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents},\n  author={Zhang, Jenny and Hu, Shengran and Lu, Cong and Lange, Robert and Clune, Jeff},\n  journal={arXiv preprint arXiv:2505.22954},\n  year={2025}\n}\n```\n"
  },
  {
    "name": "neuronpedia",
    "url": "https://github.com/davidatoms/neuronpedia",
    "description": "open source interpretability platform \ud83e\udde0",
    "type": "fork",
    "updated_at": "2025-05-30T02:06:57Z",
    "readme": "<p align=\"center\">\n  <a href=\"https://github.com/hijohnnylin/neuronpedia\">\n    <img src=\"https://github.com/user-attachments/assets/9bcea0bf-4fa9-401d-bb7a-d031a4d12636\" alt=\"Splash GIF\"/>\n  </a>\n\n<h3 align=\"center\"><a href=\"https://neuronpedia.org\">neuronpedia.org \ud83e\udde0\ud83d\udd0d</a></h3>\n\n  <p align=\"center\">\n    open source interpretability platform\n    <br />\n    <sub>\n    <strong>api \u00b7 steering \u00b7 activations \u00b7 autointerp \u00b7 scoring \u00b7 inference \u00b7 search \u00b7 filter \u00b7 dashboards \u00b7 benchmarks \u00b7 cossim \u00b7 umap \u00b7 embeds \u00b7 probes \u00b7 saes \u00b7 lists \u00b7 exports \u00b7 uploads</strong>\n    </sub>\n  </p>\n</p>\n\n<p align=\"center\" style=\"color: #cccccc;\">\n  <a href=\"https://github.com/hijohnnylin/neuronpedia/blob/main/LICENSE\"><img height=\"20px\" src=\"https://img.shields.io/badge/license-MIT-yellow.svg\" alt=\"MIT\"></a>\n  <a href=\"https://status.neuronpedia.org\"><img height=\"20px\" src=\"https://uptime.betterstack.com/status-badges/v2/monitor/1roih.svg\" alt=\"Uptime\"></a>\n  <a href=\"https://join.slack.com/t/opensourcemechanistic/shared_invite/zt-2o756ku1c-_yKBeUQMVfS_p_qcK6QLeA\"><img height=\"20px\" src=\"https://img.shields.io/badge/slack-purple?logo=slack&logoColor=white\" alt=\"Slack\"></a>\n  <a href=\"mailto:johnny@neuronpedia.org\"><img height=\"20px\" src=\"https://img.shields.io/badge/contact-blue.svg?logo=data:image/svg%2bxml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIGZpbGw9Im5vbmUiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+PGcgaWQ9IlNWR1JlcG9fYmdDYXJyaWVyIiBzdHJva2Utd2lkdGg9IjAiPjwvZz48ZyBpZD0iU1ZHUmVwb190cmFjZXJDYXJyaWVyIiBzdHJva2UtbGluZWNhcD0icm91bmQiIHN0cm9rZS1saW5lam9pbj0icm91bmQiPjwvZz48ZyBpZD0iU1ZHUmVwb19pY29uQ2FycmllciI+IDxwYXRoIGQ9Ik00IDcuMDAwMDVMMTAuMiAxMS42NUMxMS4yNjY3IDEyLjQ1IDEyLjczMzMgMTIuNDUgMTMuOCAxMS42NUwyMCA3IiBzdHJva2U9IiNmZmZmZmYiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIj48L3BhdGg+IDxyZWN0IHg9IjMiIHk9IjUiIHdpZHRoPSIxOCIgaGVpZ2h0PSIxNCIgcng9IjIiIHN0cm9rZT0iI2ZmZmZmZiIgc3Ryb2tlLXdpZHRoPSIyIiBzdHJva2UtbGluZWNhcD0icm91bmQiPjwvcmVjdD4gPC9nPjwvc3ZnPg==\" alt=\"Email\"></a>\n  <a href=\"https://neuronpedia.org/blog\"><img height=\"20px\" src=\"https://img.shields.io/badge/blog-10b981.svg\" alt=\"blog\"></a>\n  <a href=\"https://neuronpedia.org\"><img height=\"20px\" src=\"https://img.shields.io/badge/website-gray.svg\" alt=\"website\"></a>\n  <a href=\"https://www.every.org/decode-research\"><img height=\"20px\" src=\"https://img.shields.io/badge/sponsor-green.svg\" alt=\"Sponsor\"></a>\n\n</p>\n\n- [about neuronpedia](#about-neuronpedia)\n- [instant start - vercel deploy](#instant-start---vercel-deploy)\n- [quick start - local webapp + demo environment](#quick-start---local-webapp--demo-environment)\n- [setting up your local environment](#setting-up-your-local-environment)\n  - [\"i want to use a local database / import more neuronpedia data\"](#i-want-to-use-a-local-database--import-more-neuronpedia-data)\n  - [\"i want to do webapp (frontend + api) development\"](#i-want-to-do-webapp-frontend--api-development)\n  - [\"i want to run/develop inference locally\"](#i-want-to-rundevelop-inference-locally)\n  - ['i want to run/develop autointerp locally\\`](#i-want-to-rundevelop-autointerp-locally)\n  - ['i want to do high volume autointerp explanations'](#i-want-to-do-high-volume-autointerp-explanations)\n  - ['i want to generate my own dashboards/data and add it to neuronpedia'](#i-want-to-generate-my-own-dashboardsdata-and-add-it-to-neuronpedia)\n- [architecture](#architecture)\n  - [requirements](#requirements)\n  - [services](#services)\n    - [services are standalone apps](#services-are-standalone-apps)\n    - [service-specific documentation](#service-specific-documentation)\n  - [openapi schema](#openapi-schema)\n  - [monorepo directory structure](#monorepo-directory-structure)\n- [security](#security)\n- [contact / support](#contact--support)\n- [contributing](#contributing)\n- [appendix](#appendix)\n    - ['make' commands reference](#make-commands-reference)\n    - [import data into your local database](#import-data-into-your-local-database)\n    - [why an openai api key is needed for search explanations](#why-an-openai-api-key-is-needed-for-search-explanations)\n\n<!-- # ultra-quick start: one-click deploy on vercel\nTODO, after making repo public -->\n\n# about neuronpedia\n\ncheck out our [blog post](https://www.neuronpedia.org/blog/neuronpedia-is-now-open-source) about Neuronpedia, why we're open sourcing it, and other details. there's also a [tweet thread](https://x.com/neuronpedia/status/1906793456879775745) with quick demos.\n\n**feature overview**\n\na diagram showing the main features of neuronpedia as of march 2025.\n![neuronpedia-features](https://github.com/user-attachments/assets/13e07a93-e046-4e1c-b670-2d26d251d55d)\n\n# instant start - vercel deploy\n\nclick the `Deploy` button to instantly deploy a custom neuronpedia. a [free vercel account](https://vercel.com/signup) is required.\n\n<p align=\"left\">\n  <a href=\"https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Fhijohnnylin%2Fneuronpedia&env=NEXT_PUBLIC_SITE_NAME_VERCEL_DEPLOY&envDescription=***Your%20Custom%20Website%20Name.%20For%20example%3A%20PuppyNeurons***&root-directory=apps/webapp&build-command=npx%20prisma%20generate%20%26%26%20npm%20run%20build%3Ademo&project-name=my-neuronpedia&repository-name=my-neuronpedia&demo-title=Neuronpedia&demo-description=Deploy%20your%20own%20custom%20Neuronpedia%20%F0%9F%9A%80%F0%9F%A7%A0%F0%9F%A7%90&demo-url=https%3A%2F%2Fneuronpedia.org\">\n    <img src=\"https://vercel.com/button\" width=\"160\" alt=\"Deploy with Vercel\"/>\n  </a>\n</p>\n\nhere's how easy it is to deploy a \"PuppyNeurons\" fork of Neuronpedia:\n\nhttps://github.com/user-attachments/assets/707deaed-02b4-442b-8c1f-bf44d193b9fa\n\n# quick start - local webapp + demo environment\n\n#### what this does\n\nthis sets up the webapp (frontend + api) locally, and connects to a public remote demo database and public inference servers\n\n#### what you'll get\n\nafter following the quick start, you will be able to use neuronpedia for some sources/SAEs we have preloaded in `gpt2-small` and `gemma-2-2b/-it`.\n\n> \u26a0\ufe0f **warning:** since you are connecting to a public, read-only demo database, you will not be able to add new data immediately. you will need to follow [subsequent steps](#i-want-to-use-my-own-database--import-more-neuronpedia-data) to configure your own database that you can write to.\n\n#### steps\n\n1. install [docker desktop (UI)](https://docs.docker.com/desktop/) or [docker engine (no UI)](https://docs.docker.com/engine/), and launch it.\n2. generate your local `.env`\n   ```\n   make init-env\n   ```\n3. build the webapp (this will take ~10 min the first time)\n   ```\n   make webapp-demo-build\n   ```\n4. bring up the webapp\n   ```\n   make webapp-demo-run\n   ```\n5. once everything is up, open [localhost:3000](http://localhost:3000) to load the home page.\n6. your local instance is connected to the remote demo database and inference servers, with the following SAEs/sources data available:\n\n| model                          | source/sae                            | comment                                        |\n| ------------------------------ | ------------------------------------- | ---------------------------------------------- |\n| `gpt2-small`                   | `res-jb`, all layers                  | a small starter SAE set                        |\n| `gemma-2-2b` / `gemma-2-2b-it` | `gemmascope-res-16k`, all layers      | the SAEs used in the Gemma Scope demo          |\n| `deepseek-r1-distill-llama-8b` | `llamascope-slimpj-res-32k`, layer 15 | SAE for a reasoning model, trained by OpenMOSS |\n\n7. example things you can do (links work after `make webapp-demo-run`)\n\n   i. steering - [steer gpt2-small on cats](http://localhost:3000/gpt2-small/steer?source=10-res-jb&index=16899&strength=40)\n\n   ii. activation tests/search - [test activation for a gemma-2-2b feature](http://localhost:3000/gemma-2-2b/20-gemmascope-res-16k/502?defaulttesttext=what's%20the%20deal%20with%20airplane%20food%3F)\n\n   iii. search by explanation, [if you configured](<(#why-an-openai-api-key-is-needed-for-search-explanations)>) an `OPENAI_API_KEY` - [search for parrots features](http://localhost:3000/search-explanations/?q=parrots)\n\n   iv. browse dashboards - [a parrot feature](http://localhost:3000/gpt2-small/11-res-jb/23687)\n\n   v. run the [gemma-scope demo](http://localhost:3000/gemma-scope#main)\n\n8. now that we've set up a local webapp that's usable, this is a good time to quickly review neuronpedia's [simple architecture](#architecture) and its [individual services](#services), so that you can get a better understanding of what you'll set up later. then, keep going to [setting up your local environment](#setting-up-your-local-environment).\n\n> \ud83d\udd25 **pro-tip:** see all the available `make` commands by running `make help`\n\n# setting up your local environment\n\nonce you've played around with the demo, you will start running into limitations, like having a limited number of models/SAEs to use, or not being able to generate new explanations. this is because the public demo database is read-only.\n\nideally, you will probably eventually want to do all of the sub-sections below, so you can have everything running locally. however, you may only be interested in specific parts of neuronpedia to start:\n\n1. if you want to jump into developing webapp frontend or api with the demo environment, follow [webapp dev](#i-want-to-do-webapp-frontend--api-development)\n2. if you want to start loading more sources/data and relying on your own local database, follow [local database](#i-want-to-use-a-local-database--import-more-neuronpedia-data)\n\n> \ud83d\udd25 **pro-tip:** neuronpedia is configured for AI agent development. here's an example using a [single prompt](https://github.com/hijohnnylin/neuronpedia/blob/main/apps/experiments/steerify/README.md#claude-code-prompt) to build a custom app (Steerify) using Neuronpedia's inference server as a backend:\n\nhttps://github.com/user-attachments/assets/bc82f88b-8155-4c1d-948a-ea5d987ae0f8\n\n## \"i want to use a local database / import more neuronpedia data\"\n\n#### what this does + what you'll get\n\nrelying on the demo environment means you are limited to read-only access to a specific set of SAEs. these steps show you how to configure and connect to your own local database. you can then download sources/SAEs of your choosing:\n\nhttps://github.com/user-attachments/assets/d7fbb46e-8522-4f98-aa08-21c6529424af\n\n> \u26a0\ufe0f **warning:** your database will start out empty. you will need to use the admin panel to [import sources/data](#import-data-into-your-local-database) (activations, explanations, etc).\n\n> \u26a0\ufe0f **warning:** the local database environment does not have any inference servers connected, so you won't be able to do activation testing, steering, etc initially. you will need to [configure a local inference instance]().\n\n#### steps\n\n1. build the webapp\n   ```\n   make webapp-localhost-build\n   ```\n2. bring up the webapp\n   ```\n   make webapp-localhost-run\n   ```\n3. go to [localhost:3000](http://localhost:3000) to see your local webapp instance, which is now connected to your local database\n4. see the `warnings` above for caveats, and `next steps` to finish setting up\n\n#### next steps\n\n1. [click here](#import-data-into-your-local-database) for how to import data into your local database (activations, explanations, etc), because your local database will be empty to start\n2. [click here](#i-want-to-rundevelop-inference-locally) for how to bring up a local `inference` service for the model/source/SAE you're working with\n\n## \"i want to do webapp (frontend + api) development\"\n\n#### what this does\n\nthe webapp builds you've been doing so far are _production builds_, which are slow to build, and fast to run. since they are slow to build and don't have debug information, they are not ideal for development.\n\nthis subsection installs the development build on your local machine (not docker), then mounts the build inside your docker instance.\n\n#### what you'll get\n\nonce you do this section, you'll be able to do local development and quickly see changes that are made, as well as see more informative debug/errors. if you are purely interested in doing frontend/api development for neuronpedia, you don't need to set up anything else!\n\n#### steps\n\n1. install [nodejs](https://nodejs.org) via [node version manager](https://github.com/nvm-sh/nvm)\n   ```\n   make install-nodejs\n   ```\n2. install the webapp's dependencies\n   ```\n   make webapp-localhost-install\n   ```\n3. run the development instance\n   ```\n   make webapp-localhost-dev\n   ```\n4. go to [localhost:3000](http://localhost:3000) to see your local webapp instance\n\n#### doing local webapp development\n\n- **auto-reload**: when you change any files in the `apps/webapp` subdirectory, the `localhost:3000` will automatically reload\n- **install commands**: you do not need to run `make install-nodejs` again, and you only need to run `make webapp-localhost-install` if dependencies change\n\n## \"i want to run/develop inference locally\"\n\n#### what this does + what you'll get\n\nonce you start using a local environment, you won't be connected to the demo environment's inference instances. this subsection shows you how to run an inference instance locally so you can do things like steering, activation testing, etc on the sources/SAEs you've downloaded.\n\n> \u26a0\ufe0f **warning:** for the local environment, we only support running one inference server at a time. this is because you are unlikely to be running multiple models simultaneously on one machine, as they are memory and compute intensive.\n\n#### steps\n\n1. ensure you have [installed poetry](https://python-poetry.org/docs/#installation)\n2. install the inference server's dependencies\n   ```\n   make inference-localhost-install\n   ```\n3. build the image, picking the correct command based on if the machine has CUDA or not:\n   ```\n   # CUDA\n   make inference-localhost-build-gpu USE_LOCAL_HF_CACHE=1\n   ```\n   ```\n   # no CUDA\n   make inference-localhost-build USE_LOCAL_HF_CACHE=1\n   ```\n   > \u27a1\ufe0f The [`USE_LOCAL_HF_CACHE=1` flag](https://github.com/hijohnnylin/neuronpedia/pull/89) mounts your local HuggingFace cache at `${HOME}/.cache/huggingface/hub:/root/.cache/huggingface/hub`. If you wish to create a new cache in your container instead, you can omit this flag here and in the next step.\n4. run the inference server, using the `MODEL_SOURCESET` argument to specify the `.env.inference.[model_sourceset]` file you're loading from. for this example, we will run `gpt2-small`, and load the `res-jb` sourceset/SAE set, which is configured in the `.env.inference.gpt2-small.res-jb` file. you can see the other [pre-loaded inference configs](#pre-loaded-inference-server-configurations) or [create your own config](#making-your-own-inference-server-configurations) as well.\n\n   ```\n   # CUDA\n   make inference-localhost-dev-gpu \\\n        MODEL_SOURCESET=gpt2-small.res-jb \\\n        USE_LOCAL_HF_CACHE=1\n\n   # no CUDA\n   make inference-localhost-dev \\\n        MODEL_SOURCESET=gpt2-small.res-jb \\\n        USE_LOCAL_HF_CACHE=1\n   ```\n\n5. wait for it to load (first time will take longer). when you see `Initialized: True`, the local inference server is now ready on `localhost:5002`\n\n#### using the inference server\n\nto interact with the inference server, you have a few options - note that this will only work for the model / selected source you have loaded:\n\n1.  load the webapp with the [local database setup](#i-want-to-use-a-local-database--import-more-neuronpedia-data), then using the model / selected source as you would normally do on neuronpedia.\n2.  use the pre-generated inference python client at `packages/python/neuronpedia-inference-client` (set environment variable `INFERENCE_SERVER_SECRET` to `public`, or whatever it's set to in `.env.localhost` if you've changed it)\n3.  use the openapi spec, located at `schemas/openapi/inference-server.yaml` to make calls with any client of your choice.\n4.  [TODO #1](https://github.com/hijohnnylin/neuronpedia/issues/1): Use a documentation generator to make a simple tester-server that can be activated with `make doc-inference-localhost`\n\n#### pre-loaded inference server configurations\n\nwe've provided some pre-loaded inference configs as examples of how to load a specific model and sourceset for inference. view them by running `make inference-list-configs`:\n\n```\n$ make inference-list-configs\n\nAvailable Inference Configurations (.env.inference.*)\n================================================\n\ndeepseek-r1-distill-llama-8b.llamascope-slimpj-res-32k\n    Model: meta-llama/Llama-3.1-8B\n    Source/SAE Sets: '[\"llamascope-slimpj-res-32k\"]'\n    make inference-localhost-dev MODEL_SOURCESET=deepseek-r1-distill-llama-8b.llamascope-slimpj-res-32k\n\ngemma-2-2b-it.gemmascope-res-16k\n    Model: gemma-2-2b-it\n    Source/SAE Sets: '[\"gemmascope-res-16k\"]'\n    make inference-localhost-dev MODEL_SOURCESET=gemma-2-2b-it.gemmascope-res-16k\n\ngpt2-small.res-jb\n    Model: gpt2-small\n    Source/SAE Sets: '[\"res-jb\"]'\n    make inference-localhost-dev MODEL_SOURCESET=gpt2-small.res-jb\n```\n\n#### making your own inference server configurations\n\nlook at the `.env.inference.*` files for examples on how to make these inference server configurations.\n\nthe `MODEL_ID` is the model id from the [transformerlens model table](https://transformerlensorg.github.io/TransformerLens/generated/model_properties_table.html) and each of `SAE_SETS` is the text after the layer number and hyphen in a neuronpedia source ID - for example, if you have a neuronpedia feature at url `http://neuronpedia.org/gpt2-small/0-res-jb/123`, the `0-res-jb` is the source ID, and the item in the `SAE_SETS` is `res-jb`. This example matches the `.env.inference.gpt2-small.res-jb` file exactly.\n\nyou can find neuronpedia source IDs in the saelens [pretrained saes yaml file](https://github.com/jbloomAus/SAELens/blob/main/sae_lens/pretrained_saes.yaml) or by clicking into models in the [neuronpedia datasets exports](https://neuronpedia-datasets.s3.us-east-1.amazonaws.com/index.html?prefix=v1/) directory.\n\n**using models not officially supported by transformerlens**\nlook at the `.env.inference.deepseek-r1-distill-llama-8b.llamascope-slimpj-res-32k` to see an example of how to load a model not officially supported by transformerlens. this is mostly for swapping in weights of a distilled/fine-tuned model.\n\n**loading non-saelens sources/SAEs**\n\n- [TODO #2](https://github.com/hijohnnylin/neuronpedia/issues/2) document how to load SAEs/sources that are not in saelens pretrained yaml\n\n#### doing local inference development\n\n- **schema-driven development**: to add new endpoints or change existing endpoints, you will need to start by updating the openapi schemas, then generating clients from that, then finally updating the actual inference and webapp code. for details on how to do this, see the [openapi readme: making changes to the inference server](schemas/README.md#making-changes-to-the-inference-server)\n- **no auto-reload**: when you change any files in the `apps/inference` subdirectory, the inference server will _NOT_ automatically reload, because server reloads are slow: they reload the model and all sources/SAEs. if you want to enable autoreload, then append `AUTORELOAD=1` to the `make inference-localhost-dev` call, like so:\n  ```\n  make inference-localhost-dev \\\n       MODEL_SOURCESET=gpt2-small.res-jb \\\n       AUTORELOAD=1\n  ```\n\n## 'i want to run/develop autointerp locally`\n\nthis section is under construction.\n\n- check out the [autointerp readme](apps/autointerp/README.md)\n- [TODO](https://github.com/hijohnnylin/neuronpedia/issues/5) instructions for setting up autointerp server locally\n- TODO - look at the `autointerp` service in [docker-compose.yaml](docker-compose.yaml)\n- schema-driven development: [openapi readme: making changes to the autointerp server](schemas/README.md#making-changes-to-the-autointerp-server)\n\n## 'i want to do high volume autointerp explanations'\n\nthis section is under construction.\n\n- use EleutherAI's [Delphi library](https://github.com/EleutherAI/delphi)\n- for OpenAI's autointerp, use [utils/neuronpedia_utils/batch-autointerp.py](utils/neuronpedia_utils/batch-autointerp.py)\n\n## 'i want to generate my own dashboards/data and add it to neuronpedia'\n\nthis section is under construction.\n\n[TODO: simplify generation + upload of data to neuronpedia](https://github.com/hijohnnylin/neuronpedia/issues/46)\n\n[TODO: neuronpedia-utils should use poetry](https://github.com/hijohnnylin/neuronpedia/issues/43)\n\nin this example, we will generate dashboards/data for an [SAELens](https://github.com/jbloomAus/SAELens)-compatible SAE, and upload it to our own Neuronpedia instance.\n\n1. ensure you have [Poetry installed](https://python-poetry.org/docs/)\n2. [upload](https://github.com/jbloomAus/SAELens/blob/main/tutorials/uploading_saes_to_huggingface.ipynb) your SAELens-compatible source/SAE to HuggingFace.\n   > Example\n   > \u27a1\ufe0f [https://huggingface.co/chanind/gemma-2-2b-batch-topk-matryoshka-saes-w-32k-l0-40](https://huggingface.co/chanind/gemma-2-2b-batch-topk-matryoshka-saes-w-32k-l0-40)\n3. clone SAELens locally.\n   ```\n   git clone https://github.com/jbloomAus/SAELens.git\n   ```\n4. open your cloned SAELens and edit the file `sae_lens/pretrained_saes.yaml`. add a new entry at the bottom, based on the template below (see comments for how to fill it out):\n   > Example\n   > \u27a1\ufe0f [https://github.com/jbloomAus/SAELens/pull/455/files](https://github.com/jbloomAus/SAELens/pull/455/files)\n   ```\n   gemma-2-2b-res-matryoshka-dc:                 # a unique ID for your set of SAEs\n     conversion_func: null                       # null if your SAE config is already compatible with SAELens\n     links:                                      # optional links\n       model: https://huggingface.co/google/gemma-2-2b\n     model: gemma-2-2b                           # transformerlens model id - https://transformerlensorg.github.io/TransformerLens/generated/model_properties_table.html\n     repo_id: chanind/gemma-2-2b-batch-topk-matryoshka-saes-w-32k-l0-40  # the huggingface repo path\n     saes:\n     - id: blocks.0.hook_resid_post                 # an id for this SAE\n       path: standard/blocks.0.hook_resid_post      # the path in the repo_id to the SAE\n       l0: 40.0\n       neuronpedia: gemma-2-2b/0-matryoshka-res-dc  # what you expect the neuronpedia URI to be - neuronpedia.org/[this_slug]. should be [model_id]/[layer]-[identical_slug_for_this_sae_set]\n     - id: blocks.1.hook_resid_post                 # more SAEs in this SAE set\n       path: standard/blocks.1.hook_resid_post\n       l0: 40.0\n       neuronpedia: gemma-2-2b/1-matryoshka-res-dc  # note that this is identical to the entry above, except 1 instead of 0 for the layer\n     - [...]\n   ```\n5. clone [SAEDashboard](https://github.com/jbloomAus/SAEDashboard.git) locally.\n   ```\n   git clone https://github.com/jbloomAus/SAEDashboard.git\n   ```\n6. configure your cloned `SAEDashboard` to use your cloned modified `SAELens`, instead of the one in production\n   ```\n   cd SAEDashboard                    # set directory\n   poetry lock && poetry install      # install dependencies\n   poetry remove sae-lens             # remove production dependency\n   poetry add PATH/TO/CLONED/SAELENS  # set local dependency\n   ```\n7. generate dashboards for the SAE. this will take from 30 min to a few hours, depending on your hardware and size of model.\n\n   ```\n   cd SAEDashboard                    # set directory\n   rm -rf cached_activations          # clear old cached data\n\n   # start the generation. details for each argument (full details: https://github.com/jbloomAus/SAEDashboard/blob/main/sae_dashboard/neuronpedia/neuronpedia_runner_config.py)\n   #     - sae-set = should match the unique ID for the set from pretrained_saes.yaml\n   #     - sae-path = should match the id for the sae in from pretrained_saes.yaml\n   #     - np-set-name = should match the [identical_slug_for_this_sae_set] for the sae.neuronpedia from pretrained_saes.yaml\n   #     - dataset-path = the huggingface dataset to use for generating activations. usually you want to use the same dataset the model was trained on.\n   #     - output-dir = the output directory of the dashboard data\n   #     - n-prompts = number of activation texts to test from the dataset\n   #     - n-tokens-in-prompt, n-features-per-batch, n-prompts-in-forward-pass = keep these at 128\n   poetry run neuronpedia-runner \\\n        --sae-set=\"gemma-2-2b-res-matryoshka-dc\" \\\n        --sae-path=\"blocks.12.hook_resid_post\" \\\n        --np-set-name=\"matryoshka-res-dc\" \\\n        --dataset-path=\"monology/pile-uncopyrighted\" \\\n        --output-dir=\"neuronpedia_outputs/\" \\\n        --sae_dtype=\"float32\" \\\n        --model_dtype=\"bfloat16\" \\\n        --sparsity-threshold=1 \\\n        --n-prompts=24576 \\\n        --n-tokens-in-prompt=128 \\\n        --n-features-per-batch=128 \\\n        --n-prompts-in-forward-pass=128\n   ```\n\n8. convert these dashboards for import into neuronpedia\n   ```\n   cd neuronpedia/utils/neuronpedia-utils          # get into this current repository's util directory\n   python convert-saedashboard-to-neuronpedia.py   # start guided conversion script. follow the steps.\n   ```\n9. once dashboard files are generated for neuronpedia, upload these to the global Neuronpedia S3 bucket - currently you need to [contact us](mailto:johnny@neuronpedia.org) to do this.\n10. from a localhost instance, [import your data](#i-want-to-use-a-local-database--import-more-neuronpedia-data)\n\n# architecture\n\nhere's how the services/scripts connect in neuronpedia. it's easiest to read this diagram by starting at the image of the laptop (\"User\").\n\n![architecture diagram](architecture.png)\n\n## requirements\n\nyou can run neuronpedia on any cloud and on any modern OS. neuronpedia is designed to avoid vendor lock-in. these instructions were written for and tested on macos 15 (sequoia), so you may need to repurpose commands for windows/ubuntu/etc. at least 16GB ram is recommended.\n\n## services\n\n| name       | description                                                                                                                                                  | powered by                            |\n| ---------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------- |\n| webapp     | serves the neuronpedia.org frontend and [the api](neuronpedia.org/api-doc)                                                                                   | [next.js](https://nextjs.org) / react |\n| database   | stores features, activations, explanations, users, lists, etc                                                                                                | postgres                              |\n| inference  | [support server] steering, activation testing, search via inference, topk, etc. a separate instance is required for each model you want to run inference on. | python / torch                        |\n| autointerp | [support server] auto-interp explanations and scoring, using eleutherAI's [delphi](https://github.com/EleutherAI/delphi) (formerly `sae-auto-interp`)        | python                                |\n\n### services are standalone apps\n\nby design, each service can be run independently as a standalone app. this is to enable extensibility and forkability.\n\nfor example, if you like the neuronpedia webapp frontend but want to use a different API for inference, you can do that! just ensure your alternative inference server supports the `schema/openapi/inference-server.yaml` spec, and/or that you modify the neuronpedia calls to inference under `apps/webapp/lib/utils`.\n\n### service-specific documentation\n\nthere are draft `README`s for each specific app/service under `apps/[service]`, but they are heavily WIP. you can also check out the `Dockerfile` under the same directory to build your own images.\n\n## openapi schema\n\nfor services to communicate with each other in a typed and consistent way, we use openapi schemas. there are some exceptions - for example, streaming is not offically supported by the openapi spec. however, even in that case, we still try our best to define a schema and use it.\n\nespecially for inference and autointerp server development, it is critical to understand and use the instructions under the [openapi readme](schemas/README.md).\n\nopenapi schemas are located under `/schemas`. we use openapi generators to generate clients in both typescript and python.\n\n## monorepo directory structure\n\n`apps` - the three neuronpedia services: webapp, inference, and autointerp. most of the code is here.\n`schemas` - the openapi schemas. to make changes to inference and autointerp endpoints, first make changes to their schemas - see details in the [openapi readme](schemas/README.md).\n`packages` - clients generated from the `schemas` using generator tools. you will mostly not need to manually modify these files.\n`utils` - various utilities that help do offline processing, like high volume autointerp, or generating dashboards, or exporting data.\n\n# security\n\nplease report vulnerabilities to [johnny@neuronpedia.org](mailto:johnny@neuronpedia.org).\n\nwe don't currently have an official bounty program, but we'll try our best to give compensation based on the severity of the vulnerability - though it's likely we will not able able to offer awards for any low-severity vulnerabilities.\n\n# contact / support\n\n- slack: [join #neuronpedia](https://join.slack.com/t/opensourcemechanistic/shared_invite/zt-2o756ku1c-_yKBeUQMVfS_p_qcK6QLeA)\n- email: [johnny@neuronpedia.org](mailto:johnny@neuronpedia.org)\n- issues: [github issues](https://github.com/hijohnnylin/neuronpedia/issues)\n\n# contributing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md).\n\n# appendix\n\n### 'make' commands reference\n\nyou can view all available `make` commands and brief descriptions of them by running `make help`\n\n### import data into your local database\n\nif you set up your own database, it will start out empty - no features, explanations, activations, etc. to load this data, there's a built-in `admin panel` where you can download this data for SAEs (or \"sources\") of your choosing.\n\n> \u26a0\ufe0f **warning:** the admin panel is finicky and does not currently support resuming imports. if an import is interrupted, you must manually click `re-sync`. the admin panel currently does not check if your download is complete or missing parts - it is up to you to check if the data is complete, and if not, to click `re-sync` to re-download the entire dataset.\n\n> \u2139\ufe0f **recommendation:** when importing data, start with just one source (like `gpt2-small`@`10-res-jb`) instead of downloading everything at once. This makes it easier to verify the data imported correctly and lets you start using neuronpedia faster.\n\nthe instructions below demonstrate how to download the `gpt2-small`@`10-res-jb` SAE data.\n\n1. navigate to [localhost:3000/admin](http://localhost:3000/admin).\n2. scroll down to `gpt2-small`, and expand `res-jb` with the `\u25b6`.\n3. click `Download` next to `10-res-jb`.\n4. wait patiently - this can be a _LOT_ of data, and depending on your connection/cpu speed it can take up to 30 minutes or an hour.\n5. once it's done, click `Browse` or use the navbar to try it out: `Jump To`/`Search`/`Steer`.\n6. repeat for other SAE/source data you wish to download.\n\n### why an openai api key is needed for search explanations\n\nin the webapp, the `search explanations` feature requires you to set an `OPENAI_API_KEY`. otherwise you will get no search results.\n\nthis is because the `search explanations` functionality searches for features by semantic similarity. if you search `cat`, it will also return `feline`, `tabby`, `animal`, etc. to do this, it needs to calculate the embedding for your input `cat`. we use openai's embedding api (specifically, `text-embedding-3-large` with `dimension: 256`) to calculate the embeddings.\n"
  },
  {
    "name": "opencage-geocoding-mcp",
    "url": "https://github.com/davidatoms/opencage-geocoding-mcp",
    "description": "MCP server for querying the OpenCage geocoding API from within LLMs",
    "type": "fork",
    "updated_at": "2025-05-27T22:34:08Z",
    "readme": "# OpenCage Geocoding MCP Server\n\nAn MCP (Model Context Protocol) server that provides geocoding capabilities using the [OpenCage geocoding API](https://opencagedata.com/api).\nThis server allows you to convert between addresses and geographic coordinates.\n\n**PLEASE NOTE:** the examples shown here are based on an integration with [claude.ai](https://claude.ai/)'s desktop client. MCP as a concept is supported by other services, but may require a slightly different configuration.\n\n## Features\n\n- **Forward Geocoding**: Convert addresses or place names to coordinates (latitude/longitude)\n- **Reverse Geocoding**: Convert coordinates to addresses\n- **API Status Monitoring**: Check your API usage and rate limits (assuming your penCage account has hard limits).\n\n## Prerequisites\n\n1. **Node.js** (version 20 or higher)\n2. **OpenCage geocoding API Key**: Sign up on [the OpenCage website](https://opencagedata.com/) to get a free-trial geocoding API key\n\n## Installation\n\n1. Clone the repository. Change into the repository directory\n\n```bash\n\ngit clone git@github.com:OpenCageData/opencage-geocoding-mcp.git\n# or\ngit clone https://github.com/OpenCageData/opencage-geocoding-mcp.git\n\ncd opencage-geocoding-mcp\n```\n\n2. Install dependencies:\n\n```bash\nnpm install\n```\n\n3. Set your OpenCage geocoding API key as an environment variable:\n\n```bash\nexport OPENCAGE_API_KEY=\"your_opencage_geocoding_api_key_here\"\n```\n\n4. Build the project:\n\n```bash\nnpm run build\n```\n\n## Usage\n\n### Using within Claude Desktop\n\nAdd this configuration to your Claude Desktop config file\n\nOn a Mac the config file should be (`~/Library/Application Support/Claude/claude_desktop_config.json`), but you can also navigate to the file via the menu: `Settings > Developer > Edit Config`\n\n```json\n{\n  \"mcpServers\": {\n    \"opencage-geocoding\": {\n      \"command\": \"node\",\n      \"args\": [\"/ABSOLUTE/PATH/TO/opencage-geocoding-mcp/build/index.js\"],\n      \"env\": {\n        \"OPENCAGE_API_KEY\": \"your_opencage_geocoding_api_key_here\"\n      }\n    }\n  }\n}\n```\n\n## Available Tools\n\nNote: the first time you run a command you will need to give Claude permission\n\n![Allow external integration](allow-external-integration.png)\n\n### 1. geocode-forward\n\nConvert an address or place name to coordinates and information about that location.\n\n**Parameters:**\n\n- `query` (required): The address or place name to geocode\n- `countrycode` (optional): Restrict to country (ISO 3166-1 alpha-2 code)\n- `bounds` (optional): Bounding box (min_lon,min_lat,max_lon,max_lat)\n- `language` (optional): Language for results (e.g., 'en', 'de', 'fr')\n- `limit` (optional): Max results (1-100, default 10)\n- `no_annotations` (optional): Exclude location annotations\n\n**Example:**\n\n```\nQuery: \"1600 Pennsylvania Avenue, Washington, DC\"\nResult: JSON with coordinates, formatted address, confidence score, address components, annotations\n```\n\n### 2. geocode-reverse\n\nConvert coordinates to an address and information about that location\n\n**Parameters:**\n\n- `latitude` (required): Latitude coordinate (-90 to 90) in decimal format\n- `longitude` (required): Longitude coordinate (-180 to 180) in decimal format\n- `language` (optional): Language for results\n- `no_annotations` (optional): Exclude location annotations\n\n**Example:**\n\n```\nInput: 38.8976, -77.0365\nResult: \"1600 Pennsylvania Avenue NW, Washington, DC 20500, United States of America\"\n```\n\n### 3. get-opencage-info\n\nCheck your current API usage and rate limits.\n**NOTE**: subscription customers do NOT have hard usage limits. See [relevant documentation](https://opencagedata.com/api#rate-limiting).\n\n**Parameters:** None\n\n**Returns:** Information about remaining requests, rate limits, and reset times.\n\n## Available Prompts\n\n### geocoding-assistant\n\nA helpful assistant for geocoding tasks. Provides guidance on using the geocoding tools effectively.\n\n## Error Handling\n\nThe server includes comprehensive error handling:\n\n- Invalid API keys\n- Rate limit exceeded\n- Network errors\n- Invalid coordinates or addresses\n- API service unavailable\n\n## Environment Variables\n\n- `OPENCAGE_API_KEY`: Your OpenCage geocoding API key (required)\n\n## Troubleshooting\n\n1. **\"API key required\" error**: Make sure the env var `OPENCAGE_API_KEY` is set\n2. **\"No results found\"**: Try a more specific or different address format, see [the OpenCage guide to query formatting](https://opencagedata.com/guides/how-to-format-your-geocoding-query)\n3. **Rate limit errors**: Check your API usage with `get-api-status` tool\n4. **Network errors**: Verify internet connection or [the public OpenCage status page](https://status.opencagedata.com/)\n\n## Relevant Links\n\n- [OpenCage homepage](https://opencagedata.com/) - Get your geocoding API key\n- [OpenCage API Documentation](https://opencagedata.com/api) - Full OpenCage geocoding API reference\n- [Model Context Protocol](https://modelcontextprotocol.io/) - Learn more about MCP\n\n### Who is OpenCage GmbH?\n\n<a href=\"https://opencagedata.com\"><img src=\"opencage_logo_300_150.png\"></a>\n\nWe run a worldwide [geocoding API](https://opencagedata.com/api) and [geosearch](https://opencagedata.com/geosearch) service based on open data.\nLearn more [about us](https://opencagedata.com/about).\n\nWe also organize [Geomob](https://thegeomob.com), a series of regular meetups for location based service creators, where we do our best to highlight geoinnovation. If you like geo stuff, you will probably enjoy [the Geomob podcast](https://thegeomob.com/podcast/).\n"
  },
  {
    "name": "Warp",
    "url": "https://github.com/davidatoms/Warp",
    "description": "Warp is a modern, Rust-based terminal with AI built in so you and your team can build great software, faster.",
    "type": "fork",
    "updated_at": "2025-05-27T20:26:33Z",
    "readme": "<p align=\"center\">\n    <a href=\"https://app.warp.dev/get_warp\">\n    <img width=\"612\" alt=\"horz - dark\" src=\"https://storage.googleapis.com/warpdotdev-content/warp_logo-21_10.png\">\n    </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://www.warp.dev\">Website</a>\n  \u00b7\n  <a href=\"https://docs.warp.dev\">Docs</a>\n  \u00b7\n  <a href=\"https://docs.warp.dev/getting-started/getting-started-with-warp\">Install Guide</a>\n  \u00b7\n  <a href=\"https://www.warp.dev/blog/how-warp-works\">How Warp Works</a>\n</p>\n\n<a href=\"https://www.warp.dev\">\n    <img width=\"1024\" alt=\"Warp Terminal product preview\" src=\"https://storage.googleapis.com/warpdotdev-content/warp-product-24_10.png\">\n</a>\n\n<h1></h1>\n\n## About\n\nThis is an issues-only repo for [Warp](https://www.warp.dev), a modern terminal with AI and your dev team\u2019s knowledge built-in.\n\nWarp is:\n* Built with Rust\n* GPU-accelerated\n* Compatible with zsh, bash, fish, PowerShell, and Git Bash\n* Ready to use on MacOS, Linux, and Windows\n\n## Installation\n\nYou can [download Warp](https://www.warp.dev/download) and [read our docs](https://docs.warp.dev/getting-started/getting-started-with-warp) for platform-specific instructions.\n\n## Changelog and Releases\n\nWe try to release an update weekly, typically on Thursdays. Read our [changelog (release notes).](https://docs.warp.dev/getting-started/changelog)\n\n## Issues, Bugs, and Feature Requests\n\nPlease [search](https://github.com/warpdotdev/warp/issues?q=is%3Aissue+is%3Aopen+a+sort%3Areactions-%2B1-desc) through our existing issues for your bug (including workarounds) or feature request.\n\nIf you can't find a solution above, please file issue requests [in this repo!](https://github.com/warpdotdev/warp/issues/new/choose)\nWe kindly ask that you please use our issue templates to make the issues easier to track for our team.\n\n## Open Source & Contributing\n\nWe are planning to first open-source our Rust UI framework, and then parts and potentially all of our client codebase. The server portion of Warp will remain closed-source for now.\n\nYou can see how we\u2019re thinking about open source here: [https://github.com/warpdotdev/Warp/discussions/400](https://github.com/warpdotdev/Warp/discussions/400)\n\nAs a side note, we are open-sourcing our extension points as we go. The community has already been [contributing new themes](https://github.com/warpdotdev/themes). And we\u2019ve just opened our [Workflows repository](https://github.com/warpdotdev/workflows) for the community to contribute common useful commands.\n\nInterested in joining the team? See our [open roles](https://www.warp.dev/careers) and feel free to email us: hello at warpdotdev\n\n## Support and Questions\n\n1. See our [docs](https://docs.warp.dev/) for a walk-through of the features within our app.\n2. Join our [Discord](https://discord.com/invite/warpdotdev) to chat with other users and get immediate help from members of the Warp team.\n\nFor anything else, please don't hesitate to reach out via email at hello at warpdotdev\n\n## Community Guidelines\n\nAt a high level, we ask everyone to be respectful and empathetic. We follow the [GitHub Community Guidelines](https://docs.github.com/en/github/site-policy/github-community-guidelines):\n\n* Be welcoming and open-minded\n* Respect each other\n* Communicate with empathy\n* Be clear and stay on topic\n\n## Open Source Dependencies\n\nWe'd like to call out a few of the [open source dependencies](https://docs.warp.dev/help/licenses) that have helped Warp to get off the ground:\n\n* [Tokio](https://github.com/tokio-rs/tokio)\n* [NuShell](https://github.com/nushell/nushell)\n* [Fig Completion Specs](https://github.com/withfig/autocomplete)\n* [Warp Server Framework](https://github.com/seanmonstar/warp)\n* [Alacritty](https://github.com/alacritty/alacritty)\n* [Hyper HTTP library](https://github.com/hyperium/hyper)\n* [FontKit](https://github.com/servo/font-kit)\n* [Core-foundation](https://github.com/servo/core-foundation-rs)\n* [Smol](https://github.com/smol-rs/smol)\n"
  },
  {
    "name": "davidatoms",
    "url": "https://github.com/davidatoms/davidatoms",
    "description": "Github Readme Profile",
    "type": "original",
    "updated_at": "2025-05-27T12:02:31Z",
    "readme": "# David Adams Automatic GitHub Readme\n\n<p align=\"left\"><b>Last Updated:</b> <!-- last_updated starts -->May 27, 2025 at 12:02 (147/365 (0.403) of the year)<!-- last_updated ends -->\n</p>\n\n<p align=\"left\">\n  <img src=\"https://img.shields.io/badge/Python-3776AB?style=flat&logo=python&logoColor=white\" />\n  <img src=\"https://img.shields.io/badge/Go-00ADD8?style=flat&logo=go&logoColor=white\" />\n  <img src=\"https://img.shields.io/badge/Rust-000000?style=flat&logo=rust&logoColor=white\" />\n  <img src=\"https://img.shields.io/badge/React-20232A?style=flat&logo=react&logoColor=61DAFB\" />\n  <img src=\"https://img.shields.io/badge/Bash-4EAA25?style=flat&logo=gnu-bash&logoColor=white\" />\n</p>\n\nI am passionate about a better future through innovation and investments. \n\n## Recent Repositories\n<!-- recent_repos starts -->\n[**hacks-leaks-and-revelations**](https://github.com/davidatoms/hacks-leaks-and-revelations) - This repository contains code and resources accompanying the book \"Hacks, Leaks, and Revelations,\" guiding readers through acquiring datasets, using command-line tools, Python programming, and exploring real-world case studies.\n\n[**voxelize**](https://github.com/davidatoms/voxelize) - Voxelize is a multiplayer, super-fast voxel engine that runs in your browser, allowing you to create and explore voxel worlds with custom blocks and physics.\n\n[**lunar-lander-ai**](https://github.com/davidatoms/lunar-lander-ai) - A genetic algorithm-based AI system that trains neural networks to autonomously land rockets in a lunar lander game simulation.\n\n[**typehere.app**](https://github.com/davidatoms/typehere.app) - Type Here is a powerful web-based textarea with vim support, note-taking capabilities, workspaces, and various editing features like themes and scrollbar toggling.\n\n[**LegalStories**](https://github.com/davidatoms/LegalStories) - LegalStories is a dataset containing 294 complex legal doctrines with LLM-generated stories and multiple-choice questions, aimed at enhancing legal literacy through storytelling.\n\n[**tails**](https://github.com/davidatoms/tails) - Tails is a portable operating system that protects your privacy online and helps you avoid censorship by using the Tor network and leaving no trace on the computer.\n\n[**evals**](https://github.com/davidatoms/evals) - The OpenAI Evals repository provides a framework for evaluating large language models, offering existing evals, tools for creating custom evals, and the ability to run private evals with proprietary data.\n\n[**open-canvas**](https://github.com/davidatoms/open-canvas) - Open Canvas is an open-source web application that allows users to collaborate with AI agents to write and edit documents, leveraging memory and personalization for a seamless experience.\n\n[**nemo**](https://github.com/davidatoms/nemo) - The NVIDIA NeMo Framework is a conversational AI toolkit for building state-of-the-art models, offering support for large language models, multimodal models, video foundation models, and more, with a focus on performance and scalability.\n\n[**davidatoms**](https://github.com/davidatoms/davidatoms) - This repository showcases David Adams' coding projects, professional profile, and documentation generated using Anthropic's Claude AI and GitHub Actions to update the README dynamically.\n<!-- recent_repos ends -->\n\n<br>\n\n![Star this repository](https://img.shields.io/badge/Star%20this%20repository-FFDD00?style=flat&logo=github&logoColor=white)\n![Profile Views](https://komarev.com/ghpvc/?username=davidatoms&style=flat&color=blue&label=Views)\n"
  },
  {
    "name": "hacks-leaks-and-revelations",
    "url": "https://github.com/davidatoms/hacks-leaks-and-revelations",
    "description": "Code that goes along with the Hacks, Leaks, and Revelations book",
    "type": "fork",
    "updated_at": "2025-05-26T09:10:27Z",
    "readme": "# Hacks, Leaks, and Revelations\n\nThis git repository contains the code that goes along with the book [Hacks, Leaks, and Revelations](https://hacksandleaks.com/).\n\n_If you find any errors in this repo, please let me know. You can contact me at micah@micahflee.com._\n\n## Part 1: Sources and Datasets\n\n### Chapter 1: Protecting Sources and Yourself\n\n- [Exercise 1-1: Encrypt Your Internal Disk](./chapter-1/exercise-1-1.md)\n- [Exercise 1-2: Encrypt a USB Disk](./chapter-1/exercise-1-2.md)\n- [Exercise 1-3: Install and Use Dangerzone](./chapter-1/exercise-1-3.md)\n\n### Chapter 2: Acquiring Datasets\n\n- [Exercise 2-1: Download BlueLeaks](./chapter-2/exercise-2-1.md)\n- [Exercise 2-2: Installing and Using Signal](./chapter-2/exercise-2-2.md)\n- [Exercise 2-3: Play with Tor and OnionShare](./chapter-2/exercise-2-3.md)\n\n## Part 2: Tools of the Trade\n\n### Chapter 3: The Command Line Interface\n\n- [Exercise 3-1 for Windows Users: Install Ubuntu in Windows](./chapter-3/exercise-3-1.md)\n- [Exercise 3-2 for Mac Users: Manage Packages with Homebrew](./chapter-3/exercise-3-2.md)\n- [Exercise 3-3 for Windows and Linux Users: Manage Packages with Apt](./chapter-3/exercise-3-3.md)\n- [Exercise 3-4: Practice Using the CLI with cURL](./chapter-3/exercise-3-4.md)\n- [Exercise 3-5: Install a Text Editor](./chapter-3/exercise-3-5.md)\n- [Exercise 3-6: Write Your First Shell Script](./chapter-3/exercise-3-6.md)\n- [Exercise 3-7: Clone the Book's Git Repository](./chapter-3/exercise-3-7.md)\n\n### Chapter 4: Exploring Datasets in the Terminal\n\n- [Exercise 4-1: Accessing the BlueLeaks Dataset](./chapter-4/exercise-4-1.md)\n- [Exercise 4-2: Exploring BlueLeaks on the Command Line](./chapter-4/exercise-4-2.md)\n- [Exercise 4-3: Finding Revelations with Grep](./chapter-4/exercise-4-3.md)\n- [Exercise 4-4: Setting Up Your First VPS](./chapter-4/exercise-4-4.md)\n- [Exercise 4-5: Exploring the Oath Keepers Dataset Remotely](./chapter-4/exercise-4-5.md)\n\n### Chapter 5: Docker, Aleph, and Making Datasets Searchable\n\n- [Exercise 5-1 for Windows and Mac users: Install Docker Desktop](./chapter-5/exercise-5-1.md)\n- [Exercise 5-2 for Linux users: Install Docker CE](./chapter-5/exercise-5-2.md)\n- [Exercise 5-3: Run a WordPress Site with Docker Compose](./chapter-5/exercise-5-3.md)\n- [Exercise 5-4: Run Aleph Locally in Linux Containers](./chapter-5/exercise-5-4.md)\n- [Exercise 5-5: Add Part of BlueLeaks to Aleph](./chapter-5/exercise-5-5.md)\n\n### Chapter 6: Reading Other People's Emails\n\n- [Exercise 6-1: Download Email Dumps](./chapter-6/exercise-6-1.md)\n- [Exercise 6-2: Configure Thunderbird for Email Dumps](./chapter-6/exercise-6-2.md)\n- [Exercise 6-3: Import EML Files Into Thunderbird](./chapter-6/exercise-6-3.md)\n- [Exercise 6-4: Import MBOX Files Into Thunderbird](./chapter-6/exercise-6-4.md)\n- [Exercise 6-5: Import PST Files Into Thunderbird](./chapter-6/exercise-6-5.md)\n\n## Part 3: Python Programming\n\n### Chapter 7: An Introduction to Python\n\n- [Exercise 7-1: Install Python](./chapter-7/exercise-7-1.md)\n- [Exercise 7-2: Your First Python Script](./chapter-7/exercise-7-2.md)\n- [Exercise 7-3: Practice the Basics](./chapter-7/exercise-7-3.md)\n- [Exercise 7-4: Practice Loops and Control Flow](./chapter-7/exercise-7-4.md)\n- [Exercise 7-5: Practice Writing Functions](./chapter-7/exercise-7-5.py)\n\n### Chapter 8: Working With Data in Python\n\n- [Exercise 8-1: Traverse the Filesystem](./chapter-8/exercise-8-1.md)\n- [Exercise 8-2: Find the Largest Files in BlueLeaks](./chapter-8/exercise-8-2.md)\n- [Exercise 8-3: Practice Command Line Arguments with Click](./chapter-8/exercise-8-3.md)\n- [Exercise 8-4: Find the Largest Files in Any Dataset](./chapter-8/exercise-8-4.md)\n- [Exercise 8-5: Map Out the CSVs in BlueLeaks](./chapter-8/exercise-8-5.md)\n- [Exercise 8-6: Practice Reading and Writing Files](./chapter-8/exercise-8-6.md)\n\n## Part 4: Structured Data\n\n### Chapter 9: BlueLeaks and the CSV File Format\n\n- [Exercise 9-1: Make BlueLeaks CSVs More Readable](./chapter-9/exercise-9-1.md)\n- [Exercise 9-2: Make Bulk Emails Readable](./chapter-9/exercise-9-2.md)\n- [Exercise 9-3: Make a CSV of BlueLeaks Sites](./chapter-9/exercise-9-3.md)\n\n### Chapter 10: BlueLeaks Explorer\n\n- [Exercise 10-1: Install BlueLeaks Explorer](./chapter-10/exercise-10-1.md)\n- [Exercise 10-2: Finish Building the Structure for JRIC](./chapter-10/exercise-10-2.md)\n\n### Chapter 11: Parler, January 6, and the JSON File Format\n\n- [Exercise 11-1: Download the Parler Video Metadata](./chapter-11/exercise-11-1.md)\n- [Exercise 11-2: Write a Script to Filter for Videos with GPS From January 6, 2021](./chapter-11/exercise-11-2.md)\n- [Exercise 11-3: Update the Script to Filter for Insurrection Videos](./chapter-11/exercise-11-4.md)\n- [Exercise 11-4: Update the Script to Create KML Files to Visualize](./chapter-11/exercise-11-5.md)\n\n### Chapter 12: Epik Fail and SQL Databases\n\n- [Exercise 12-1: Create and Test a MySQL Server Using Docker and Adminer](./chapter-12/exercise-12-1.md)\n- [Exercise 12-2: Query Your SQL Database](./chapter-12/exercise-12-2.md)\n- [Exercise 12-3: Install and Test the Command Line MySQL Client](./chapter-12/exercise-12-3.md)\n- [Exercise 12-4: Download and Extract Part of the Epik Dataset](./chapter-12/exercise-12-4.md)\n- [Exercise 12-5: Import Epik Data Into MySQL](./chapter-12/exercise-12-5.md)\n\n## Part 5: Case Studies\n\n### Chapter 13: Pandemic Profiteers and COVID-19 Disinformation\n\n- [create-aflds-patients-csv.py](./chapter-13/create-aflds-patients-csv.py)\n- [create-ravkoo-csv.py](./chapter-13/create-ravkoo-csv.py)\n- [create-ravkoo-categories-csv.py](./chapter-13/create-ravkoo-categories-csv.py)\n- [create-cadence-partners-csv.py](./chapter-13/create-cadence-partners-csv.py)\n- [create-cities-csv.py](./chapter-13/create-cities-csv.py)\n- [create-ages-csv.py](./chapter-13/create-ages-csv.py)\n- [The Intercept: Network of Right-Wing Health Care Providers Is Making Millions Off Hydroxychloroquine and Ivermectin, Hacked Data Reveals](https://theintercept.com/2021/09/28/covid-telehealth-hydroxychloroquine-ivermectin-hacked/)\n\n### Chapter 14: Neo-Nazis and Their Chat Rooms\n\n- [Discord Analysis](./chapter-14/discord-analysis/README.md)\n- [The Intercept: How Right-Wing Extremists Stalk, Dox, and Harass Their Enemies](https://theintercept.com/2017/09/06/how-right-wing-extremists-stalk-dox-and-harass-their-enemies/)\n- [Unicorn Riot's Discord Leaks](https://discordleaks.unicornriot.ninja/)\n\n## Appendixes\n\n### Appendix A: Using the Windows Subsystem for Linux\n\n- [Official WSL Documentation](https://learn.microsoft.com/en-us/windows/wsl/)\n\n### Appendix B: Scraping the Web\n\n- [HTTPX example script](./appendix-b/httpx-example.py)\n- [Beautiful Soup example script](./appendix-b/bs4-example.py)\n- [Selenium example script](./appendix-b/selenium-example.py)\n\n# Licenses\n\nAll of the source code in this repository is licensed [GPLv3](./LICENSE).\n\nAll of the human language text in this repository is licensed [CC BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/)."
  },
  {
    "name": "voxelize",
    "url": "https://github.com/davidatoms/voxelize",
    "description": ":mushroom: Build your own voxel games with Voxelize! Multiplayer, optimized, highly customizable full stack library.",
    "type": "fork",
    "updated_at": "2025-05-26T08:50:08Z",
    "readme": "<a href=\"https://shaoruu.io\">\n  <p align=\"center\">\n    <img src=\"examples/client/src/assets/logo-circle.png\" width=\"100px\" height=\"100px\" />\n  </p>\n  <h1 align=\"center\">Voxelize</h1>\n</a>\n\n<p align=\"center\">A multiplayer, <i>super fast</i>, voxel engine in your browser!</p>\n\n<p align=\"center\">\n  <a href=\"https://discord.gg/9483RZtWVU\">\n  <img alt=\"Discord Server\" src=\"https://img.shields.io/discord/1229328337713762355?label=Discord&logo=Discord&style=for-the-badge\">\n  </a>\n  <img src=\"https://img.shields.io/npm/v/@voxelize/core?logo=npm&style=for-the-badge\">\n  <img src=\"https://img.shields.io/crates/v/voxelize?style=for-the-badge\"/>\n</p>\n\n<a href=\"https://shaoruu.io\">\n  <p align=\"center\">\n  LIVE DEMO\n  </p>\n</a>\n\n![](/assets/Screenshot%202024-02-19%20at%201.37.53\u202fAM.png)\n![](/assets/Screen%20Shot%202022-07-13%20at%201.01.08%20AM.png)\n![](/assets/minejs.png)\n![](/assets/Screen%20Shot%202022-07-19%20at%209.54.24%20PM.png)\n![](/assets/Screen%20Shot%202022-07-31%20at%2011.58.11%20PM.png)\n![](</assets/Screen%20Shot%202022-07-22%20at%208.01.48%20PM%20(2).png>)\n\n## Disclaimer\n\nThis is purely a passionate project. The v0 of this engine, [mc.js](https://github.com/shaoruu/mc.js), was <i>brutally</i> taken down by Microsoft by a DMCA strike with some false claims (claimed that I was collecting actual MC user information even though mc.js wasn't deployed anywhere), so although inspired, I have to clarify that this voxel engine is NOT affiliated with Minecraft, nor does it have any intention collecting existing Minecraft user information (or from any licensed voxel engines). This engine is simply made out of passion, and the textures and assets used in the game are all either licensed for free use or hand-drawn by me. I am a big fan of Minecraft, so Mojang/Microsoft, if you see this, let's work together instead of taking me down :) (Minecraft web demo?)\n\n[@shaoruu](https://github.com/shaoruu)\n\n## Features\n\n- Define custom blocks with custom static or dynamic mesh\n  - Great support for flexible combinational rendering logic\n- Easy-to-decouple server structure to refine the server-side logic\n- Isolated modules that just work\n- Realtime built-in multiplayer support\n- Fast voxel chunk mesh generation on both client and server side (multithreaded)\n- Multi-stage chunk generation with chunk overflow support\n  - No need to worry if a tree overflows to neighboring chunk, that is handled automatically\n- Fully configurable chat system with commands registry\n- AABB Physics engine that works with any static or dynamic blocks\n  - Auto-stepping, raycasting, all included\n- Entity-to-entity collision detection and resolution system\n- Periodic world data persistence\n- Robust event system for custom game events\n- For-dev debug panels that look nice\n\n## Documentation\n\nCheckout the Voxelize documentations here:\n\n- [Backend](https://docs.rs/voxelize/0.8.11/voxelize/index.html)\n- [Frontend](https://docs.voxelize.io/tutorials/intro/what-is-voxelize)\n\n## Development\n\nBefore starting, make sure to install the following:\n\n- [rust](https://www.rust-lang.org/tools/install)\n- [node.js](https://nodejs.org/en/download/)\n- [cargo-watch](https://crates.io/crates/cargo-watch)\n- [protoc](https://grpc.io/docs/protoc-installation/)\n\n```bash\n# clone the repository\ngit clone https://github.com/shaoruu/voxelize.git\ncd voxelize\n\n# download dependencies\npnpm install\n\n# generate protocol buffers\npnpm run proto\n\n# fresh build\npnpm run build\n\n# in a separate terminal, start both frontend/backend demo\npnpm run demo\n```\n\nvisit http://localhost:3000\n\n## Supporting\n\nIf you like our work, please consider supporting us on Patreon, BuyMeACoffee, or PayPal. Thanks a lot!\n\n<p align=\"center\">\n  <a href=\"https://www.patreon.com/voxelize\"><img src=\"https://c5.patreon.com/external/logo/become_a_patron_button.png\" alt=\"Patreon donate button\" /> </a>\n  <a href=\"https://paypal.me/iantheboss\"><img src=\"https://werwolv.net/assets/paypal_banner.png\" alt=\"PayPal donate button\" /> </a>\n  <a href=\"https://www.buymeacoffee.com/shaoruu\"><img src=\"https://i.imgur.com/xPDiGKQ.png\" alt=\"Buy Me A Coffee\" style=\"height: 50px\"/> </a>\n</p>\n\n<p align=\"center\">\n  <img src=\"https://api.star-history.com/svg?repos=voxelize/voxelize&type=Date\" />\n</p>\n\n## Assets Used\n\n- [Connection Serif Font (SIL Open Font)](https://fonts2u.com/connection-serif.font)\n"
  },
  {
    "name": "lunar-lander-ai",
    "url": "https://github.com/davidatoms/lunar-lander-ai",
    "description": ":rocket: Lunar Lander with Genetic Algorithm",
    "type": "fork",
    "updated_at": "2025-05-26T08:49:53Z",
    "readme": "# :rocket: Lunar Lander with Genetic Algorithm\n\nThis project was highly inspired by [Machine Learning Flappy Bird](https://github.com/ssusnic/Machine-Learning-Flappy-Bird).\n\n<a href=\"https://ian13456.github.io/lunar-lander-ai/\">\n<img src=\"https://i.imgur.com/0LRpwjS.png\" style=\"padding-bottom: 20px\"/>\n</a>\n\nLunar Lander is a single-player arcade game in the Lunar Lander subgenre. In the game, the player controls a lunar landing module as viewed from the side and attempts to land safely on the Moon.\n\nFor this project, I thought it would be a nice idea to combine this classic game with modern technology, and train these rockets to land by themselves.\n\n## How?\n\nInspired by [Machine Learning Flappy Bird](https://github.com/ssusnic/Machine-Learning-Flappy-Bird), I decided to combine the concept of Neuron Networks with Genetic Algorithm.\n\n### Neural Network\n\nEach rocket is assigned with a neural network, with an input layer of 9 neurons, a hidden layer of 16 neurons, and an output layer of 4 neurons:\n![](https://i.imgur.com/Gpcfwf2.png)\n\n#### Inputs of the NN\n\n9 input neurons:\n\n- 8 inputs represent the surroundings of the rocket, telling the rocket the distances to the obstacles around it\n- 1 input is the closest \"landable\" platform to the rocket\n\n![](https://i.imgur.com/EThdEsG.png)\n\n#### Outputs of the NN\n\nThe outputs of the NN comes in an array of 4 numbers between 0-1. Four of them each represents `Thrust`, `Rotate Left`, `Rotate Right`, and `Do Nothing`. If the value is over 0.5, the action is performed by the rocket.\n\n### Fitness\n\nEach rocket has a `fitness` function that determines how successful a rocket is upon these criteria:\n\n- Angle difference of landing/crashing platform\n- Fuel used in rocket's lifetime\n- Landing/crashing speed\n- Distance of rocket to the closest landable platform\n\nThis is essentially the score of the rocket, used as a measure of how successful a certain rocket is at landing.\n\n### Genetic Algorithm\n\nI mass spawn 30 rockets each iteration. In a certain time period, the rockets either crash, land or stay in the air.\n\nAfter each iteration, I calculate the fitness of each and every rocket, and rank them from high to low. I keep the top rockets, and [reproduce](https://natureofcode.com/book/chapter-9-the-evolution-of-code/#96-the-genetic-algorithm-part-iii-reproduction) new rockets by crossing over and mutating their weights and biases out of the top units.\n\n## Results\n\nAfter a few iterations, rockets start to land by themselves, and the average fitness slowly reaches a maximum value.\n\n<p align=\"center\">\n<img src=\"https://i.imgur.com/fZeiCZW.gif\" width=\"1000\"/>\n</p>\n\n## Resources\n\n- [Machine Learning Flappy Bird](https://github.com/ssusnic/Machine-Learning-Flappy-Bird)\n- [Coding Train's Evolution of Code](https://natureofcode.com/book/chapter-9-the-evolution-of-code/)\n- [My friend Baltazar's project](https://github.com/balta-z-r/lunar-lander)\n- [Artistic inspiration from online lunar lander](http://moonlander.seb.ly/)\n- [Synaptic NN Library](https://github.com/cazala/synaptic)\n"
  },
  {
    "name": "typehere.app",
    "url": "https://github.com/davidatoms/typehere.app",
    "description": "\u270e A quick, simple, and powerful textarea",
    "type": "fork",
    "updated_at": "2025-05-26T08:49:15Z",
    "readme": "# Type Here: A Powerful Textarea\n\nA textarea (with vim)\n\n<img width=\"2560\" alt=\"image\" src=\"https://github.com/shaoruu/typehere.app/assets/35216312/4fdcbb50-6d84-48f4-88d8-07e5f3547a92\">\n\n## Inspirations\n\nI used to use typehere.co a lot until the site was taken down, so I created [typehere.app](https://typehere.app). For a long while, it was also just a textarea that saved its contents to `localStorage`, but recently I've decided to add more features that I would find useful myself.\n\n## How To Use\n\n- Most things are in the ctrl/cmd-K menu. I will call it cmd-K for this guide.\n- Cmd-K uses fuzzy search. There are two types of things that cmd-K has: notes and commands.\n  - Notes are the notes you've created, commands are things like theme toggle, vim toggle, show/hide scrollbar, import/export, etc.\n- Navigate in the Cmd-K menu by up/down arrows.\n- Create a new note by typing the note title and run the create note command.\n  - Or you could do cmd+shift+enter. \n- Enter a note or run a command by pressing \"Enter\" or clicking on it.\n- Workspaces is the way to separate notes into different \"groups\" under cmd-K.\n  - You can create a workspace by doing cmd-K, type in a workspace name, and \"Create workspace\". This creates a workspace with an empty note.\n  - You can switch between workspaces in cmd-K by doing left/right arrow keys.\n  - You can also switch between workspace by just typing in the workspace name you want to go to and run the command.\n  - You can select a note you want (arrow up/down), and cmd + left/right arrow to move it between workspace.\n  - If there are no notes in a workspace, the workspace is automatically deleted. Under the hood, each note has a workspace string, and all workspace is just a set of all the notes workspaces.\n- For vim/keyboard-only users (like me)\n  - Toggle vim by doing cmd-K, toggle vim.\n  - Arrow up/down keys work the same as cmd+J/K inside the cmd-K menu.\n  - Cmd+B to open a note. This means to switch between the top two notes, hold cmd, and press K-J-B.\n  - Cmd+U/I to switch between workspaces.\n  - Cmd+E to toggle narrow screen view.\n  - Cmd+G to pin a note to all workspaces. this means the note will be displayed no matter which workspace you're in.\n  - Cmd+H to hide a note. In order to access hidden notes, you need to type the first 5 characters of the title right in the cmd-K menu.\n\n## Other Features\n\n- Offline mode \n- Everything client-side, all in `localStorage`\n- Periodically backed up to `indexedDB` (also in your browser)\n- Import/export notes\n- Desktop app (build on own machine)\n"
  }
]